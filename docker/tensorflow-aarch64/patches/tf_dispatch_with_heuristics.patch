 *******************************************************************************
 Copyright 2023 Arm Limited and affiliates.
 SPDX-License-Identifier: Apache-2.0

 Licensed under the Apache License, Version 2.0 (the "License");
 you may not use this file except in compliance with the License.
 You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License.
 *******************************************************************************
diff --git a/tensorflow/core/common_runtime/mkl_layout_pass.cc b/tensorflow/core/common_runtime/mkl_layout_pass.cc
index 7125cbbd31e..833ef686122 100644
--- a/tensorflow/core/common_runtime/mkl_layout_pass.cc
+++ b/tensorflow/core/common_runtime/mkl_layout_pass.cc
@@ -33,6 +33,7 @@ limitations under the License.
 #include "absl/base/call_once.h"
 #include "tensorflow/core/common_runtime/function.h"
 #include "tensorflow/core/common_runtime/optimization_registry.h"
+#include "tensorflow/core/common_runtime/process_util.h"
 #include "tensorflow/core/framework/node_def_util.h"
 #include "tensorflow/core/framework/tensor.pb.h"
 #include "tensorflow/core/graph/algorithm.h"
@@ -49,6 +50,18 @@ limitations under the License.
 
 namespace tensorflow {
 
+// Table storing thread synchronization and framework overhead costs on each CPU
+// architecture for each oneNN-eligible operation. Our heuristics use these
+// costs to determine whether we should rewrite the operation to use oneDNN.
+static const RewriteThreshold rewrite_thresholds[] = {
+#ifdef DNNL_AARCH64_USE_ACL
+    {"Conv2D", 0x41, 0xd40, {0.9349, 22.603}},
+    {"_FusedConv2D", 0x41, 0xd40, {0.9349, 22.603}},
+    {"FusedBatchNormV3", 0x41, 0xd40, {0.3223, -0.8822}},
+    {"Sigmoid", 0x41, 0xd40, {0.0, 0.064736}},
+#endif  // DNNL_AARCH64_USE_ACL
+    {"", 0x0, 0x0, {0, 0}}};
+
 // This pass implements rewriting of graph to support following scenarios:
 // (A) Merging nodes in the graph
 // (B) Rewriting a node in the graph to a new node
@@ -239,7 +252,7 @@ namespace tensorflow {
 //
 class MklLayoutRewritePass : public GraphOptimizationPass {
  public:
-  MklLayoutRewritePass() {
+  MklLayoutRewritePass() : num_intra_threads_(port::MaxParallelism()) {
     // NOTE: names are alphabetically sorted.
     csinfo_.addn = "AddN";
     csinfo_.avg_pool = "AvgPool";
@@ -316,6 +329,7 @@ class MklLayoutRewritePass : public GraphOptimizationPass {
     csinfo_.mkl_native_pad_with_fused_conv2d = "_MklNativePadWithFusedConv2D";
     csinfo_.mkl_pad_with_conv2d = "_MklPadWithConv2D";
     csinfo_.mkl_pad_with_fused_conv2d = "_MklPadWithFusedConv2D";
+    csinfo_.mkl_swish = "_MklSwish";
     csinfo_.pad = "Pad";
     csinfo_.pad_with_conv2d = "__MklDummyPadWithConv2D";
     csinfo_.pad_with_fused_conv2d = "__MklDummyPadWithFusedConv2D";
@@ -379,6 +393,7 @@ class MklLayoutRewritePass : public GraphOptimizationPass {
     csinfo_.mul = "Mul";
     csinfo_.squared_difference = "SquaredDifference";
     csinfo_.sub = "Sub";
+    csinfo_.sigmoid = "Sigmoid";
     // End - element-wise ops. See note above.
 
     const bool native_fmt = NativeFormatEnabled();
@@ -422,9 +437,12 @@ class MklLayoutRewritePass : public GraphOptimizationPass {
         {csinfo_.conjugate_transpose,
          mkl_op_registry::GetMklOpName(csinfo_.conjugate_transpose),
          CopyAttrsAll, AlwaysRewrite, kRewriteForOpNameChange});
-    rinfo_.push_back(
-        {csinfo_.conv2d, mkl_op_registry::GetMklOpName(csinfo_.conv2d),
-         CopyAttrsConvCheckConstFilter, AlwaysRewrite, GetRewriteCause()});
+    rinfothr_.push_back(
+        {{csinfo_.conv2d, mkl_op_registry::GetMklOpName(csinfo_.conv2d),
+          CopyAttrsConvCheckConstFilter,
+          std::function<bool(const Node*)>(),  // we set this function to empty
+          GetRewriteCause()},
+         Conv2DRewrite});
     rinfo_.push_back({csinfo_.conv2d_with_bias,
                       native_fmt ? csinfo_.mkl_native_conv2d_with_bias
                                  : csinfo_.mkl_conv2d_with_bias,
@@ -483,10 +501,11 @@ class MklLayoutRewritePass : public GraphOptimizationPass {
 
     // Using CopyAttrsAll for V3 on CPU, as there are no additional
     // attributes.
-    rinfo_.push_back(
-        {csinfo_.fused_batch_norm_v3,
-         mkl_op_registry::GetMklOpName(csinfo_.fused_batch_norm_v3),
-         CopyAttrsAll, FusedBatchNormV3Rewrite, GetRewriteCause()});
+    rinfothr_.push_back(
+        {{csinfo_.fused_batch_norm_v3,
+          mkl_op_registry::GetMklOpName(csinfo_.fused_batch_norm_v3),
+          CopyAttrsAll, std::function<bool(const Node*)>(), GetRewriteCause()},
+         FusedBatchNormV3RewriteWithThreads});
     rinfo_.push_back(
         {csinfo_.fused_batch_norm_grad_v3,
          mkl_op_registry::GetMklOpName(csinfo_.fused_batch_norm_grad_v3),
@@ -496,11 +515,14 @@ class MklLayoutRewritePass : public GraphOptimizationPass {
                                  : csinfo_.mkl_fused_batch_norm_ex,
                       CopyAttrsAll, FusedBatchNormExRewrite,
                       GetRewriteCause()});
-    rinfo_.push_back({csinfo_.fused_conv2d,
-                      native_fmt ? csinfo_.mkl_native_fused_conv2d
-                                 : csinfo_.mkl_fused_conv2d,
-                      CopyAttrsFusedConv2DCheckConstFilter, FusedConv2DRewrite,
-                      GetRewriteCause()});
+    rinfothr_.push_back(
+        {{csinfo_.fused_conv2d,
+          native_fmt ? csinfo_.mkl_native_fused_conv2d
+                     : csinfo_.mkl_fused_conv2d,
+          CopyAttrsFusedConv2DCheckConstFilter,
+          std::function<bool(const Node*)>(),  // we set this function to empty
+          GetRewriteCause()},
+          FusedConv2DRewrite});
     rinfo_.push_back({csinfo_.fused_conv3d, csinfo_.mkl_native_fused_conv3d,
                       CopyAttrsAllCheckConstFilter, AlwaysRewrite,
                       kRewriteForOpNameChange});
@@ -721,6 +743,9 @@ class MklLayoutRewritePass : public GraphOptimizationPass {
     wsinfo_.push_back(
         {csinfo_.max_pool3d, csinfo_.max_pool3d_grad, 0, 1, 1, 3});
 
+    // Rule for merging sigmoid and multiplication to oneDNN swish.
+    minfo_.push_back(
+        {csinfo_.sigmoid, csinfo_.mul, csinfo_.mkl_swish, GetSigmoidAndMul});
     // Add a rule for merging nodes
     minfo_.push_back({csinfo_.conv2d, csinfo_.bias_add,
                       csinfo_.conv2d_with_bias, GetConv2DOrBiasAdd});
@@ -786,6 +811,15 @@ class MklLayoutRewritePass : public GraphOptimizationPass {
     RewriteCause rewrite_cause;
   } RewriteInfo;
 
+  /// Structure that carries the original rewrite info, but
+  /// in this case it is using the function that can accept
+  /// what is number of threads that will be used to run
+  /// the operation in parallel.
+  typedef struct {
+    RewriteInfo rinfo;
+    std::function<bool(const Node*, const int)> rewrite_rule;
+  } RewriteInfoThreadCount;
+
   /// Structure to specify a forward op, a backward op, and the slot numbers
   /// in the forward and backward ops where we will add a workspace edge.
   typedef struct {
@@ -920,6 +954,7 @@ class MklLayoutRewritePass : public GraphOptimizationPass {
     string mkl_native_pad_with_fused_conv2d;
     string mkl_pad_with_conv2d;
     string mkl_pad_with_fused_conv2d;
+    string mkl_swish;
     string mul;
     string pad;
     string pad_with_conv2d;
@@ -954,6 +989,7 @@ class MklLayoutRewritePass : public GraphOptimizationPass {
     string relu6;
     string relu6_grad;
     string requantize;
+    string sigmoid;
     string tanh;
     string tanh_grad;
     string transpose;
@@ -968,6 +1004,11 @@ class MklLayoutRewritePass : public GraphOptimizationPass {
  private:
   /// Maintain info about nodes to rewrite
   std::vector<RewriteInfo> rinfo_;
+  /// Maintain info about nodes to rewrite with additional
+  /// information that holds number of threads that should
+  /// be used to parallelise the kernel so that we can decide
+  /// whether it is worth rewriting op to run with oneDNN.
+  std::vector<RewriteInfoThreadCount> rinfothr_;
 
   /// Maintain info about nodes to add workspace edge
   std::vector<WorkSpaceInfo> wsinfo_;
@@ -981,6 +1022,9 @@ class MklLayoutRewritePass : public GraphOptimizationPass {
   /// Maintain structure of constant strings
   static ConstStringsInfo csinfo_;
 
+  /// Number of threads used for intra-parallelism.
+  int num_intra_threads_;
+
  private:
   // Is OpDef::ArgDef a list type? It could be N * T or list(type).
   // Refer to opdef.proto for details of list type.
@@ -1066,11 +1110,26 @@ class MklLayoutRewritePass : public GraphOptimizationPass {
   Status MergeNode(std::unique_ptr<Graph>* g, Node* m, Node* n);
 
   // Helper function to merge different nodes
+  Status MergeSigmoidWithMul(std::unique_ptr<Graph>* g, Node* m, Node* n);
   Status MergeConv2DWithBiasAdd(std::unique_ptr<Graph>* g, Node* m, Node* n);
   Status MergePadWithConv2D(std::unique_ptr<Graph>* g, Node* m, Node* n);
   Status MergeConv2DBackpropFilterWithBiasAddGrad(std::unique_ptr<Graph>* g,
                                                   Node* m, Node* n);
 
+  static Node* GetSigmoidAndMul(const Node* m) {
+    Node* n = nullptr;
+
+    if (m && m->type_string() == csinfo_.sigmoid) {
+      for (const Edge* e : m->out_edges()) {
+        if (!e->IsControlEdge() && e->dst()->type_string() == csinfo_.mul &&
+            e->dst_input() == 0) {
+          n = e->dst();
+          break;
+        }
+      }
+    }
+    return n;
+  }
   // Find BiasAdd or Conv2D node that can be merged with input node 'm'.
   // If input 'm' is BiasAdd, then check if there exists Conv2D node that can be
   // merged with 'm'. If input 'm' is Conv2D, then check if there exists BiasAdd
@@ -1111,6 +1170,27 @@ class MklLayoutRewritePass : public GraphOptimizationPass {
     return n;
   }
 
+  static double FindRewriteThreshold(const Node* n, int threads) {
+    int cpu_family_ = port::CPUFamily();
+    int cpu_model_num_ = port::CPUModelNum();
+
+    if (threads == 0) {
+      // if we do not have information how many threads are used
+      // to parallelise operation we revert to the old behaviour
+      return 0;
+    }
+
+    for (const RewriteThreshold* i = rewrite_thresholds;
+         i->op != "" && threads > 0; i++) {
+      if (n->type_string() == i->op && cpu_family_ == i->cpu_family &&
+          cpu_model_num_ == i->cpu_model_num) {
+        return i->params.thread_sync_cost * threads + i->params.framework_cost;
+      }
+    }
+
+    return 0;
+  }
+
   // Find Pad or Conv2D node that can be merged with input node 'm'.
   // If input 'm' is Pad, then check if there exists Conv2D node that can be
   // merged with 'm'. If input 'm' is Conv2D, then check if there exists Pad
@@ -1721,6 +1801,16 @@ class MklLayoutRewritePass : public GraphOptimizationPass {
     return true;
   }
 
+  static bool FusedBatchNormV3RewriteWithThreads(const Node* n, int threads) {
+    double mflops = CalculateNodeMFlops(n);
+    double thr = FindRewriteThreshold(n, threads);
+    if (mflops > 0 && mflops < thr) {
+      return false;
+    }
+
+    return FusedBatchNormV3Rewrite(n);
+  }
+
   static bool FusedBatchNormExRewrite(const Node* n) {
     DCHECK(n);
 
@@ -1746,7 +1836,66 @@ class MklLayoutRewritePass : public GraphOptimizationPass {
     return true;
   }
 
-  static bool FusedConv2DRewrite(const Node* n) {
+  static double CalculateNodeMFlops(const Node* n) {
+    // Check if we can obtained dimensions for this node.
+    std::vector<const TensorShapeProto*> shape_attrs;
+    if (!TryGetNodeAttr(n->attrs(), "_input_shapes", &shape_attrs)) {
+      // We can't obtain shape so we will revert to default behaviour
+      // to rewrite node.
+      return -1;
+    }
+
+    if ((n->type_string() == "Conv2D" || n->type_string() == "_FusedConv2D") &&
+        shape_attrs.size() == 2) {
+      TensorShape input_shape, filter_shape;
+      if (TensorShape::BuildTensorShape(*shape_attrs[0], &input_shape) !=
+          tsl::OkStatus()) {
+        return -1;
+      }
+      if (TensorShape::BuildTensorShape(*shape_attrs[1], &filter_shape) !=
+          tsl::OkStatus()) {
+        return -1;
+      }
+
+      // MFLOPS = N * H * W * C * FH * FW * FC / 1e6.
+      return input_shape.dim_size(0) * input_shape.dim_size(1) *
+             input_shape.dim_size(2) * input_shape.dim_size(3) *
+             filter_shape.dim_size(0) * filter_shape.dim_size(1) *
+             filter_shape.dim_size(3) / (double)1e6;
+    } else if ((n->type_string() == "FusedBatchNormV3" ||
+                n->type_string() == "Sigmoid") &&
+               shape_attrs.size() >= 1) {
+      TensorShape input_shape;
+      if (TensorShape::BuildTensorShape(*shape_attrs[0], &input_shape) !=
+          tsl::OkStatus()) {
+        return -1;
+      }
+      return input_shape.dim_size(0) * input_shape.dim_size(1) *
+             input_shape.dim_size(2) * input_shape.dim_size(3) / (double)1e6;
+    }
+
+    return -1;
+  }
+
+  static bool Conv2DRewrite(const Node* n, int threads) {
+    // Find out what are dimensions of the convolution,
+    // if dimensions are small we will not rewrite node
+    // to use oneDNN operations as overhead to call into oneDNN
+    // as data setup is higher then actual useful work we
+    // might end up doing.
+    double total_mflops = CalculateNodeMFlops(n);
+    double thr = FindRewriteThreshold(n, threads);
+
+    return true ? (total_mflops < 0 || total_mflops >= thr) : false;
+  }
+
+  static bool FusedConv2DRewrite(const Node* n, int threads) {
+    // Decide whether it is worth rewriting it to oneDNN operation
+    // due to overheads as they will dominate for small shapes.
+    if (!Conv2DRewrite(n, threads)) {
+      return false;
+    }
+
     // MKL DNN currently doesn't support all fusions that grappler fuses
     // together with Conv2D (ex. batchnorm). We rewrite _FusedConv2D only if
     // it includes those we support.
@@ -3032,6 +3181,147 @@ Node* MklLayoutRewritePass::CheckForNodeMerge(const Node* a) const {
   return nullptr;
 }
 
+Status MklLayoutRewritePass::MergeSigmoidWithMul(std::unique_ptr<Graph>* g,
+                                                 Node* m, Node* n) {
+  if (!(m->type_string() == csinfo_.sigmoid &&
+        n->type_string() == csinfo_.mul)) {
+    return Status(error::Code::CANCELLED,
+                  "Mul doesn't follow Sigmoid. "
+                  "Will skip node merge optimization");
+  }
+
+  // Decide whether it is worth optimizing SigMoid+Mul to
+  // call to _MklSwish.
+  double total_mflops = CalculateNodeMFlops(m);
+  double thr = FindRewriteThreshold(m, num_intra_threads_);
+
+  if (total_mflops != -1 && total_mflops < thr) {
+    // Do not merge and execute them as they are,
+    // because overheads of going to oneDNN will dominate
+    // any benefits from accelerating compute.
+    return Status(error::Code::CANCELLED,
+                  "Sigmoid and Mul operate on small shapes, "
+                  "so there is no benefit in optimizing to Swish. "
+                  "Will skip node merge optimization");
+  }
+
+  Node* sigmoid = m;
+  Node* mul = n;
+
+  DataType T_sigmoid, T_mul;
+  TF_CHECK_OK(GetNodeAttr(sigmoid->def(), "T", &T_sigmoid));
+  TF_CHECK_OK(GetNodeAttr(mul->def(), "T", &T_mul));
+
+  const int mul_num = mul->num_inputs();
+  gtl::InlinedVector<Node*, 4> mul_control_edges;
+  gtl::InlinedVector<std::pair<Node*, int>, 4> mul_in(mul_num);
+  FillInputs(mul, &mul_control_edges, &mul_in);
+
+  const int sigmoid_num = sigmoid->num_inputs();
+  gtl::InlinedVector<Node*, 4> sigmoid_control_edges;
+  gtl::InlinedVector<std::pair<Node*, int>, 4> sigmoid_in(sigmoid_num);
+  FillInputs(sigmoid, &sigmoid_control_edges, &sigmoid_in);
+
+  // Sigmoid has 1 input.
+  if (sigmoid->in_edges().size() != 1) {
+    return Status(error::Code::CANCELLED,
+                  "Sigmoid must have only one input edge."
+                  "Will skip node merge optimization");
+  }
+  // Mul has 2 inputs.
+  if (mul->in_edges().size() != 2) {
+    return Status(error::Code::CANCELLED,
+                  "Mul must have only two input edges."
+                  "Will skip node merge optimization");
+  }
+
+  for (const Edge* e : mul->in_edges()) {
+    const int kFirstInputSlot = 0;  // This should be sigmoid.
+    const int kSecondInputSlot =
+        1;  // This should be the same input as in sigmoid.
+    if (e->dst_input() == kFirstInputSlot && e->src() != sigmoid) {
+      return Status(error::Code::INVALID_ARGUMENT,
+                    "Sigmoid doesn't feed to Mul. "
+                    "Will skip node merge optimization");
+    }
+    const Edge* kSigmoidInputEdge = nullptr;
+    TF_CHECK_OK(sigmoid->input_edge(kFirstInputSlot, &kSigmoidInputEdge));
+    if (e->dst_input() == kSecondInputSlot &&
+        e->src() != kSigmoidInputEdge->src()) {
+      return Status(error::Code::INVALID_ARGUMENT,
+                    "Input to Sigmoid and Mul is not the same. "
+                    "Will skip node merge optimization");
+    }
+  }
+
+  NodeBuilder nb(mul->name(), csinfo_.mkl_swish);
+  nb.Input(sigmoid_in[0].first, sigmoid_in[0].second);
+  nb.Attr("T", T_mul);  // Copy type attribute.
+  nb.Device(mul->def().device());
+
+  // Create new node.
+  Node* new_node;
+  TF_CHECK_OK(nb.Finalize(&**g, &new_node));
+
+  std::unordered_set<Node*> unique_node;
+  for (const Edge* e : sigmoid->in_edges()) {
+    if (e->IsControlEdge()) {
+      auto result = unique_node.insert(e->src());
+      if (result.second) {
+        (*g)->AddControlEdge(e->src(), new_node, true);
+      }
+    }
+  }
+  unique_node.clear();
+
+  for (const Edge* e : mul->in_edges()) {
+    if (e->IsControlEdge()) {
+      auto result = unique_node.insert(e->src());
+      if (result.second) {
+        (*g)->AddControlEdge(e->src(), new_node, true);
+      }
+    }
+  }
+  unique_node.clear();
+
+  for (const Edge* e : sigmoid->out_edges()) {
+    if (e->IsControlEdge()) {
+      auto result = unique_node.insert(e->dst());
+      if (result.second) {
+        (*g)->AddControlEdge(new_node, e->dst(), true);
+      }
+    }
+  }
+  unique_node.clear();
+  for (const Edge* e : mul->out_edges()) {
+    if (e->IsControlEdge()) {
+      auto results = unique_node.insert(e->dst());
+      if (results.second) {
+        (*g)->AddControlEdge(new_node, e->dst(), true);
+      }
+    } else {
+      const int kMulOutputSlot = 0;
+      auto new_edge =
+          (*g)->AddEdge(new_node, kMulOutputSlot, e->dst(), e->dst_input());
+      if (!new_edge) {
+        return Status(error::Code::CANCELLED,
+                      "Failed to create a new edge from new node to output."
+                      "Will skip node merge optimization");
+      }
+    }
+  }
+
+  new_node->set_assigned_device_name(mul->assigned_device_name());
+  VLOG(1) << "MklLayoutRewritePass: Merged old node: " << sigmoid->DebugString()
+          << ", and node: " << mul->DebugString()
+          << ", into node: " << new_node->DebugString();
+
+  (*g)->RemoveNode(sigmoid);
+  (*g)->RemoveNode(mul);
+
+  return OkStatus();
+}
+
 Status MklLayoutRewritePass::MergeConv2DWithBiasAdd(std::unique_ptr<Graph>* g,
                                                     Node* m, Node* n) {
   CHECK_EQ(((m->type_string() == csinfo_.bias_add &&
@@ -3515,6 +3805,9 @@ Status MklLayoutRewritePass::MergeNode(std::unique_ptr<Graph>* g, Node* m,
   DCHECK(m);
   DCHECK(n);
 
+  if (m->type_string() == csinfo_.sigmoid && n->type_string() == csinfo_.mul) {
+    return this->MergeSigmoidWithMul(g, m, n);
+  }
   if (((m->type_string() == csinfo_.bias_add &&
         n->type_string() == csinfo_.conv2d)) ||
       ((n->type_string() == csinfo_.bias_add &&
@@ -3523,10 +3816,12 @@ Status MklLayoutRewritePass::MergeNode(std::unique_ptr<Graph>* g, Node* m,
   }
   if ((m->type_string() == csinfo_.pad &&
        (n->type_string() == csinfo_.conv2d ||
-        (n->type_string() == csinfo_.fused_conv2d && FusedConv2DRewrite(n)))) ||
+        (n->type_string() == csinfo_.fused_conv2d &&
+         FusedConv2DRewrite(n, num_intra_threads_)))) ||
       (n->type_string() == csinfo_.pad &&
        (m->type_string() == csinfo_.conv2d ||
-        (m->type_string() == csinfo_.fused_conv2d && FusedConv2DRewrite(m))))) {
+        (m->type_string() == csinfo_.fused_conv2d &&
+         FusedConv2DRewrite(m, num_intra_threads_))))) {
     return this->MergePadWithConv2D(g, m, n);
   }
 
@@ -3845,6 +4140,13 @@ MklLayoutRewritePass::CheckForNodeRewrite(const Node* n) const {
     }
   }
 
+  for (auto rit = rinfothr_.cbegin(); rit != rinfothr_.cend(); ++rit) {
+    if (n->type_string().compare(rit->rinfo.name) == 0 &&
+        rit->rewrite_rule(n, num_intra_threads_)) {
+      return &(rit->rinfo);
+    }
+  }
+
   // Else return not found.
   return nullptr;
 }
@@ -4201,6 +4503,10 @@ Status MklLayoutRewritePass::Run(const GraphOptimizationPassOptions& options) {
     return OkStatus();
   }
 
+  if (options.session_options != nullptr) {
+    num_intra_threads_ =
+        options.session_options->config.intra_op_parallelism_threads();
+  }
   auto process_graph = [&](std::unique_ptr<Graph>* g) {
     // Get the ownership of a graph
     std::unique_ptr<Graph>* ng = std::move(g);
diff --git a/tensorflow/core/common_runtime/mkl_layout_pass.h b/tensorflow/core/common_runtime/mkl_layout_pass.h
index 6b5c586ceab..d4303f1f1e8 100644
--- a/tensorflow/core/common_runtime/mkl_layout_pass.h
+++ b/tensorflow/core/common_runtime/mkl_layout_pass.h
@@ -25,6 +25,26 @@ limitations under the License.
 #include "tensorflow/core/graph/graph.h"
 
 namespace tensorflow {
+
+struct RewriteThreshold {
+  string op;
+  int cpu_family;
+  int cpu_model_num;
+  // The model that is used to decide whether it is worth
+  // accelerating operations using oneDNN is:
+  //
+  // threshold = thread_synchronisation * thread_num + framework_tax
+  //
+  // This finds threshold when framework overhead and thread synchronisations
+  // are amortized with amount of computation that has to be performed.
+  // If we are below this threshold then we will not rewrite the operation to
+  // to be run using oneDNN primitive.
+  struct PerformanceParameters {
+    double thread_sync_cost;
+    double framework_cost;
+  } params;
+};
+
 // Interface to invoke the pass for unit test
 //
 // Returns true if and only if 'g' is mutated.
diff --git a/tensorflow/core/common_runtime/optimize_function_graph_utils.cc b/tensorflow/core/common_runtime/optimize_function_graph_utils.cc
index 1a71a1a4e1c..ab2045a7c8f 100644
--- a/tensorflow/core/common_runtime/optimize_function_graph_utils.cc
+++ b/tensorflow/core/common_runtime/optimize_function_graph_utils.cc
@@ -445,6 +445,13 @@ StatusOr<OptimizedFunctionGraphInfo> OptimizeFunctionGraph(
   optimization_options.debug_filename_prefix = "pflr_optmz_";
   env->CreateUniqueFileName(&optimization_options.debug_filename_prefix, "_");
 
+  if (cpu_device->tensorflow_cpu_worker_threads() != nullptr) {
+    // Pass to the optimisation pass number of intra threads that are used to
+    // parallelise operations
+    session_options.config.set_intra_op_parallelism_threads(
+        cpu_device->tensorflow_cpu_worker_threads()->num_threads);
+  }
+
   DumpGraph("Before running PRE_PLACEMENT passes", graph.get());
   if (should_run_optimization_passes) {
     TF_RETURN_IF_ERROR(OptimizationPassRegistry::Global()->RunGrouping(
diff --git a/tensorflow/core/graph/mkl_graph_util.h b/tensorflow/core/graph/mkl_graph_util.h
index 886c4051c8c..8cf7788327a 100644
--- a/tensorflow/core/graph/mkl_graph_util.h
+++ b/tensorflow/core/graph/mkl_graph_util.h
@@ -288,6 +288,7 @@ static inline bool IsMklElementWiseOp(const string& op_name, DataType T) {
                  0 == op_name.compare(GetMklOpName("Sub")) ||
                  0 == op_name.compare(GetMklOpName("Mul")) ||
                  0 == op_name.compare(GetMklOpName("Maximum")) ||
+                 0 == op_name.compare(GetMklOpName("Sigmoid")) ||
                  0 == op_name.compare(GetMklOpName("SquaredDifference")));
 
   return result;
diff --git a/tensorflow/core/grappler/optimizers/remapper.cc b/tensorflow/core/grappler/optimizers/remapper.cc
index b825e3c8a91..31e2891c3e5 100644
--- a/tensorflow/core/grappler/optimizers/remapper.cc
+++ b/tensorflow/core/grappler/optimizers/remapper.cc
@@ -704,6 +704,23 @@ bool IsBiasSemanticAdd(const RemapperContext& ctx,
   return false;
 }
 
+void AddInputShapesAttr(const RemapperContext& ctx, int node_index) {
+  auto mutable_node = ctx.graph_view.graph()->mutable_node(node_index);
+
+  AttrValue attr_input_shape;
+  auto tensor_properties =
+      ctx.graph_properties.GetInputProperties(mutable_node->name());
+  for (const auto& tensor_property : tensor_properties) {
+    TensorShapeProto* proto = attr_input_shape.mutable_list()->add_shape();
+    *proto = tensor_property.shape();
+  }
+
+  if (IsMKLEnabled()) {
+    (*mutable_node->mutable_attr())["_input_shapes"] =
+        std::move(attr_input_shape);
+  }
+}
+
 bool FindContractionWithBias(const RemapperContext& ctx, int node_index,
                              ContractionWithBiasAdd* matched,
                              bool check_device_compatible = true) {
@@ -1744,50 +1761,6 @@ bool FindMulAndMaximum(RemapperContext* ctx, int node_index,
   return found_op_type_match;
 }
 
-bool FindSigmoidAndMul(RemapperContext* ctx, int node_index,
-                       std::map<string, int>* matched_nodes_map,
-                       std::set<int>* remove_node_indices) {
-  // Gelu fusion is enabled only with oneDNN library.
-  if (!IsMKLEnabled()) return false;
-
-  using utils::MatchingDirection;
-  using utils::NodeStatus;
-  // clang-format off
-  // Convert Sigmoid+Mul to Swish
-  // Mul(x, Sigmoid(x)) --> _MklSwish(x)
-
-  utils::OpTypePattern sigmoidmul_pattern{
-    "Mul", "mul_to_swish", NodeStatus::kReplace,
-    {
-      { "Sigmoid", "sigmoid", NodeStatus::kRemove,
-        {
-          { "*", "input", NodeStatus::kRemain}
-        }
-      },
-      { "*", "input", NodeStatus::kRemain}
-    }
-  };
-  // clang-format on
-  // check for data types
-  auto* mul_node_def = ctx->graph_view.GetNode(node_index)->node();
-  if (!HasDataType(mul_node_def, DT_FLOAT) &&
-      !HasDataType(mul_node_def, DT_BFLOAT16))
-    return false;
-
-  if (!NodeIsOnCpu(mul_node_def)) return false;
-
-  bool found_op_type_match = false;
-  utils::SubGraphMatcher<MatchingDirection::kFollowInputs> graph_matcher(
-      &(ctx->graph_view));
-  matched_nodes_map->clear();
-  remove_node_indices->clear();
-  found_op_type_match = graph_matcher.GetMatchedNodes(
-      sigmoidmul_pattern, {}, ctx->graph_view.GetNode(node_index),
-      matched_nodes_map, remove_node_indices);
-
-  return found_op_type_match;
-}
-
 // Keras LayerNormalization api uses multiple TensorFlow ops. Current fusion
 // pattern is only for the case, when LayerNormalization uses FusedBatcNormV3.
 // We further restrict it to only 2D or 3D tensor inputs to keras
@@ -2431,6 +2404,11 @@ void CopyConv2DAttributes(const NodeDef& conv2d, NodeDef* fused_conv2d,
   (*attr)["dilations"] = src_attr.at("dilations");
   (*attr)["data_format"] = src_attr.at("data_format");
   (*attr)["use_cudnn_on_gpu"] = src_attr.at("use_cudnn_on_gpu");
+  // When copying attributes check whether this convolution has
+  // attribute that describes the shapes on which it is working.
+  if (IsMKLEnabled()) {
+    (*attr)["_input_shapes"] = src_attr.at("_input_shapes");
+  }
   // Copy LeakyRelu's attr alpha to FusedConv2D's attr leakyrelu_alpha
   if (activation != nullptr && IsLeakyRelu(*activation)) {
     auto& activation_attr = activation->attr();
@@ -2583,6 +2561,7 @@ Status AddFusedContractionNode(RemapperContext* ctx,
   fused_op.add_input(bias_add.input(matched.bias_port));  // 2: bias
   if (IsConv2D(contraction)) {
     fused_op.set_op(kFusedConv2D);
+    AddInputShapesAttr(*ctx, matched.contraction);
     CopyConv2DAttributes(contraction, &fused_op);
   } else if (IsDepthwiseConv2dNative(contraction)) {
     fused_op.set_op(kFusedDepthwiseConv2dNative);
@@ -2633,6 +2612,7 @@ Status AddFusedContractionNode(
 
   if (IsConv2D(contraction)) {
     fused_op.set_op(kFusedConv2D);
+    AddInputShapesAttr(*ctx, matched.contraction);
     // leaky relu has a special attribute alpha
     CopyConv2DAttributes(contraction, &fused_op, &activation);
   } else if (IsDepthwiseConv2dNative(contraction)) {
@@ -2687,6 +2667,7 @@ Status AddFusedConvNode(RemapperContext* ctx,
 
   if (IsConv2D(contraction)) {
     fused_conv.set_op(kFusedConv2D);
+    AddInputShapesAttr(*ctx, matched.contraction);
     CopyConv2DAttributes(contraction, &fused_conv);
   } else if (IsConv3D(contraction)) {
     fused_conv.set_op(kFusedConv3D);
@@ -2737,6 +2718,7 @@ Status AddFusedConv2DNode(RemapperContext* ctx,
   fused_conv2d.add_input(fused_batch_norm.input(3));  // 4: mean
   fused_conv2d.add_input(fused_batch_norm.input(4));  // 5: variance
 
+  AddInputShapesAttr(*ctx, matched.contraction);
   CopyConv2DAttributes(contraction, &fused_conv2d);
   SetFusedOpAttributes(&fused_conv2d, {"FusedBatchNorm"},
                        /*num_args=*/4, /*epsilon=*/matched.epsilon);
@@ -2780,6 +2762,7 @@ Status AddFusedConv2DNode(RemapperContext* ctx,
   fused_conv2d.add_input(fused_batch_norm.input(3));  // 4: mean
   fused_conv2d.add_input(fused_batch_norm.input(4));  // 5: variance
 
+  AddInputShapesAttr(*ctx, matched.contraction);
   CopyConv2DAttributes(contraction, &fused_conv2d, &activation);
   SetFusedOpAttributes(&fused_conv2d, {"FusedBatchNorm", activation.op()},
                        /*num_args=*/4, /*epsilon=*/matched.epsilon);
@@ -2825,6 +2808,7 @@ Status AddFusedContractionNode(RemapperContext* ctx,
 
   if (IsConv2D(contraction)) {
     contraction_node.set_op(kFusedConv2D);
+    AddInputShapesAttr(*ctx, matched.contraction);
     CopyConv2DAttributes(contraction, &contraction_node);
   } else if (IsMatMul(contraction)) {
     contraction_node.set_op(kFusedMatMul);
@@ -2925,6 +2909,7 @@ Status AddFusedContractionNode(
 
   if (IsConv2D(contraction)) {
     fused_conv.set_op(kFusedConv2D);
+    AddInputShapesAttr(*ctx, matched.contraction);
     CopyConv2DAttributes(contraction, &fused_conv);
   } else if (IsConv3D(contraction)) {
     fused_conv.set_op(kFusedConv3D);
@@ -3942,6 +3927,15 @@ Status Remapper::Optimize(Cluster* cluster, const GrapplerItem& item,
     ContractionWithBiasAddAndAdd contract_with_bias_and_add;
     ContractionWithBiasAndAddActivation contract_with_bias_and_add_activation;
 
+    // Store dimensions so that they can be retrieved later in
+    // mkl_layout_rewrite_pass when deciding whether to rewrite node.
+    if (IsConv2D(ctx.graph_view.graph()->node(i)) ||
+        IsFusedBatchNorm(ctx.graph_view.graph()->node(i)) ||
+        IsDepthwiseConv2dNative(ctx.graph_view.graph()->node(i)) ||
+        IsSigmoid(ctx.graph_view.graph()->node(i))) {
+      AddInputShapesAttr(ctx, i);
+    }
+
     if (IsMKLEnabled()) {
       // Remap Conv2D+BiasAdd+Add+relu into the _FusedConv2D.
       // or Remap Conv3D+BiasAdd+Add+relu into _FusedConv3D
@@ -4017,17 +4011,6 @@ Status Remapper::Optimize(Cluster* cluster, const GrapplerItem& item,
         continue;
       }
 
-      // Remap Mul(x, Sigmoid(x)) pattern, fuse them into the Swish(x).
-      std::map<string, int> sigmoidmul_matched_nodes_map;
-      std::set<int> sigmoidmul_remove_node_indices;
-      if (FindSigmoidAndMul(&ctx, i, &sigmoidmul_matched_nodes_map,
-                            &sigmoidmul_remove_node_indices)) {
-        TF_RETURN_IF_ERROR(ReplaceSigmoidMulWithSwish(
-            &ctx, sigmoidmul_matched_nodes_map, sigmoidmul_remove_node_indices,
-            &invalidated_nodes, &nodes_to_delete));
-        continue;
-      }
-
       // Remap smaller ops from layernorm python api into _MklLayerNorm
       matched_nodes_map.clear();
       remove_node_indices.clear();
diff --git a/tensorflow/tsl/platform/cpu_info.cc b/tensorflow/tsl/platform/cpu_info.cc
index fae0be99ac2..081ff0a49d6 100644
--- a/tensorflow/tsl/platform/cpu_info.cc
+++ b/tensorflow/tsl/platform/cpu_info.cc
@@ -22,6 +22,11 @@ limitations under the License.
 #if defined(PLATFORM_IS_X86)
 #include <mutex>  // NOLINT
 #endif
+#if defined(PLATFORM_IS_ARM64)
+#include <sys/auxv.h>
+
+#include <fstream>
+#endif
 
 // SIMD extension querying is only available on x86.
 #ifdef PLATFORM_IS_X86
@@ -345,6 +350,89 @@ void InitCPUIDInfo() {
 
 #endif  // PLATFORM_IS_X86
 
+#ifdef PLATFORM_IS_ARM64
+
+class CPUIDInfo;
+void InitCPUIDInfo();
+
+CPUIDInfo* cpuid = nullptr;
+
+// Structure for basic CPUID info.
+class CPUIDInfo {
+ public:
+  CPUIDInfo() : implementer_(0), variant_(0), cpunum_(0) {}
+
+  static void Initialize() {
+    // Initialize cpuid struct.
+    if (cpuid != nullptr) {
+      return;
+    }
+
+    cpuid = new CPUIDInfo;
+
+    if (!(getauxval(AT_HWCAP) & HWCAP_CPUID)) {
+      return;
+    }
+
+    std::ifstream CPUspresent;
+    CPUspresent.open("/sys/devices/system/cpu/present", std::ios::in);
+    int present_cpu = -1;
+    if (CPUspresent.is_open()) {
+      std::string line;
+      if (bool(getline(CPUspresent, line))) {
+        // We just need to find one CPU that is active
+        // from which we can read MIDR register to find
+        // implement, variant and revision information.
+        auto ending = line.end();
+        for (auto i = line.begin(); i < line.end(); ++i) {
+          if (*i == '-' || *i == ',') {
+            ending = i;
+            break;
+          }
+        }
+        line.erase(ending, line.end());
+        // That should be the fist number.
+        present_cpu = std::stoi(line);
+      }
+    }
+
+    if (present_cpu == -1) {
+      return;
+    }
+
+    std::stringstream str;
+    str << "/sys/devices/system/cpu/cpu" << present_cpu
+        << "/regs/identification/midr_el1";
+    std::ifstream midr_el1_file(str.str(), std::ios::in);
+    if (midr_el1_file.is_open()) {
+      std::string line;
+      if (bool(getline(midr_el1_file, line))) {
+        uint32 midr_el1 = std::stoul(line, nullptr, 16);
+
+        // Unpack variant and CPU ID.
+        cpuid->implementer_ = (midr_el1 >> 24) & 0xFF;
+        cpuid->variant_ = (midr_el1 >> 20) & 0xF;
+        cpuid->cpunum_ = (midr_el1 >> 4) & 0xFFF;
+      }
+    }
+  }
+
+  int implementer() const { return implementer_; }
+  int cpunum() const { return cpunum_; }
+
+ private:
+  int implementer_;
+  int variant_;
+  int cpunum_;
+};
+
+absl::once_flag cpuid_once_flag;
+
+void InitCPUIDInfo() {
+  absl::call_once(cpuid_once_flag, CPUIDInfo::Initialize);
+}
+
+#endif
 }  // namespace
 
 bool TestCPUFeature(CPUFeature feature) {
@@ -368,6 +456,9 @@ int CPUFamily() {
 #ifdef PLATFORM_IS_X86
   InitCPUIDInfo();
   return cpuid->family();
+#elif defined(PLATFORM_IS_ARM64)
+  InitCPUIDInfo();
+  return cpuid->implementer();
 #else
   return 0;
 #endif
@@ -377,6 +468,9 @@ int CPUModelNum() {
 #ifdef PLATFORM_IS_X86
   InitCPUIDInfo();
   return cpuid->model_num();
+#elif defined(PLATFORM_IS_ARM64)
+  InitCPUIDInfo();
+  return cpuid->cpunum();
 #else
   return 0;
 #endif
