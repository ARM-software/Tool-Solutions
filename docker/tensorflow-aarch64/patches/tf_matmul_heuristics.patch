 *******************************************************************************
 Copyright 2023 Arm Limited and affiliates.
 SPDX-License-Identifier: Apache-2.0

 Licensed under the Apache License, Version 2.0 (the "License");
 you may not use this file except in compliance with the License.
 You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License.
 *******************************************************************************
diff --git a/tensorflow/core/common_runtime/mkl_layout_pass.cc b/tensorflow/core/common_runtime/mkl_layout_pass.cc
index 025cba5f89c..efb0d110eea 100644
--- a/tensorflow/core/common_runtime/mkl_layout_pass.cc
+++ b/tensorflow/core/common_runtime/mkl_layout_pass.cc
@@ -1539,7 +1539,11 @@ class MklLayoutRewritePass : public GraphOptimizationPass {
     TF_CHECK_OK(GetNodeAttr(n->def(), "T", &T));
     if ((T == DT_FLOAT) || (T == DT_BFLOAT16)) {
       VLOG(2) << "Rewriting MatMul to _MklMatMul";
+#ifdef DNNL_AARCH64_USE_ACL
+      return MatMulHeuristic(n);
+#else
       return true;
+#endif
     }
     return false;
   }
@@ -1829,6 +1833,68 @@ class MklLayoutRewritePass : public GraphOptimizationPass {
     return true ? (total_mflops < 0 || total_mflops >= thr) : false;
   }
 
+#ifdef DNNL_AARCH64_USE_ACL
+  static bool MatMulHeuristic(const Node* n) {
+    // Check if we can obtain dimensions for this node.
+    std::vector<const TensorShapeProto*> shape_attrs;
+    if (!TryGetNodeAttr(n->attrs(), "_input_shapes", &shape_attrs)) {
+      // We can't obtain shape so we will revert to default behaviour
+      // to rewrite node.
+      return -1;
+    }
+
+    if ((n->type_string() == "MatMul" || n->type_string() == "_FusedMatMul")) {
+      TensorShape lhs_shape, rhs_shape;
+      if (TensorShape::BuildTensorShape(*shape_attrs[0], &lhs_shape) !=
+          tsl::OkStatus()) {
+        return -1;
+      }
+      if (TensorShape::BuildTensorShape(*shape_attrs[1], &rhs_shape) !=
+          tsl::OkStatus()) {
+        return -1;
+      }
+
+      auto M = lhs_shape.dim_size(0);
+      auto K = lhs_shape.dim_size(1);
+      auto N = rhs_shape.dim_size(1);
+      auto ops = M * N * K;
+      std::array<int, 3> n_threshold = {7560, 250, 1536};
+      std::array<int, 2> m_threshold = {378, 80};
+      std::array<int, 2> ops_threshold = {5242880, 1090519040};
+       if (N <= n_threshold.at(0)) {
+        if (ops <= ops_threshold.at(0)) {
+          if (M <= m_threshold.at(0)) {
+            return 0;
+          } else {
+            if (N <= n_threshold.at(1)) {
+              return 0;
+            } else {
+              return -1;
+            }
+          }
+        } else {
+          if (M <= m_threshold.at(1)) {
+            if (N <= n_threshold.at(2)) {
+              return -1;
+            } else {
+              return 0;
+            }
+          } else {
+            if (ops <= ops_threshold.at(1)) {
+              return -1;
+            } else {
+              return 0;
+            }
+          }
+        }
+      } else {
+        return 0;
+      }
+    }
+    return -1;
+  }
+ #endif
+
   static bool FusedConv2DRewrite(const Node* n, int threads) {
     // Decide whether it is worth rewriting it to oneDNN operation
     // due to overheads as they will dominate for small shapes.
diff --git a/tensorflow/core/graph/mkl_testlib.cc b/tensorflow/core/graph/mkl_testlib.cc
index 2029207ee43..05d0b67d3e1 100644
--- a/tensorflow/core/graph/mkl_testlib.cc
+++ b/tensorflow/core/graph/mkl_testlib.cc
@@ -23,19 +23,6 @@ namespace tensorflow {
 namespace test {
 namespace graph {
 
-Node* oneDNNMatmul(Graph* g, Node* in0, Node* in1, bool transpose_a,
-                   bool transpose_b) {
-  Node* ret = nullptr;
-  TF_CHECK_OK(NodeBuilder(g->NewName("n"), "_MklMatMul")
-                  .Input(in0)
-                  .Input(in1)
-                  .Attr("transpose_a", transpose_a)
-                  .Attr("transpose_b", transpose_b)
-                  .Attr("_kernel", mkl_op_registry::kMklNameChangeOpLabel)
-                  .Finalize(g, &ret));
-  return ret;
-}
-
 Node* oneDNNSoftmax(Graph* g, Node* input) {
   Node* ret = nullptr;
   TF_CHECK_OK(NodeBuilder(g->NewName("n"), "_MklSoftmax")
diff --git a/tensorflow/core/graph/mkl_testlib.h b/tensorflow/core/graph/mkl_testlib.h
index 7f3fb726c80..71a3758c73a 100644
--- a/tensorflow/core/graph/mkl_testlib.h
+++ b/tensorflow/core/graph/mkl_testlib.h
@@ -24,9 +24,6 @@ namespace tensorflow {
 namespace test {
 namespace graph {
 
-// Adds a _MklMatmul node in g doing in0.contract(in1).
-Node* oneDNNMatmul(Graph* g, Node* in0, Node* in1, bool transpose_a,
-                   bool transpose_b);
 
 Node* oneDNNSoftmax(Graph* g, Node* input);
 
diff --git a/tensorflow/core/grappler/optimizers/remapper.cc b/tensorflow/core/grappler/optimizers/remapper.cc
index e9488d3358f..3f85aed9bb3 100644
--- a/tensorflow/core/grappler/optimizers/remapper.cc
+++ b/tensorflow/core/grappler/optimizers/remapper.cc
@@ -2895,6 +2895,9 @@ void CopyMatMulAttributes(const NodeDef& matmul, NodeDef* fused_matmul,
     auto& activation_attr = activation->attr();
     (*attr)["leakyrelu_alpha"] = activation_attr.at("alpha");
   }
+  if (IsMKLEnabled() && src_attr.find("_input_shapes") != src_attr.end()) {
+    (*attr)["_input_shapes"] = src_attr.at("_input_shapes");
+  }
 }
 
 void CopyBatchMatMulAttributes(const NodeDef& batchmatmul,
@@ -2907,6 +2910,9 @@ void CopyBatchMatMulAttributes(const NodeDef& batchmatmul,
   (*attr)["T"] = src_attr.at("T");
   (*attr)["adj_x"] = src_attr.at("adj_x");
   (*attr)["adj_y"] = src_attr.at("adj_y");
+  if (IsMKLEnabled() && src_attr.find("_input_shapes") != src_attr.end()) {
+    (*attr)["_input_shapes"] = src_attr.at("_input_shapes");
+  }
 }
 
 void SetFusedOpAttributes(NodeDef* fused,
@@ -2946,6 +2952,7 @@ Status AddFusedContractionNode(RemapperContext* ctx,
     CopyDepthwiseConv2dNativeAttributes(contraction, &fused_op);
   } else if (IsMatMul(contraction)) {
     fused_op.set_op(kFusedMatMul);
+    AddInputShapesAttr(*ctx, matched.contraction);
     CopyMatMulAttributes(contraction, &fused_op);
   } else if (IsConv3D(contraction)) {
     fused_op.set_op(kFusedConv3D);
@@ -3051,6 +3058,7 @@ Status AddFusedContractionNode(
     CopyDepthwiseConv2dNativeAttributes(contraction, &fused_op);
   } else if (IsMatMul(contraction)) {
     fused_op.set_op(kFusedMatMul);
+    AddInputShapesAttr(*ctx, matched.contraction);
     CopyMatMulAttributes(contraction, &fused_op, &activation);
   } else if (IsConv3D(contraction)) {
     fused_op.set_op(kFusedConv3D);
@@ -3243,6 +3251,7 @@ Status AddFusedContractionNode(RemapperContext* ctx,
     CopyConv2DAttributes(contraction, &contraction_node);
   } else if (IsMatMul(contraction)) {
     contraction_node.set_op(kFusedMatMul);
+    AddInputShapesAttr(*ctx, matched.contraction);
     CopyMatMulAttributes(contraction, &contraction_node);
   } else if (IsConv3D(contraction)) {
     contraction_node.set_op(kFusedConv3D);
@@ -4467,7 +4476,8 @@ Status Remapper::Optimize(Cluster* cluster, const GrapplerItem& item,
         IsDepthwiseConv2dNative(ctx.graph_view.graph()->node(i)) ||
         IsBiasAdd(ctx.graph_view.graph()->node(i)) ||
         IsTranspose(ctx.graph_view.graph()->node(i)) ||
-        IsSigmoid(ctx.graph_view.graph()->node(i))) {
+        IsSigmoid(ctx.graph_view.graph()->node(i)) ||
+        IsMatMul(ctx.graph_view.graph()->node(i))) {
       AddInputShapesAttr(ctx, i);
     }
 
diff --git a/tensorflow/core/kernels/mkl/mkl_batch_matmul_op.cc b/tensorflow/core/kernels/mkl/mkl_batch_matmul_op.cc
index 3562e2c83da..eb3ab9fd6e1 100644
--- a/tensorflow/core/kernels/mkl/mkl_batch_matmul_op.cc
+++ b/tensorflow/core/kernels/mkl/mkl_batch_matmul_op.cc
@@ -208,6 +208,44 @@ class BatchMatMulMkl : public OpKernel {
             reinterpret_cast<Trhs*>(weights_mkl.GetOpMem().get_data_handle());
       }
     }
+    // Reorder src
+    Tlhs* src_data = const_cast<Tlhs*>(lhs.flat<Tlhs>().data());
+
+    memory::format_tag src_format;
+    switch (params->a_dims.size()) {
+      case 2:
+        src_format = adj_x_ ? memory::format_tag::ba : memory::format_tag::ab;
+        break;
+      case 3:
+        src_format = adj_x_ ? memory::format_tag::acb : memory::format_tag::abc;
+        break;
+      case 4:
+        src_format =
+            adj_x_ ? memory::format_tag::abdc : memory::format_tag::abcd;
+        break;
+      case 5:
+        src_format =
+            adj_x_ ? memory::format_tag::abced : memory::format_tag::abcde;
+        break;
+      default:
+        src_format = memory::format_tag::undef;
+    }
+    MklDnnData<Tlhs> src_mkl(&(this->cpu_engine_));
+    if (src_format != memory::format_tag::undef) {
+      auto src_md =
+          memory::desc(params->a_dims, MklDnnType<Tlhs>(), src_format);
+      std::shared_ptr<dnnl::matmul::primitive_desc> matmul_pd =
+          matmul_prim->GetPrimitiveDesc();
+      // Reorder src if necessary.
+      // Check whether we need to do reorder.
+      if (src_md != matmul_pd->src_desc()) {
+        src_mkl.SetUsrMem(src_md, src_data);
+        src_mkl.CheckReorderToOpMem(matmul_pd.get()->src_desc(),
+                                    this->cpu_engine_, ctx);
+        src_data =
+            reinterpret_cast<Tlhs*>(src_mkl.GetOpMem().get_data_handle());
+      }
+    }
 #endif  // DNNL_AARCH64_USE_ACL
 
     UserScratchPad<unsigned char> scratch_pad;
diff --git a/tensorflow/core/kernels/mkl/mkl_matmul_op_benchmark.cc b/tensorflow/core/kernels/mkl/mkl_matmul_op_benchmark.cc
index f05f8d112d4..ad2f565c4fd 100644
--- a/tensorflow/core/kernels/mkl/mkl_matmul_op_benchmark.cc
+++ b/tensorflow/core/kernels/mkl/mkl_matmul_op_benchmark.cc
@@ -16,7 +16,10 @@ limitations under the License.
 #if defined(INTEL_MKL) && !defined(ENABLE_ONEDNN_V3)
 
 #include "tensorflow/core/common_runtime/kernel_benchmark_testlib.h"
-#include "tensorflow/core/graph/mkl_testlib.h"
+#include "tensorflow/core/common_runtime/mkl_layout_pass.h"
+#include "tensorflow/core/graph/mkl_graph_util.h"
+#include "tensorflow/core/graph/node_builder.h"
+#include "tensorflow/core/util/util.h"
 
 namespace tensorflow {
 namespace {
@@ -29,9 +32,36 @@ static Graph* Matmul(int m, int k, int n, bool transpose_a, bool transpose_b,
   in0.flat<T>().setRandom();
   Tensor in1(type, transpose_b ? TensorShape({n, k}) : TensorShape({k, n}));
   in1.flat<T>().setRandom();
-  test::graph::oneDNNMatmul(g, test::graph::Constant(g, in0),
-                            test::graph::Constant(g, in1), transpose_a,
-                            transpose_b);
+  Node* src0 = test::graph::Constant(g, in0);
+  Node* src1 = test::graph::Constant(g, in1);
+  g->AddEdge(g->source_node(), 0, src0, 0);
+  g->AddEdge(g->source_node(), 1, src1, 0);
+  // Add shape sizes
+  AttrValue attr_input_shape;
+  TensorShapeProto* proto = attr_input_shape.mutable_list()->add_shape();
+  proto->add_dim()->set_size(m);
+  proto->add_dim()->set_size(k);
+  proto = attr_input_shape.mutable_list()->add_shape();
+  proto->add_dim()->set_size(k);
+  proto->add_dim()->set_size(n);
+
+  Node* ret = nullptr;
+
+  TF_CHECK_OK(NodeBuilder(g->NewName("matmul"), "MatMul")
+                  .Input(src0)
+                  .Input(src1)
+                  .Attr("transpose_a", transpose_a)
+                  .Attr("transpose_b", transpose_b)
+                  .Attr("_input_shapes", attr_input_shape)
+                  .Finalize(g, &ret));
+  int ret_id = ret->id();
+  if (IsMKLEnabled()) {
+    std::unique_ptr<Graph>* ug = new std::unique_ptr<Graph>(g);
+    RunMklLayoutRewritePass(ug);
+    if (!g->FindNodeId(ret_id)) {
+      ret = g->FindNodeId(ret_id + 1);
+    }
+  }
   return g;
 }
 
diff --git a/tensorflow/core/util/mkl_heuristics.h b/tensorflow/core/util/mkl_heuristics.h
index 162dbe81331..9b46a847a8b 100644
--- a/tensorflow/core/util/mkl_heuristics.h
+++ b/tensorflow/core/util/mkl_heuristics.h
@@ -25,7 +25,7 @@ limitations under the License.
 namespace tensorflow {
 
 struct RewriteThreshold {
-  string op;
+  std::string op;
   int cpu_family;
   int cpu_model_num;
   // The model that is used to decide whether it is worth
@@ -55,7 +55,7 @@ static const RewriteThreshold rewrite_thresholds[] = {
 #endif  // DNNL_AARCH64_USE_ACL
     {"", 0x0, 0x0, {0, 0}}};
 
-static double FindRewriteThreshold(const string node_name, int threads) {
+static double FindRewriteThreshold(const std::string node_name, int threads) {
   int cpu_family_ = tsl::port::CPUFamily();
   int cpu_model_num_ = tsl::port::CPUModelNum();
 
@@ -77,7 +77,7 @@ static double FindRewriteThreshold(const string node_name, int threads) {
 }
 
 static double CalculateNodeMFlops(const AttrSlice& attrs,
-                                  const string node_name) {
+                                  const std::string node_name) {
   // Check if we can obtained dimensions for this node.
   std::vector<const TensorShapeProto*> shape_attrs;
   if (!TryGetNodeAttr(attrs, "_input_shapes", &shape_attrs)) {
diff --git a/tensorflow/core/util/mkl_heuristics_test.cc b/tensorflow/core/util/mkl_heuristics_test.cc
index 5dfdd1fa24a..47d1394bca5 100644
--- a/tensorflow/core/util/mkl_heuristics_test.cc
+++ b/tensorflow/core/util/mkl_heuristics_test.cc
@@ -16,12 +16,15 @@ limitations under the License.
 #ifdef INTEL_MKL
 #define EIGEN_USE_THREADS
 
-#include "tensorflow/core/util/mkl_heuristics.h"
+#include <vector>
 
 #include "tensorflow/core/framework/node_def.pb.h"
+#include "tensorflow/core/framework/node_def_util.h"
 #include "tensorflow/core/kernels/ops_testutil.h"
 #include "tensorflow/core/platform/test.h"
 
+#include "tensorflow/core/util/mkl_heuristics.h"
+
 namespace tensorflow {
 
 namespace {
