 *******************************************************************************
 Copyright 2023-2024 Arm Limited and affiliates.
 SPDX-License-Identifier: Apache-2.0

 Licensed under the Apache License, Version 2.0 (the "License");
 you may not use this file except in compliance with the License.
 You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License.
 *******************************************************************************

diff --git a/src/cpu/aarch64/acl_quantization.hpp b/src/cpu/aarch64/acl_quantization.hpp
new file mode 100644
index 000000000..726364961
--- /dev/null
+++ b/src/cpu/aarch64/acl_quantization.hpp
@@ -0,0 +1,170 @@
+/*******************************************************************************
+* Copyright 2023 Arm Ltd. and affiliates
+*
+* Licensed under the Apache License, Version 2.0 (the "License");
+* you may not use this file except in compliance with the License.
+* You may obtain a copy of the License at
+*
+*     http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing, software
+* distributed under the License is distributed on an "AS IS" BASIS,
+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+* See the License for the specific language governing permissions and
+* limitations under the License.
+*******************************************************************************/
+
+#ifndef CPU_AARCH64_ACL_QUANTIZATION_HPP
+#define CPU_AARCH64_ACL_QUANTIZATION_HPP
+
+#include "cpu/reorder/cpu_reorder_pd.hpp"
+
+#include "acl_utils.hpp"
+#include "arm_compute/runtime/NEON/functions/NEQuantizationLayer.h"
+
+namespace dnnl {
+namespace impl {
+namespace cpu {
+namespace aarch64 {
+
+struct acl_quantization_obj_t {
+    arm_compute::NEQuantizationLayer quant;
+    arm_compute::Tensor src_tensor;
+    arm_compute::Tensor dst_tensor;
+};
+
+struct acl_quantization_conf_t {
+    arm_compute::TensorInfo src_tensor_info;
+    arm_compute::TensorInfo dst_tensor_info;
+};
+
+struct acl_quantization_resource_t : public resource_t {
+    acl_quantization_resource_t()
+        :  acl_obj_(utils::make_unique<
+                acl_quantization_obj_t>()) {}
+
+    status_t configure(const acl_quantization_conf_t &aqc) {
+        if (!acl_obj_) return status::out_of_memory;
+
+        acl_obj_->src_tensor.allocator()->init(aqc.src_tensor_info);
+        acl_obj_->dst_tensor.allocator()->init(aqc.dst_tensor_info);
+        acl_obj_->quant.configure(
+            &acl_obj_->src_tensor,
+            &acl_obj_->dst_tensor);
+
+        return status::success;
+    }
+
+    acl_quantization_obj_t &get_acl_obj() const {
+        return *acl_obj_;
+    }
+
+    DNNL_DISALLOW_COPY_AND_ASSIGN(acl_quantization_resource_t);
+
+private:
+    std::unique_ptr<acl_quantization_obj_t> acl_obj_;
+
+};
+
+struct acl_quantization_t : public primitive_t {
+    using primitive_t::primitive_t;
+    struct pd_t : public cpu_reorder_pd_t {
+        using cpu_reorder_pd_t::cpu_reorder_pd_t;
+
+        pd_t(const primitive_attr_t *attr, engine_kind_t src_engine_kind,
+            const memory_desc_t *src_md, engine_kind_t dst_engine_kind,
+            const memory_desc_t *dst_md)
+            : cpu_reorder_pd_t(attr, src_engine_kind, src_md, dst_engine_kind, dst_md), aqc_() {}
+
+        DECLARE_COMMON_PD_T("quantization:acl", acl_quantization_t);
+
+        status_t init(engine_t *engine) {
+            const memory_desc_wrapper src_d(src_md_);
+            aqc_.src_tensor_info = arm_compute::TensorInfo(
+                arm_compute::TensorShape(src_d.dims()[1], src_d.dims()[0]),
+                arm_compute::Format::F32);
+
+            const memory_desc_wrapper dst_d(dst_md_);
+            aqc_.dst_tensor_info = arm_compute::TensorInfo(
+                arm_compute::TensorShape(dst_d.dims()[1], dst_d.dims()[0]),
+                1,
+                arm_compute::DataType::QASYMM8_SIGNED);
+
+            return status::success;
+        }
+
+        acl_quantization_conf_t aqc_;
+
+    private:
+        static status_t create(reorder_pd_t **reorder_pd, engine_t *engine,
+                const primitive_attr_t *attr, engine_t *src_engine,
+                const memory_desc_t *src_md, engine_t *dst_engine,
+                const memory_desc_t *dst_md) {
+                    auto _pd = make_unique_pd<pd_t>(
+                            attr, src_engine->kind(), src_md, dst_engine->kind(), dst_md);
+                    if(_pd == nullptr) return status::out_of_memory;
+                    _pd->init(engine);
+                    _pd->init_scratchpad_md();
+
+                    return safe_ptr_assign(*reorder_pd, _pd.release());
+                }
+
+        friend dnnl::impl::impl_list_item_t;
+    };
+
+    status_t create_resource(
+            engine_t *engine, resource_mapper_t &mapper) const {
+        if (mapper.has_resource(this)) return status::success;
+
+        auto r = utils::make_unique<acl_quantization_resource_t>();
+        if (!r) return status::out_of_memory;
+
+        CHECK(r->configure(pd()->aqc_));
+        mapper.add(this, std::move(r));
+
+        return status::success;
+    }
+
+
+    status_t execute(const exec_ctx_t &ctx) const  {
+        std::lock_guard<std::mutex> _lock {this->mtx};
+
+        auto *acl_resource
+                = ctx.get_resource_mapper()
+                            ->get<acl_quantization_resource_t>(this);
+        acl_quantization_obj_t &acl_quantization_obj = acl_resource->get_acl_obj();
+
+        auto src = CTX_IN_MEM(const float*, DNNL_ARG_SRC);
+        auto dst = CTX_OUT_MEM(int8_t*, DNNL_ARG_DST);
+
+        acl_quantization_obj.src_tensor.allocator()->import_memory(
+            const_cast<float *>(src));
+        acl_quantization_obj.dst_tensor.allocator()->import_memory(dst);
+
+        auto scale = CTX_IN_MEM(const float *, DNNL_ARG_ATTR_SCALES | DNNL_ARG_DST);
+        auto zero_point = CTX_IN_MEM(const int32_t*, DNNL_ARG_ATTR_ZERO_POINTS | DNNL_ARG_DST);
+
+        acl_quantization_obj.dst_tensor.info()->set_quantization_info(
+            arm_compute::QuantizationInfo(*scale, *zero_point));
+
+        acl_quantization_obj.quant.run();
+
+        acl_quantization_obj.src_tensor.allocator()->free();
+        acl_quantization_obj.dst_tensor.allocator()->free();
+
+        return status::success;
+    };
+
+    acl_quantization_t(const pd_t *apd) : primitive_t(apd) {}
+
+    private:
+        mutable std::mutex mtx;
+        const pd_t *pd() const { return (const pd_t*)primitive_t::pd().get(); }
+};
+
+} // aarch64
+} // cpu
+} // impl
+} // dnnl
+
+#endif // CPU_AARCH64_ACL_QUANTIZATION_HPP
\ No newline at end of file
diff --git a/src/cpu/aarch64/matmul/acl_lowp_matmul.hpp b/src/cpu/aarch64/matmul/acl_lowp_matmul.hpp
new file mode 100644
index 000000000..cbdd0165b
--- /dev/null
+++ b/src/cpu/aarch64/matmul/acl_lowp_matmul.hpp
@@ -0,0 +1,217 @@
+/*******************************************************************************
+* Copyright 2023 Arm Ltd. and affiliates
+*
+* Licensed under the Apache License, Version 2.0 (the "License");
+* you may not use this file except in compliance with the License.
+* You may obtain a copy of the License at
+*
+*     http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing, software
+* distributed under the License is distributed on an "AS IS" BASIS,
+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+* See the License for the specific language governing permissions and
+* limitations under the License.
+*******************************************************************************/
+
+#ifndef ACL_LOWP_MATMUL_HPP
+#define ACL_LOWP_MATMUL_HPP
+
+#include "cpu/matmul/cpu_matmul_pd.hpp"
+
+#include "arm_compute/runtime/NEON/functions/NEGEMMLowpMatrixMultiplyCore.h"
+#include "arm_compute/runtime/NEON/functions/NEDequantizationLayer.h"
+
+namespace dnnl {
+namespace impl {
+namespace cpu {
+namespace aarch64 {
+namespace matmul {
+
+struct acl_lowp_matmul_obj_t {
+    arm_compute::NEGEMMLowpMatrixMultiplyCore gemm;
+    arm_compute::NEDequantizationLayer dequant;
+    arm_compute::Tensor src_tensor;
+    arm_compute::Tensor wei_tensor;
+    arm_compute::Tensor bia_tensor;
+    arm_compute::Tensor dst_tensor;
+    arm_compute::Tensor dst_dequant_tensor;
+};
+
+struct acl_lowp_matmul_conf_t {
+    arm_compute::TensorInfo src_tensor_info;
+    arm_compute::TensorInfo wei_tensor_info;
+    bool with_bias;
+    arm_compute::TensorInfo bia_tensor_info;
+    arm_compute::TensorInfo dst_tensor_info;
+    arm_compute::TensorInfo dst_dequant_tensor_info;
+};
+
+struct acl_lowp_matmul_resource_t : public resource_t {
+    acl_lowp_matmul_resource_t()
+        :  acl_obj_(utils::make_unique<
+                acl_lowp_matmul_obj_t>()) {}
+
+    status_t configure(const acl_lowp_matmul_conf_t &almc) {
+
+        if (!acl_obj_) return status::out_of_memory;
+
+        acl_obj_->src_tensor.allocator()->init(almc.src_tensor_info);
+        acl_obj_->wei_tensor.allocator()->init(almc.wei_tensor_info);
+        if(almc.with_bias) {
+            acl_obj_->bia_tensor.allocator()->init(almc.bia_tensor_info);
+        }
+        acl_obj_->dst_tensor.allocator()->init(almc.dst_tensor_info);
+
+        acl_obj_->gemm.configure(
+            &acl_obj_->src_tensor,
+            &acl_obj_->wei_tensor,
+            almc.with_bias ? &acl_obj_->bia_tensor : nullptr,
+            &acl_obj_->dst_tensor);
+
+        return status::success;
+    }
+
+    acl_lowp_matmul_obj_t &get_acl_obj() const {
+        return *acl_obj_;
+    }
+
+    DNNL_DISALLOW_COPY_AND_ASSIGN(acl_lowp_matmul_resource_t);
+
+private:
+    std::unique_ptr<acl_lowp_matmul_obj_t> acl_obj_;
+
+};
+
+struct acl_lowp_matmul_t : public primitive_t {
+    struct pd_t : public dnnl::impl::cpu::matmul::cpu_matmul_pd_t {
+
+        pd_t(const matmul_desc_t *adesc, const primitive_attr_t *attr,
+                const cpu_matmul_pd_t *hint_fwd_pd)
+            : cpu_matmul_pd_t(adesc, attr, hint_fwd_pd), almc_() {}
+
+        using cpu_matmul_pd_t::cpu_matmul_pd_t;
+
+        DECLARE_COMMON_PD_T("lowp_gemm:acl", acl_lowp_matmul_t, USE_GLOBAL_SCRATCHPAD);
+
+        status_t init(engine_t *engine) {
+
+            if(src_md()->data_type != data_type::s8 ||
+                weights_md()->data_type != data_type::s8 ||
+                dst_md()->data_type != data_type::f32) {
+                    return status::unimplemented;
+                }
+            memory_desc_init_by_tag(bias_md_, bias_md_.ndims, bias_md_.dims, bias_md_.data_type, dnnl_ab); 
+
+            const memory_desc_wrapper src_d(src_md_);
+            almc_.src_tensor_info = arm_compute::TensorInfo(
+                arm_compute::TensorShape(src_d.dims()[1], src_d.dims()[0]),
+                1,
+                arm_compute::DataType::QASYMM8_SIGNED);
+
+            const memory_desc_wrapper wei_d(weights_md_);
+            almc_.wei_tensor_info = arm_compute::TensorInfo(
+                arm_compute::TensorShape(wei_d.dims()[1], wei_d.dims()[0]),
+                1,
+                arm_compute::DataType::QASYMM8_SIGNED);
+
+            const memory_desc_wrapper bia_d(bias_md_);
+            almc_.with_bias = desc()->bias_desc.format_kind != format_kind::undef;
+            almc_.bia_tensor_info = arm_compute::TensorInfo(
+                almc_.with_bias ? arm_compute::TensorShape(bia_d.dims()[1], bia_d.dims()[0])
+                                : arm_compute::TensorShape(),
+                1,
+                arm_compute::DataType::F32);
+
+            const memory_desc_wrapper dst_d(dst_md_);
+            almc_.dst_tensor_info = arm_compute::TensorInfo(
+                arm_compute::TensorShape(dst_d.dims()[1], dst_d.dims()[0]),
+                arm_compute::Format::F32);
+
+            return status::success;
+        }
+
+        acl_lowp_matmul_conf_t almc_;
+    };
+
+    acl_lowp_matmul_t(const pd_t *apd) : primitive_t(apd) {}
+
+    status_t create_resource(
+            engine_t *engine, resource_mapper_t &mapper) const {
+
+        if (mapper.has_resource(this)) return status::success;
+
+        auto r = utils::make_unique<acl_lowp_matmul_resource_t>();
+        if (!r) return status::out_of_memory;
+
+        CHECK(r->configure(pd()->almc_));
+
+        mapper.add(this, std::move(r));
+
+        return status::success;
+    }
+
+
+    status_t execute(const exec_ctx_t &ctx) const  {
+        std::lock_guard<std::mutex> _lock {this->mtx};
+
+        bool with_bias = pd()->almc_.with_bias;
+
+        auto *acl_resource
+                = ctx.get_resource_mapper()
+                            ->get<acl_lowp_matmul_resource_t>(this);
+        acl_lowp_matmul_obj_t &acl_lowp_matmul_obj = acl_resource->get_acl_obj();
+
+        auto src = CTX_IN_MEM(const int8_t*, DNNL_ARG_SRC);
+        auto wei = CTX_IN_MEM(const int8_t*, DNNL_ARG_WEIGHTS);
+        auto dst = CTX_OUT_MEM(float*, DNNL_ARG_DST);
+
+        acl_lowp_matmul_obj.src_tensor.allocator()->import_memory(
+            const_cast<int8_t *>(src));
+        acl_lowp_matmul_obj.wei_tensor.allocator()->import_memory(
+            const_cast<int8_t *>(wei));
+        if(with_bias) {
+            auto bias = CTX_IN_MEM(const float*, DNNL_ARG_BIAS);
+            acl_lowp_matmul_obj.bia_tensor.allocator()->import_memory(
+                const_cast<float*>(bias));
+        }
+        acl_lowp_matmul_obj.dst_tensor.allocator()->import_memory(dst);
+
+        auto src_scale = CTX_IN_MEM(const float *, DNNL_ARG_ATTR_SCALES | DNNL_ARG_SRC_0);
+        auto src_zero_point = CTX_IN_MEM(const int32_t*, DNNL_ARG_ATTR_ZERO_POINTS | DNNL_ARG_SRC_0);
+        auto wei_scale = CTX_IN_MEM(const float *, DNNL_ARG_ATTR_SCALES | DNNL_ARG_WEIGHTS);
+
+        float src_scale_val = src_scale ? *src_scale : 1;
+        int32_t src_zero_point_val = src_zero_point ? *src_zero_point : 0;
+        float wei_scale_val = wei_scale ? *wei_scale : 1;
+
+
+        acl_lowp_matmul_obj.src_tensor.info()->set_quantization_info(
+            arm_compute::QuantizationInfo(src_scale_val, -1 * src_zero_point_val));
+        acl_lowp_matmul_obj.wei_tensor.info()->set_quantization_info(
+            arm_compute::QuantizationInfo(wei_scale_val, 0));
+
+        acl_lowp_matmul_obj.dst_tensor.info()->set_quantization_info(
+            arm_compute::QuantizationInfo(src_scale_val*wei_scale_val, src_zero_point_val));
+
+        acl_lowp_matmul_obj.gemm.run();
+
+        acl_lowp_matmul_obj.src_tensor.allocator()->free();
+        acl_lowp_matmul_obj.wei_tensor.allocator()->free();
+        acl_lowp_matmul_obj.dst_tensor.allocator()->free();
+
+        return status::success;
+    };
+
+    private:
+        mutable std::mutex mtx;
+        const pd_t *pd() const { return (const pd_t*)primitive_t::pd().get(); }
+};
+
+} // matmul
+} // aarch64
+} // cpu
+} // impl
+} // dnnl
+
+#endif // CPU_AARCH64_ACL_LOWP_MATMUL_HPP
\ No newline at end of file
diff --git a/src/cpu/aarch64/matmul/acl_matmul.hpp b/src/cpu/aarch64/matmul/acl_matmul.hpp
index 451cc78d5..6d848abe5 100644
--- a/src/cpu/aarch64/matmul/acl_matmul.hpp
+++ b/src/cpu/aarch64/matmul/acl_matmul.hpp
@@ -79,7 +79,7 @@ struct acl_matmul_t : public primitive_t {
             bool ok = is_dense_data()
                     && utils::one_of(true, is_fp32_ok, is_fp16_ok)
                     && !has_zero_dim_memory()
-                    && weights_md_.format_kind == format_kind::any
+                    //&& weights_md_.format_kind == format_kind::any
                     && set_default_formats()
                     && attr()->has_default_values(
                             smask_t::oscale | smask_t::post_ops)
diff --git a/src/cpu/matmul/cpu_matmul_list.cpp b/src/cpu/matmul/cpu_matmul_list.cpp
index e1fd76c6c..c997c99e4 100644
--- a/src/cpu/matmul/cpu_matmul_list.cpp
+++ b/src/cpu/matmul/cpu_matmul_list.cpp
@@ -31,6 +31,7 @@ using namespace dnnl::impl::cpu::x64::matmul;
 using namespace dnnl::impl::cpu::x64;
 #elif DNNL_AARCH64 && DNNL_AARCH64_USE_ACL
 #include "cpu/aarch64/matmul/acl_matmul.hpp"
+#include "cpu/aarch64/matmul/acl_lowp_matmul.hpp"
 using namespace dnnl::impl::cpu::aarch64::matmul;
 using namespace dnnl::impl::cpu::aarch64;
 
@@ -67,6 +68,7 @@ using namespace dnnl::impl::cpu::matmul;
 
 // clang-format off
 constexpr impl_list_item_t impl_list[] = REG_MATMUL_P({
+        CPU_INSTANCE_AARCH64_ACL(acl_lowp_matmul_t)
         CPU_INSTANCE_AARCH64_ACL(acl_matmul_t)
         CPU_INSTANCE_AMX(brgemm_matmul_t<avx512_core_amx_fp16>)
         CPU_INSTANCE_AMX(brgemm_matmul_t<avx512_core_amx>)
diff --git a/src/cpu/reorder/cpu_reorder_regular_f32_s8.cpp b/src/cpu/reorder/cpu_reorder_regular_f32_s8.cpp
index 6bd305c7b..f6015334c 100644
--- a/src/cpu/reorder/cpu_reorder_regular_f32_s8.cpp
+++ b/src/cpu/reorder/cpu_reorder_regular_f32_s8.cpp
@@ -16,6 +16,9 @@
 *******************************************************************************/
 
 #include "cpu/reorder/cpu_reorder.hpp"
+#if DNNL_AARCH64 && DNNL_AARCH64_USE_ACL
+#include "cpu/aarch64/acl_quantization.hpp"
+#endif // DNNL_AARCH64 && DNNL_AARCH64_USE_ACL
 
 namespace dnnl {
 namespace impl {
@@ -36,6 +39,7 @@ const impl_list_map_t &regular_f32_s8_impl_list_map() {
             DNNL_X64_ONLY(CPU_REORDER_INSTANCE(x64::jit_blk_reorder_t))
             DNNL_X64_ONLY(CPU_REORDER_INSTANCE(x64::jit_uni_reorder_t))
 
+            DNNL_AARCH64_ACL_ONLY(CPU_REORDER_INSTANCE(aarch64::acl_quantization_t))
             DNNL_AARCH64_ONLY(CPU_REORDER_INSTANCE(aarch64::jit_blk_reorder_t))
             DNNL_AARCH64_ONLY(CPU_REORDER_INSTANCE(aarch64::jit_uni_reorder_t))
 
