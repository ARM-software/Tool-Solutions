 *******************************************************************************
 Copyright 2021 Arm Limited and affiliates.
 SPDX-License-Identifier: Apache-2.0

 Licensed under the Apache License, Version 2.0 (the "License");
 you may not use this file except in compliance with the License.
 You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License.
 *******************************************************************************
diff --git a/doc/performance_considerations/verbose.md b/doc/performance_considerations/verbose.md
index d5e739238..fbc5f7224 100644
--- a/doc/performance_considerations/verbose.md
+++ b/doc/performance_considerations/verbose.md
@@ -128,6 +128,12 @@ synchronization on entry and on exit in the dnnl::primitive::execute() call.
 The execution time is calculated based on wall time measured before and after
 primitive execution.
 
+@note
+When oneDNN verbose mode is enabled for builds with
+[Compute Library for the Arm architecture](https://oneapi-src.github.io/oneDNN/dev_guide_build.html#gcc-with-arm-compute-library-acl-on-aarch64-host),
+any failures in the validation of Compute Library primitives will be detailed
+in the verbose output.
+
 @warning
 Verbose mode has non-negligible performance impact especially on GPU or if the
 output rate is high.
diff --git a/src/cpu/aarch64/acl_convolution_utils.cpp b/src/cpu/aarch64/acl_convolution_utils.cpp
index d27c58d7e..ca91de49e 100644
--- a/src/cpu/aarch64/acl_convolution_utils.cpp
+++ b/src/cpu/aarch64/acl_convolution_utils.cpp
@@ -214,6 +214,33 @@ status_t acl_init_conf(acl_conv_conf_t &acp, memory_desc_t &src_md,
             && post_ops.entry_[1].is_eltwise(true);
     acp.act_info = acl_common_utils::get_acl_act(attr);
 
+    if (acp.sum_with_eltwise) {
+        // clang-format off
+        // Validate activation layer manually to check for return status
+        auto acl_al_st = arm_compute::NEActivationLayer::validate(
+            &acp.dst_info,
+            &acp.dst_info,
+            acp.act_info);
+        // clang-format on
+        if (acl_al_st.error_code() != arm_compute::ErrorCode::OK) {
+            MAYBE_REPORT_ACL_ERROR(acl_al_st.error_description().c_str());
+            return status::unimplemented;
+        }
+
+        // clang-format off
+        // Validate arithmetic addition manually to check for return status
+        auto acl_aa_st = arm_compute::NEArithmeticAddition::validate(
+            &acp.dst_info,
+            &acp.dst_info,
+            &acp.dst_info,
+            arm_compute::ConvertPolicy::SATURATE);
+        // clang-format on
+        if (acl_aa_st.error_code() != arm_compute::ErrorCode::OK) {
+            MAYBE_REPORT_ACL_ERROR(acl_aa_st.error_description().c_str());
+            return status::unimplemented;
+        }
+    }
+
     return status::success;
 }
 
@@ -239,6 +266,7 @@ status_t init_conf_gemm(acl_conv_conf_t &acp, memory_desc_t &src_md,
         acp.fast_math);
     // clang-format on
     if (acl_st.error_code() != arm_compute::ErrorCode::OK) {
+        MAYBE_REPORT_ACL_ERROR(acl_st.error_description().c_str());
         return status::unimplemented;
     }
 
@@ -273,6 +301,7 @@ status_t init_conf_indirect_gemm(acl_conv_conf_t &acp, memory_desc_t &src_md,
                                 1));
     // clang-format on
     if (acl_st.error_code() != arm_compute::ErrorCode::OK) {
+        MAYBE_REPORT_ACL_ERROR(acl_st.error_description().c_str());
         return status::unimplemented;
     }
 
@@ -317,6 +346,7 @@ status_t init_conf_wino(acl_conv_conf_t &acp, memory_desc_t &src_md,
         true); // enable_fast_math flag in ACL Winograd
     // clang-format on
     if (acl_st.error_code() != arm_compute::ErrorCode::OK) {
+        MAYBE_REPORT_ACL_ERROR(acl_st.error_description().c_str());
         return status::unimplemented;
     }
 
diff --git a/src/cpu/aarch64/acl_inner_product_utils.cpp b/src/cpu/aarch64/acl_inner_product_utils.cpp
index 8a8d1cc93..66e8dc643 100644
--- a/src/cpu/aarch64/acl_inner_product_utils.cpp
+++ b/src/cpu/aarch64/acl_inner_product_utils.cpp
@@ -134,19 +134,41 @@ status_t init_conf_ip(acl_ip_conf_t &aip, memory_desc_t &src_md,
     const auto &post_ops = attr.post_ops_;
     aip.with_sum = (post_ops.len() == 1) && post_ops.entry_[0].is_sum();
 
+    // Fast math mode
+    auto math_mode = get_fpmath_mode();
+    bool is_fastmath_enabled = one_of(math_mode, fpmath_mode::bf16, fpmath_mode::any);
+    aip.fc_info.enable_fast_math = is_fastmath_enabled;
+
     // clang-format off
-    // Validate convolution manually to check for return status
+    // Validate fully connected layer manually to check for return status
     auto acl_st = arm_compute::NEFullyConnectedLayer::validate(
         &aip.src_info,
         &aip.wei_info,
         aip.with_bias ? &aip.bia_info : nullptr,
         &aip.dst_info,
-	aip.fc_info);
+        aip.fc_info);
     // clang-format on
     if (acl_st.error_code() != arm_compute::ErrorCode::OK) {
+        MAYBE_REPORT_ACL_ERROR(acl_st.error_description().c_str());
         return status::unimplemented;
     }
 
+    if (aip.with_sum) {
+        // clang-format off
+        // Validate arithmetic addition manually to check for return status
+        auto acl_aa_st = arm_compute::NEArithmeticAddition::validate(
+            &aip.dst_info,
+            &aip.dst_info,
+            &aip.dst_info,
+            arm_compute::ConvertPolicy::SATURATE);
+
+        // clang-format on
+        if (acl_aa_st.error_code() != arm_compute::ErrorCode::OK) {
+            MAYBE_REPORT_ACL_ERROR(acl_aa_st.error_description().c_str());
+            return status::unimplemented;
+        }
+    }
+
     return status::success;
 }
 
diff --git a/src/cpu/aarch64/acl_softmax.cpp b/src/cpu/aarch64/acl_softmax.cpp
new file mode 100644
index 000000000..81935282f
--- /dev/null
+++ b/src/cpu/aarch64/acl_softmax.cpp
@@ -0,0 +1,56 @@
+/*******************************************************************************
+* Copyright 2021 Arm Ltd. and affiliates
+*
+* Licensed under the Apache License, Version 2.0 (the "License");
+* you may not use this file except in compliance with the License.
+* You may obtain a copy of the License at
+*
+*     http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing, software
+* distributed under the License is distributed on an "AS IS" BASIS,
+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+* See the License for the specific language governing permissions and
+* limitations under the License.
+*******************************************************************************/
+
+#include "cpu/aarch64/acl_softmax.hpp"
+
+namespace dnnl {
+namespace impl {
+namespace cpu {
+namespace aarch64 {
+
+template <impl::data_type_t data_type>
+status_t acl_softmax_fwd_t<data_type>::execute_forward(
+        const exec_ctx_t &ctx) const {
+
+    // Lock here is needed because resource_mapper does not support
+    // concurrent multithreaded access.
+    std::lock_guard<std::mutex> _lock {this->mtx};
+
+    auto src = CTX_IN_MEM(const data_t *, DNNL_ARG_SRC);
+    auto dst = CTX_OUT_MEM(data_t *, DNNL_ARG_DST);
+
+    // Retrieve primitive resource and configured Compute Library objects
+    auto *acl_resource
+            = ctx.get_resource_mapper()->get<acl_softmax_resource_t>(this);
+    acl_softmax_obj_t &acl_obj = acl_resource->get_acl_obj();
+
+    acl_obj.src_tensor.allocator()->import_memory(const_cast<data_t *>(src));
+    acl_obj.dst_tensor.allocator()->import_memory(dst);
+
+    acl_obj.softmax.run();
+
+    acl_obj.src_tensor.allocator()->free();
+    acl_obj.dst_tensor.allocator()->free();
+
+    return status::success;
+}
+
+template struct acl_softmax_fwd_t<data_type::f32>;
+
+} // namespace aarch64
+} // namespace cpu
+} // namespace impl
+} // namespace dnnl
diff --git a/src/cpu/aarch64/acl_softmax.hpp b/src/cpu/aarch64/acl_softmax.hpp
new file mode 100644
index 000000000..36cc73c00
--- /dev/null
+++ b/src/cpu/aarch64/acl_softmax.hpp
@@ -0,0 +1,216 @@
+/*******************************************************************************
+* Copyright 2021 Arm Ltd. and affiliates
+*
+* Licensed under the Apache License, Version 2.0 (the "License");
+* you may not use this file except in compliance with the License.
+* You may obtain a copy of the License at
+*
+*     http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing, software
+* distributed under the License is distributed on an "AS IS" BASIS,
+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+* See the License for the specific language governing permissions and
+* limitations under the License.
+*******************************************************************************/
+
+#ifndef CPU_AARCH64_ACL_SOFTMAX_HPP
+#define CPU_AARCH64_ACL_SOFTMAX_HPP
+
+#include "cpu/cpu_softmax_pd.hpp"
+
+#include "cpu/aarch64/acl_utils.hpp"
+
+namespace dnnl {
+namespace impl {
+namespace cpu {
+namespace aarch64 {
+
+struct acl_softmax_obj_t {
+    arm_compute::NESoftmaxLayer softmax;
+    arm_compute::Tensor src_tensor;
+    arm_compute::Tensor dst_tensor;
+};
+
+struct acl_softmax_conf_t {
+    arm_compute::TensorInfo src_info;
+    arm_compute::TensorInfo dst_info;
+    float beta;
+    int32_t axis;
+};
+
+struct acl_softmax_resource_t : public resource_t {
+    acl_softmax_resource_t()
+        : acl_obj_(utils::make_unique<acl_softmax_obj_t>()) {}
+
+    status_t configure(const acl_softmax_conf_t &asp) {
+        if (!acl_obj_) return status::out_of_memory;
+
+        // Init Compute Library tensors based on info from descriptor
+        acl_obj_->src_tensor.allocator()->init(asp.src_info);
+        acl_obj_->dst_tensor.allocator()->init(asp.dst_info);
+        // clang-format off
+        acl_obj_->softmax.configure(
+            &acl_obj_->src_tensor,
+            &acl_obj_->dst_tensor,
+            asp.beta,
+            asp.axis);
+        // clang-format on
+        return status::success;
+    }
+
+    acl_softmax_obj_t &get_acl_obj() const { return *acl_obj_; }
+
+    DNNL_DISALLOW_COPY_AND_ASSIGN(acl_softmax_resource_t);
+
+private:
+    std::unique_ptr<acl_softmax_obj_t> acl_obj_;
+}; // acl_softmax_resource_t
+
+template <impl::data_type_t data_type>
+struct acl_softmax_fwd_t : public primitive_t {
+    struct pd_t : public cpu_softmax_fwd_pd_t {
+        using cpu_softmax_fwd_pd_t::cpu_softmax_fwd_pd_t;
+
+        DECLARE_COMMON_PD_T("acl", acl_softmax_fwd_t);
+
+        status_t init(engine_t *engine) {
+
+            // Forward only
+            if (desc()->prop_kind != dnnl_forward_inference)
+                return status::unimplemented;
+
+            if (src_md()->data_type != data_type) return status::unimplemented;
+
+            if (!attr()->has_default_values()) return status::unimplemented;
+
+            // Get memory desc to find sizes and dims
+            const memory_desc_wrapper src_d(src_md());
+
+            // ACL only supports plain tensors, can be permuted but not blocked
+            if (!src_d.is_plain()) return status::unimplemented;
+
+            // Guards against a 0-sized dimension
+            if (src_d.has_zero_dim()) return status::unimplemented;
+
+            // Not yet implemented using ACL
+            if (is_logsoftmax()) return status::unimplemented;
+
+            // No scaling
+            asp_.beta = 1;
+
+            // The strides give us the in memory inner size
+            dim_t inner_size_ = src_d.blocking_desc().strides[axis()];
+
+            dim_t axis_size_ = axis_size();
+
+            // The outer size is any left-over dimensions not inner or on the axis
+            dim_t outer_size_ = src_d.nelems() / (inner_size_ * axis_size_);
+
+            // In this context, NHWC tells ACL that the logical and physical
+            // dimensions are the same
+            arm_compute::DataLayout acl_layout = arm_compute::DataLayout::NHWC;
+            const arm_compute::DataType acl_data_t
+                    = acl_common_utils::get_acl_data_t(data_type);
+
+            const int threads = dnnl_get_max_threads();
+            dim_t total_size = outer_size_ * axis_size_ * inner_size_;
+            if (inner_size_ == 1) {
+                // A rough empirical heuristic created by fitting a polynomial
+                // of the tensor sizes and thread count to the run time of the
+                // ref and ACL softmax. It is greater than zero when ref is
+                // faster, and less than zero when ACL is faster. We can
+                // interpret the constant term as the constant overhead
+                // associated with calling the external library and the negative
+                // coefficient on total_size as ACL being faster at processing
+                // each element
+                if (threads == 1 || outer_size_ == 1) {
+                    if (2 + 0.03 * outer_size_ - 0.005 * total_size > 0)
+                        return status::unimplemented;
+                } else {
+                    if (20 + 0.02 * outer_size_ - 0.006 * total_size / threads
+                                    + 3 * threads
+                            > 0)
+                        return status::unimplemented;
+                }
+
+                // If the inner size is 1, we can get rid of the dimension.
+                // This stops ACL doing a unnecessary permute
+                arm_compute::TensorShape acl_tensor_shape
+                        = arm_compute::TensorShape(axis_size_, outer_size_);
+                asp_.axis = 0;
+
+                asp_.src_info = arm_compute::TensorInfo(
+                        acl_tensor_shape, 1, acl_data_t, acl_layout);
+                asp_.dst_info = arm_compute::TensorInfo(
+                        acl_tensor_shape, 1, acl_data_t, acl_layout);
+            } else {
+                // A rough empirical heuristic, see comment above
+                if (2 - 0.005 * total_size / threads + 3 * threads > 0)
+                    return status::unimplemented;
+
+                // Irrespective of the input dimensions, we construct a tensor
+                // with dimensions such that softmax can be applied over the
+                // middle axis (1), with the correct stride and vector length.
+                arm_compute::TensorShape acl_tensor_shape
+                        = arm_compute::TensorShape(
+                                inner_size_, axis_size_, outer_size_);
+                asp_.axis = 1;
+
+                asp_.src_info = arm_compute::TensorInfo(
+                        acl_tensor_shape, 1, acl_data_t, acl_layout);
+                asp_.dst_info = arm_compute::TensorInfo(
+                        acl_tensor_shape, 1, acl_data_t, acl_layout);
+            }
+
+            // Validate manually to check for return status
+            auto acl_st = arm_compute::NESoftmaxLayer::validate(
+                    &asp_.src_info, &asp_.dst_info, asp_.beta, asp_.axis);
+            if (acl_st.error_code() != arm_compute::ErrorCode::OK) {
+                MAYBE_REPORT_ACL_ERROR(acl_st.error_description().c_str());
+                return status::unimplemented;
+            }
+
+            acl_common_utils::acl_thread_bind();
+
+            return status::success;
+        }
+
+        acl_softmax_conf_t asp_;
+    }; // pd_t
+
+    acl_softmax_fwd_t(const pd_t *apd) : primitive_t(apd) {}
+
+    status_t create_resource(
+            engine_t *engine, resource_mapper_t &mapper) const override {
+        if (mapper.has_resource(this)) return status::success;
+
+        auto r = utils::make_unique<acl_softmax_resource_t>();
+        if (!r) return status::out_of_memory;
+
+        // Configure the resource based on information from primitive descriptor
+        auto st = r->configure(pd()->asp_);
+        if (st == status::success) { mapper.add(this, std::move(r)); }
+
+        return st;
+    }
+
+    typedef typename prec_traits<data_type>::type data_t;
+
+    status_t execute(const exec_ctx_t &ctx) const override {
+        return execute_forward(ctx);
+    }
+
+private:
+    // To guard the const execute_forward, the mutex must be 'mutable'
+    mutable std::mutex mtx;
+    status_t execute_forward(const exec_ctx_t &ctx) const;
+    const pd_t *pd() const { return (const pd_t *)primitive_t::pd().get(); }
+}; // acl_softmax_fwd_t
+
+} // namespace aarch64
+} // namespace cpu
+} // namespace impl
+} // namespace dnnl
+
+#endif
diff --git a/src/cpu/aarch64/acl_utils.hpp b/src/cpu/aarch64/acl_utils.hpp
index 9bd0036cd..8dfc234f5 100644
--- a/src/cpu/aarch64/acl_utils.hpp
+++ b/src/cpu/aarch64/acl_utils.hpp
@@ -46,6 +46,11 @@ arm_compute::ActivationLayerInfo get_acl_act(const eltwise_desc_t &ed);
 bool acl_act_ok(alg_kind_t eltwise_activation);
 void acl_thread_bind();
 
+#define MAYBE_REPORT_ACL_ERROR(msg) \
+    do { \
+        if (get_verbose()) printf("dnnl_verbose,cpu,error,acl,%s\n", (msg)); \
+    } while (0)
+
 } // namespace acl_common_utils
 
 } // namespace aarch64
diff --git a/src/cpu/aarch64/matmul/acl_matmul_utils.cpp b/src/cpu/aarch64/matmul/acl_matmul_utils.cpp
index bf35ef83c..943e34c41 100644
--- a/src/cpu/aarch64/matmul/acl_matmul_utils.cpp
+++ b/src/cpu/aarch64/matmul/acl_matmul_utils.cpp
@@ -82,16 +82,23 @@ status_t init_conf_matmul(acl_matmul_conf_t &amp, memory_desc_t &src_md,
             = arm_compute::TensorInfo(arm_compute::TensorShape(N, M, 1, batch),
                     1, arm_compute::DataType::F32);
 
+    // Fast-math mode
+    auto math_mode = get_fpmath_mode();
+    bool is_fastmath_enabled = one_of(math_mode, fpmath_mode::bf16, fpmath_mode::any);
+    amp.gemm_info.set_fast_math(is_fastmath_enabled);
+
     // Fused ReLU activation
     amp.gemm_info.set_activation_info(get_acl_act(attr));
+
     // Set alpha (output scaling)
     amp.alpha = attr.output_scales_.scales_[0];
+
     // Validate ACL transpose
     if (amp.is_transA) {
         auto acl_transA_st = arm_compute::NETranspose::validate(
                 &amp.src_acc_info, &amp.src_info);
         if (acl_transA_st.error_code() != arm_compute::ErrorCode::OK) {
-            printf("%s\n", acl_transA_st.error_description().c_str());
+            MAYBE_REPORT_ACL_ERROR(acl_transA_st.error_description().c_str());
             return status::unimplemented;
         }
     }
@@ -99,7 +106,7 @@ status_t init_conf_matmul(acl_matmul_conf_t &amp, memory_desc_t &src_md,
         auto acl_transB_st = arm_compute::NETranspose::validate(
                 &amp.wei_acc_info, &amp.wei_info);
         if (acl_transB_st.error_code() != arm_compute::ErrorCode::OK) {
-            printf("%s\n", acl_transB_st.error_description().c_str());
+            MAYBE_REPORT_ACL_ERROR(acl_transB_st.error_description().c_str());
             return status::unimplemented;
         }
     }
@@ -107,7 +114,7 @@ status_t init_conf_matmul(acl_matmul_conf_t &amp, memory_desc_t &src_md,
     auto acl_st = arm_compute::NEGEMM::validate(&amp.src_info, &amp.wei_info,
             nullptr, &amp.dst_info, amp.alpha, 0.0f, amp.gemm_info);
     if (acl_st.error_code() != arm_compute::ErrorCode::OK) {
-        printf("%s\n", acl_st.error_description().c_str());
+        MAYBE_REPORT_ACL_ERROR(acl_st.error_description().c_str());
         return status::unimplemented;
     }
 
diff --git a/src/cpu/cpu_softmax_list.cpp b/src/cpu/cpu_softmax_list.cpp
index 584b3fd75..6b9046e4c 100644
--- a/src/cpu/cpu_softmax_list.cpp
+++ b/src/cpu/cpu_softmax_list.cpp
@@ -1,6 +1,7 @@
 /*******************************************************************************
 * Copyright 2019-2021 Intel Corporation
 * Copyright 2021 FUJITSU LIMITED
+* Copyright 2021 Arm Ltd. and affiliates
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
@@ -24,6 +25,9 @@
 using namespace dnnl::impl::cpu::x64;
 #elif DNNL_AARCH64
 #include "cpu/aarch64/jit_uni_softmax.hpp"
+#if DNNL_AARCH64_USE_ACL
+#include "cpu/aarch64/acl_softmax.hpp"
+#endif
 using namespace dnnl::impl::cpu::aarch64;
 #endif
 
@@ -42,10 +46,11 @@ const impl_list_item_t impl_list[] = {
         REG_SOFTMAX_P_FWD(CPU_INSTANCE_X64(jit_uni_softmax_fwd_t<sse41>))
         CPU_INSTANCE_AARCH64(jit_uni_softmax_fwd_t<sve_512>)
         CPU_INSTANCE_AARCH64(jit_uni_softmax_bwd_t<sve_512>)
-        REG_SOFTMAX_P_FWD(CPU_INSTANCE(ref_softmax_fwd_t<f32>))
-        REG_SOFTMAX_P_BWD(CPU_INSTANCE(ref_softmax_bwd_t<f32>))
-        REG_SOFTMAX_P_FWD(CPU_INSTANCE(ref_softmax_fwd_t<bf16>))
-        REG_SOFTMAX_P_BWD(CPU_INSTANCE(ref_softmax_bwd_t<bf16>))
+        CPU_INSTANCE_AARCH64_ACL(acl_softmax_fwd_t<f32>)
+        CPU_INSTANCE(ref_softmax_fwd_t<f32>)
+        CPU_INSTANCE(ref_softmax_bwd_t<f32>)
+        CPU_INSTANCE(ref_softmax_fwd_t<bf16>)
+        CPU_INSTANCE(ref_softmax_bwd_t<bf16>)
         /* eol */
         nullptr,
 };
diff --git a/tests/benchdnn/inputs/softmax/test_softmax_all b/tests/benchdnn/inputs/softmax/test_softmax_all
index fde8ac007..4e1dbbeaf 100644
--- a/tests/benchdnn/inputs/softmax/test_softmax_all
+++ b/tests/benchdnn/inputs/softmax/test_softmax_all
@@ -2,7 +2,7 @@
 
 # f32
 --dt=f32
---dir=FWD_D,BWD_D
+--dir=FWD_I,FWD_D,BWD_D
 --alg=SOFTMAX,LOGSOFTMAX
 --inplace=true,false
 
diff --git a/tests/benchdnn/inputs/softmax/test_softmax_ci b/tests/benchdnn/inputs/softmax/test_softmax_ci
index 5b5efa41d..48c786e45 100644
--- a/tests/benchdnn/inputs/softmax/test_softmax_ci
+++ b/tests/benchdnn/inputs/softmax/test_softmax_ci
@@ -2,7 +2,7 @@
 
 --inplace=true,false
 
---dir=FWD_D,BWD_D
+--dir=FWD_I,FWD_D,BWD_D
 --dt=f32,bf16,f16
 --tag=abx,axb
 --alg=SOFTMAX,LOGSOFTMAX
diff --git a/src/cpu/aarch64/acl_utils.cpp b/src/cpu/aarch64/acl_utils.cpp
index ed1376dbf..84a7ff639 100644
--- a/src/cpu/aarch64/acl_utils.cpp
+++ b/src/cpu/aarch64/acl_utils.cpp
@@ -105,7 +105,11 @@ void acl_thread_bind() {
     static std::once_flag flag_once;
     // The threads in Compute Library are bound for the cores 0..max_threads-1
     // dnnl_get_max_threads() returns OMP_NUM_THREADS
-    const int max_threads = dnnl_get_max_threads();
+    // Cap the number of threads to 90% of the total core count
+    // to ensure Compute Library doesn't use too much resource
+    int capped_threads = (int)std::floor(0.9*std::thread::hardware_concurrency());
+    const int max_threads = std::min(capped_threads, dnnl_get_max_threads());
+
     // arm_compute::Scheduler does not support concurrent access thus a
     // workaround here restricts it to only one call
     std::call_once(flag_once, [&]() {