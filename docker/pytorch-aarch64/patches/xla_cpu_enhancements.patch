# *******************************************************************************
# Copyright 2022 Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# *******************************************************************************
diff --git a/.bazelrc b/.bazelrc
index fc01c887dfa..7a83caf7ce8 100644
--- a/.bazelrc
+++ b/.bazelrc
@@ -231,6 +231,7 @@ build:mkl_threadpool -c opt
 # Config setting to build oneDNN with Compute Library for the Arm Architecture (ACL).
 build:mkl_aarch64 --define=build_with_mkl_aarch64=true
 build:mkl_aarch64 --define=build_with_openmp=true
+build:mkl_aarch64 --define=build_with_acl=true
 build:mkl_aarch64 -c opt
 
 # This config refers to building CUDA op kernels with nvcc.
diff --git a/tensorflow/compiler/xla/BUILD b/tensorflow/compiler/xla/BUILD
index c8aba665b8e..2df64fe017e 100644
--- a/tensorflow/compiler/xla/BUILD
+++ b/tensorflow/compiler/xla/BUILD
@@ -4,6 +4,7 @@ load("//tensorflow:tensorflow.bzl", "filegroup")
 load("//tensorflow:tensorflow.bzl", "get_compatible_with_portable")
 load("//tensorflow/core/platform:rules_cc.bzl", "cc_library")
 load("//tensorflow:tensorflow.bzl", "cc_header_only_library", "tf_cc_test")
+load("//tensorflow:tensorflow.bzl", "tf_xla_acl_copts")
 load(
     "//tensorflow/core/platform:build_config.bzl",
     "tf_proto_library",
@@ -963,6 +964,7 @@ cc_library(
     ],
     hdrs = ["debug_options_flags.h"],
     visibility = [":friends"],
+    copts = tf_xla_acl_copts(),
     deps =
         [
             ":parse_flags_from_env",
diff --git a/tensorflow/compiler/xla/debug_options_flags.cc b/tensorflow/compiler/xla/debug_options_flags.cc
index 6ea9f9635aa..f876b80ee49 100644
--- a/tensorflow/compiler/xla/debug_options_flags.cc
+++ b/tensorflow/compiler/xla/debug_options_flags.cc
@@ -48,6 +48,9 @@ DebugOptions DefaultDebugOptionsIgnoringFlags() {
 #ifdef ENABLE_MKL
   opts.set_xla_cpu_use_mkl_dnn(true);
 #endif  // ENABLE_MKL
+#ifdef XLA_CPU_USE_ACL
+  opts.set_xla_cpu_use_acl(true);
+#endif
   opts.set_xla_gpu_max_kernel_unroll_factor(4);
 
   // Run all GPU work on one stream by default.  Using multiple streams
@@ -439,6 +442,11 @@ static void AllocateFlags() {
                        bool_setter_for(&DebugOptions::set_xla_cpu_use_mkl_dnn),
                        flag_values->xla_cpu_use_mkl_dnn(),
                        "Generate calls to MKL-DNN in the CPU backend."));
+  flag_objects->push_back(
+      tensorflow::Flag("xla_cpu_use_acl",
+                       bool_setter_for(&DebugOptions::set_xla_cpu_use_acl),
+                       flag_values->xla_cpu_use_acl(),
+                       "Generate calls to ACL (Arm Compute Library) in the CPU backend."));
   flag_objects->push_back(tensorflow::Flag(
       "xla_gpu_crash_on_verification_failures",
       bool_setter_for(
diff --git a/tensorflow/compiler/xla/service/cpu/BUILD b/tensorflow/compiler/xla/service/cpu/BUILD
index b9fe6d65b1e..16a46286dd4 100644
--- a/tensorflow/compiler/xla/service/cpu/BUILD
+++ b/tensorflow/compiler/xla/service/cpu/BUILD
@@ -7,6 +7,11 @@ load(
     "//third_party/mkl:build_defs.bzl",
     "mkl_deps",
 )
+load("//tensorflow:tensorflow.bzl", "tf_xla_acl_copts")
+load(
+    "//third_party/compute_library:build_defs.bzl",
+    "acl_deps",
+)
 
 # buildifier: disable=same-origin-load
 load("//tensorflow:tensorflow.bzl", "filegroup")
@@ -338,6 +343,7 @@ cc_library(
         "windows_compatibility.h",
     ],
     hdrs = ["simple_orc_jit.h"],
+    copts = tf_xla_acl_copts(),
     deps = [
         ":compiler_functor",
         ":cpu_runtime",
@@ -358,6 +364,8 @@ cc_library(
         ":runtime_single_threaded_conv3d",
         ":runtime_single_threaded_fft",
         ":runtime_single_threaded_matmul",
+        ":runtime_matmul_acl",
+        ":runtime_conv2d_acl",
         "@com_google_absl//absl/memory",
         "@llvm-project//llvm:ExecutionEngine",
         "@llvm-project//llvm:Core",
@@ -586,6 +594,7 @@ cc_library(
     hdrs = [
         "dot_op_emitter.h",
     ],
+    copts = tf_xla_acl_copts(),
     deps = [
         ":cpu_options",
         ":cpu_runtime",
@@ -844,6 +853,43 @@ cc_library(
     ] + mkl_deps(),
 )
 
+cc_library(
+    name = "runtime_matmul_acl",
+    srcs = ["runtime_matmul_acl.cc"],
+    hdrs = ["runtime_matmul_acl.h"],
+    copts = tf_xla_acl_copts(),
+    visibility = ["//visibility:public"],
+    deps = [
+        ":runtime_lightweight_check",
+        ":runtime_matmul",
+        "//tensorflow/compiler/xla:executable_run_options",
+        "//tensorflow/core/platform:types",
+        "//third_party/eigen3",
+        "//tensorflow/core/platform:dynamic_annotations",
+        "//tensorflow/core/platform:logging",
+    ] + acl_deps(),
+)
+
+cc_library(
+    name = "runtime_conv2d_acl",
+    srcs = [
+        "runtime_conv2d_acl.cc",
+    ],
+    hdrs = ["runtime_conv2d_acl.h"],
+    copts = tf_xla_acl_copts(),
+    visibility = ["//visibility:public"],
+    deps = [
+        ":runtime_conv2d",
+        ":runtime_single_threaded_conv2d",
+        "//tensorflow/compiler/xla:executable_run_options",
+        "//tensorflow/core/platform:dynamic_annotations",
+        "//tensorflow/core/platform:types",
+        "//tensorflow/core/kernels:eigen_helpers",
+        "//third_party/eigen3",
+	"//tensorflow/core/platform:logging",
+    ] + acl_deps(),
+)
+
 cc_library(
     name = "runtime_single_threaded_conv2d",
     srcs = [
diff --git a/tensorflow/compiler/xla/service/cpu/cpu_runtime.cc b/tensorflow/compiler/xla/service/cpu/cpu_runtime.cc
index 4897ee99126..413e8b43cb1 100644
--- a/tensorflow/compiler/xla/service/cpu/cpu_runtime.cc
+++ b/tensorflow/compiler/xla/service/cpu/cpu_runtime.cc
@@ -75,12 +75,20 @@ extern const char* const kEigenMatMulC128SymbolName =
     "__xla_cpu_runtime_EigenMatMulC128";
 extern const char* const kEigenMatMulS32SymbolName =
     "__xla_cpu_runtime_EigenMatMulS32";
+extern const char* const kEigenBatchMatMulF32SymbolName =
+    "__xla_cpu_runtime_EigenBatchMatMulF32";
 extern const char* const kMKLConv2DF32SymbolName =
     "__xla_cpu_runtime_MKLConv2DF32";
+extern const char* const kACLConv2DF32SymbolName =
+    "__xla_cpu_runtime_ACLConv2DF32";
 extern const char* const kMKLMatMulF32SymbolName =
     "__xla_cpu_runtime_MKLMatMulF32";
 extern const char* const kMKLMatMulF64SymbolName =
     "__xla_cpu_runtime_MKLMatMulF64";
+extern const char* const kACLMatMulF32SymbolName =
+    "__xla_cpu_runtime_ACLMatMulF32";
+extern const char* const kACLBatchMatMulF32SymbolName =
+    "__xla_cpu_runtime_ACLBatchMatMulF32";
 extern const char* const kMKLSingleThreadedMatMulF32SymbolName =
     "__xla_cpu_runtime_MKLSingleThreadedMatMulF32";
 extern const char* const kMKLSingleThreadedMatMulF64SymbolName =
diff --git a/tensorflow/compiler/xla/service/cpu/cpu_runtime.h b/tensorflow/compiler/xla/service/cpu/cpu_runtime.h
index 2d45e09abbe..46b0e21d7d6 100644
--- a/tensorflow/compiler/xla/service/cpu/cpu_runtime.h
+++ b/tensorflow/compiler/xla/service/cpu/cpu_runtime.h
@@ -49,9 +49,13 @@ extern const char* const kEigenMatMulF64SymbolName;
 extern const char* const kEigenMatMulC64SymbolName;
 extern const char* const kEigenMatMulC128SymbolName;
 extern const char* const kEigenMatMulS32SymbolName;
+extern const char* const kEigenBatchMatMulF32SymbolName;
 extern const char* const kMKLConv2DF32SymbolName;
+extern const char* const kACLConv2DF32SymbolName;
 extern const char* const kMKLMatMulF32SymbolName;
 extern const char* const kMKLMatMulF64SymbolName;
+extern const char* const kACLMatMulF32SymbolName;
+extern const char* const kACLBatchMatMulF32SymbolName;
 extern const char* const kMKLSingleThreadedMatMulF32SymbolName;
 extern const char* const kMKLSingleThreadedMatMulF64SymbolName;
 extern const char* const kEigenConv2DF16SymbolName;
diff --git a/tensorflow/compiler/xla/service/cpu/dot_op_emitter.cc b/tensorflow/compiler/xla/service/cpu/dot_op_emitter.cc
index a1de0744dd1..15ecb343d23 100644
--- a/tensorflow/compiler/xla/service/cpu/dot_op_emitter.cc
+++ b/tensorflow/compiler/xla/service/cpu/dot_op_emitter.cc
@@ -134,6 +134,9 @@ class DotOpEmitter {
   // Emits the IR to perform the dot operation.
   Status Emit();
 
+  // Emits the IR to perform the batch dot operation.
+  Status EmitBatch();
+
  private:
   // Emits instructions to perform a scalar dot product (a multiply of the
   // LHS and RHS) and store the results in the target.
@@ -142,6 +145,9 @@ class DotOpEmitter {
   // Emits a call to the CPU runtime to perform the matrix multiply.
   Status EmitCallToRuntime();
 
+  // Emits a call to the CPU runtime to perform the batch matrix multiply.
+  Status EmitCallToBatchRuntime();
+
   // Represents the dimensions of a matrix-matrix multiply operation.
   struct MatMultDims {
     // The number of rows in the LHS.
@@ -172,6 +178,11 @@ class DotOpEmitter {
   // of rank 2 as well).
   MatMultDims GetMatMultDims() const;
 
+  // Get the MatMultDims instance for the dot product this DotOpEmitter
+  // represents.  Precondition: the dot is of rank 3 (and thus its operands are
+  // of rank 3 as well).
+  MatMultDims GetBatchMatMultDims() const;
+
   // Lowers the dot operation as a tiled Matrix*Vector loop.
   void EmitTiledLlvmIrGemv();
 
@@ -569,6 +580,32 @@ Status DotOpEmitter::Emit() {
   }
 }
 
+Status DotOpEmitter::EmitBatch() {
+  // The dot operation performs a sum of products over dimension 0 of the left
+  // hand side operand and dimension 1 of the right hand side operand.
+  //
+  // Let the shapes of lhs and rhs be defined as below:
+  //
+  //   lhs = [L{n-1} x L{n-2} x ... L{0}]
+  //   rhs = [R{m-1} x R{m-2} x ... R{0}]
+  //
+  // The sum-of-products dimension in the lhs has size L{0} and the dimension in
+  // the rhs has size R{1}. Necessarily, then:
+  //
+  //   L{0} == R{1}
+  //
+  // The output of the operation has the following shape:
+  //
+  //   output = [L{n-1} x L{n-2} x ... L{1} x R{m-1} x R{m-2} x ... R{2} x R{0}]
+  //
+  // To perform the operation we construct a loop nest with one for-loop for
+  // each dimension of the output. Inside this loop nest is another for-loop
+  // which performs the sum-of-products (the reduction loop) before storing
+  // the result in the output buffer.
+
+      return EmitCallToBatchRuntime();
+}
+
 void DotOpEmitter::EmitNaiveLlvmIrGemm() {
   CHECK_EQ(addend_array_, nullptr);
 
@@ -761,6 +798,7 @@ Status DotOpEmitter::EmitCallToRuntime() {
 
   bool multi_threaded = ShouldUseMultiThreadedEigen(hlo_module_config_);
   bool use_mkl_dnn = hlo_module_config_.debug_options().xla_cpu_use_mkl_dnn();
+  bool use_acl = hlo_module_config_.debug_options().xla_cpu_use_acl();
   PrimitiveType type = target_array_.GetShape().element_type();
   llvm::Function* function = b_->GetInsertBlock()->getParent();
   llvm::Module* module = function->getParent();
@@ -776,7 +814,8 @@ Status DotOpEmitter::EmitCallToRuntime() {
     case F32:
       fn_name = multi_threaded
                     ? (use_mkl_dnn ? runtime::kMKLMatMulF32SymbolName
-                                   : runtime::kEigenMatMulF32SymbolName)
+				   : (use_acl ? runtime::kACLMatMulF32SymbolName
+                                              : runtime::kEigenMatMulF32SymbolName))
                     : (use_mkl_dnn
                            ? runtime::kMKLSingleThreadedMatMulF32SymbolName
                            : runtime::kEigenSingleThreadedMatMulF32SymbolName);
@@ -870,6 +909,92 @@ Status DotOpEmitter::EmitCallToRuntime() {
   return Status::OK();
 }
 
+Status DotOpEmitter::EmitCallToBatchRuntime() {
+  // The signature of the runtime batch matmul function is:
+  //
+  //   (void)(void* run_options, float* out, float* lhs, float* rhs,
+  //          int64_t m, int64_t n, int64_t k, int64_t batch_size, int32_t transpose_lhs,
+  //          int32_t transpose_rhs);
+  // The two transpose_... parameters are actually booleans, but we use int32_t
+  // to avoid target-dependent calling convention details.
+
+  PrimitiveType type = target_array_.GetShape().element_type();
+  bool use_acl = hlo_module_config_.debug_options().xla_cpu_use_acl();
+  llvm::Function* function = b_->GetInsertBlock()->getParent();
+  llvm::Module* module = function->getParent();
+  llvm::Type* float_type;
+  const char* fn_name;
+  switch (type) {
+    case F32:
+      fn_name = use_acl ? runtime::kACLBatchMatMulF32SymbolName
+	                : runtime::kEigenBatchMatMulF32SymbolName;
+
+      float_type = b_->getFloatTy();
+      break;
+    default:
+      return Unimplemented("Invalid type %s for dot operation",
+                           PrimitiveType_Name(type));
+  }
+
+  llvm::Type* float_ptr_type = float_type->getPointerTo();
+  llvm::Type* int64_type = b_->getInt64Ty();
+  llvm::Type* int32_type = b_->getInt32Ty();
+  llvm::Type* int8_ptr_type = b_->getInt8Ty()->getPointerTo();
+  llvm::Type* int32_ptr_type = b_->getInt32Ty()->getPointerTo();
+  llvm::FunctionType* matmul_type = llvm::FunctionType::get(
+      b_->getVoidTy(),
+      {int8_ptr_type, float_ptr_type, float_ptr_type, float_ptr_type,
+       int64_type, int64_type, int64_type, int64_type, int32_type, int32_type},
+      /*isVarArg=*/false);
+
+  llvm::FunctionCallee matmul_func =
+      module->getOrInsertFunction(fn_name, matmul_type);
+  if (auto* fn = llvm::dyn_cast<llvm::Function>(matmul_func.getCallee())) {
+    fn->setCallingConv(llvm::CallingConv::C);
+    fn->setDoesNotThrow();
+    fn->setOnlyAccessesArgMemory();
+  }
+
+  // The ACL runtime function expects column-major layout. If the matrices are
+  // row major, then use the following identity to compute the product:
+  //
+  //   (A x B)^T = B^T x A^T
+  //
+  // The connection between this identity and memory layout is that the
+  // transpose operation can also be considered as an operation that changes the
+  // memory layout of a matrix from row-major to column-major or vice versa.
+  //
+  // Effectively this involves swapping the 'lhs' with 'rhs' and 'm' with 'n'.
+
+  MatMultDims mat_mult_dims = GetBatchMatMultDims();
+  CHECK_EQ(mat_mult_dims.lhs_column_major, mat_mult_dims.rhs_column_major);
+
+  const llvm_ir::IrArray* lhs = &lhs_array_;
+  const llvm_ir::IrArray* rhs = &rhs_array_;
+  bool transpose_lhs = !mat_mult_dims.lhs_canonical;
+  bool transpose_rhs = !mat_mult_dims.rhs_canonical;
+  const Shape& lhs_shape = lhs_array_.GetShape();
+
+  if (!mat_mult_dims.lhs_column_major) {
+    std::swap(mat_mult_dims.m, mat_mult_dims.n);
+    std::swap(lhs, rhs);
+    std::swap(transpose_lhs, transpose_rhs);
+  }
+
+  VLOG(1) << "Batch dot emitted with runtime:" << fn_name;
+
+  b_->CreateCall(
+      matmul_func,
+      {b_->CreateBitCast(executable_run_options_value_, int8_ptr_type),
+       b_->CreateBitCast(target_array_.GetBasePointer(), float_ptr_type),
+       b_->CreateBitCast(lhs->GetBasePointer(), float_ptr_type),
+       b_->CreateBitCast(rhs->GetBasePointer(), float_ptr_type),
+       b_->getInt64(mat_mult_dims.m), b_->getInt64(mat_mult_dims.n),
+       b_->getInt64(mat_mult_dims.k), b_->getInt64(lhs_shape.dimensions(0)),
+       b_->getInt32(transpose_lhs), b_->getInt32(transpose_rhs)});
+  return Status::OK();
+}
+
 DotOpEmitter::MatMultDims DotOpEmitter::GetMatMultDims() const {
   CHECK_LE(dot_info_.result_shape.dimensions_size(), 2);
 
@@ -900,6 +1025,36 @@ DotOpEmitter::MatMultDims DotOpEmitter::GetMatMultDims() const {
       /*rhs_canonical=*/dim_nums.rhs_contracting_dimensions(0) == 0};
 }
 
+DotOpEmitter::MatMultDims DotOpEmitter::GetBatchMatMultDims() const {
+  CHECK_LE(dot_info_.result_shape.dimensions_size(), 2);
+
+  const Shape& lhs_shape = lhs_array_.GetShape();
+  const Shape& rhs_shape = rhs_array_.GetShape();
+  const DotDimensionNumbers& dim_nums = dot_info_.dim_nums;
+
+  auto is_column_major = [](const Shape& shape) {
+    return shape.rank() > 1 && LayoutUtil::Minor(shape.layout(), 0) == 0;
+  };
+
+  // Non-contracting dots should never make it here.
+  CHECK_GE(dim_nums.lhs_contracting_dimensions_size(), 0);
+  CHECK_GE(dim_nums.rhs_contracting_dimensions_size(), 0);
+
+  return {
+      /*m=*/lhs_shape.rank() <= 1
+          ? 1LL
+          : lhs_shape.dimensions(2LL - dim_nums.lhs_contracting_dimensions(0)),
+      /*k=*/lhs_shape.dimensions(1LL + dim_nums.lhs_contracting_dimensions(0)),
+      /*n=*/rhs_shape.rank() <= 1
+          ? 1LL
+          : rhs_shape.dimensions(2LL - dim_nums.rhs_contracting_dimensions(0)),
+      /*lhs_column_major=*/is_column_major(lhs_shape),
+      /*lhs_canonical=*/lhs_shape.rank() <= 1 ||
+          dim_nums.lhs_contracting_dimensions(0) == 1,
+      /*rhs_column_major=*/is_column_major(rhs_shape),
+      /*rhs_canonical=*/dim_nums.rhs_contracting_dimensions(0) == 0};
+}
+
 // For vector-matrix dot products, it is always profitable to make the Rhs
 // column major.
 absl::optional<int64_t> ProfitableToMakeDotOperandColumnMajor(
@@ -1159,15 +1314,12 @@ llvm_ir::IrArray SliceOutInnerArray(llvm_ir::IrArray outer_array,
                           new_ir_type, std::move(inner_shape));
 }
 
-Status EmitBatchDotOperation(
+bool PotentiallyImplementedAsEigenMatmul(
     const HloInstruction& dot, const llvm_ir::IrArray& target_array,
     const llvm_ir::IrArray& lhs_array, const llvm_ir::IrArray& rhs_array,
     llvm::Value* executable_run_options_value, llvm::IRBuilder<>* b,
     mlir::MLIRContext* mlir_context, const HloModuleConfig& hlo_module_config,
-    const TargetMachineFeatures& target_machine_features) {
-  TF_RETURN_IF_ERROR(ValidateDotDimensionNumbers(dot.dot_dimension_numbers()));
-
-  // Lower a batch dot into a sequence of non-batch dot operations.
+    const TargetMachineFeatures& target_machine_features, DotInfo& dot_info) {
 
   int64_t num_batch_dims =
       dot.dot_dimension_numbers().lhs_batch_dimensions_size();
@@ -1184,46 +1336,118 @@ Status EmitBatchDotOperation(
   llvm_ir::IrArray target_array_reshaped =
       CollapseFirstNDims(b, target_array, num_batch_dims);
 
-  int64_t batch_count = lhs_array_reshaped.GetShape().dimensions(0);
-
-  KernelSupportLibrary ksl(b);
-
-  return ksl.ForWithStatus(
-      llvm_ir::IrName(&dot, "bdot"), /*start=*/0, /*end=*/batch_count,
-      /*step=*/1, [&](llvm::Value* indvar) {
-        DotDimensionNumbers adjusted_dim_numbers = dot.dot_dimension_numbers();
-        adjusted_dim_numbers.clear_lhs_batch_dimensions();
-        adjusted_dim_numbers.clear_rhs_batch_dimensions();
-
-        // Create a DotInfo representing the "inner" non-batch dot operation.
-        DotInfo dot_info;
-        dot_info.lhs_shape = DropFirstDim(lhs_array_reshaped.GetShape());
-        dot_info.rhs_shape = DropFirstDim(rhs_array_reshaped.GetShape());
-        dot_info.result_shape = DropFirstDim(target_array_reshaped.GetShape());
-        dot_info.dim_nums = dot.dot_dimension_numbers();
-        dot_info.dim_nums.clear_lhs_batch_dimensions();
-        dot_info.dim_nums.clear_rhs_batch_dimensions();
-
-        dot_info.dim_nums.set_lhs_contracting_dimensions(
-            0,
-            dot_info.dim_nums.lhs_contracting_dimensions(0) - num_batch_dims);
-        dot_info.dim_nums.set_rhs_contracting_dimensions(
-            0,
-            dot_info.dim_nums.rhs_contracting_dimensions(0) - num_batch_dims);
-
-        llvm_ir::IrArray lhs_slice =
-            SliceOutInnerArray(lhs_array_reshaped, /*batch_index=*/indvar, b);
-        llvm_ir::IrArray rhs_slice =
-            SliceOutInnerArray(rhs_array_reshaped, /*batch_index=*/indvar, b);
-        llvm_ir::IrArray target_slice = SliceOutInnerArray(
-            target_array_reshaped, /*batch_index=*/indvar, b);
-
-        // Emit the inner non-batch dot operation.
-        return EmitNonBatchDotOperation(
-            dot_info, dot.name(), target_slice, lhs_slice, rhs_slice, nullptr,
-            executable_run_options_value, b, mlir_context, hlo_module_config,
-            target_machine_features);
-      });
+  DotDimensionNumbers adjusted_dim_numbers = dot.dot_dimension_numbers();
+  adjusted_dim_numbers.clear_lhs_batch_dimensions();
+  adjusted_dim_numbers.clear_rhs_batch_dimensions();
+
+  // Create a DotInfo representing the batch of "inner" dot operations.
+  dot_info.lhs_shape = DropFirstDim(lhs_array_reshaped.GetShape());
+  dot_info.rhs_shape = DropFirstDim(rhs_array_reshaped.GetShape());
+  dot_info.result_shape = DropFirstDim(target_array_reshaped.GetShape());
+  dot_info.dim_nums = dot.dot_dimension_numbers();
+  dot_info.dim_nums.clear_lhs_batch_dimensions();
+  dot_info.dim_nums.clear_rhs_batch_dimensions();
+
+  dot_info.dim_nums.set_lhs_contracting_dimensions(0,
+       dot_info.dim_nums.lhs_contracting_dimensions(0) - num_batch_dims);
+  dot_info.dim_nums.set_rhs_contracting_dimensions(0,
+       dot_info.dim_nums.rhs_contracting_dimensions(0) - num_batch_dims);
+
+  PrimitiveType type = target_array.GetShape().element_type();
+  if (F32 != type) return false;
+
+  if (ShapeUtil::IsScalar(dot_info.lhs_shape) || ShapeUtil::IsScalar(dot_info.rhs_shape)) {
+    // If the operands are scalar, don't emit any loops.
+    return false;
+  }
+
+  DotImplementationStrategy impl_strategy =
+      GetDotImplementationStrategy(dot.parent()->parent()->config(),
+                                   dot_info, target_machine_features);
+
+  return impl_strategy == DotImplementationStrategy::kEigen;
+}
+
+Status EmitBatchDotOperation(
+    const HloInstruction& dot, const llvm_ir::IrArray& target_array,
+    const llvm_ir::IrArray& lhs_array, const llvm_ir::IrArray& rhs_array,
+    llvm::Value* executable_run_options_value, llvm::IRBuilder<>* b,
+    mlir::MLIRContext* mlir_context, const HloModuleConfig& hlo_module_config,
+    const TargetMachineFeatures& target_machine_features) {
+  TF_RETURN_IF_ERROR(ValidateDotDimensionNumbers(dot.dot_dimension_numbers()));
+
+  //first check if the batch can be rendered directly by the runtime
+  //otherwise lower it to a sequence of non-batch dot operations
+  DotInfo dot_info;
+  if (PotentiallyImplementedAsEigenMatmul(dot, target_array, lhs_array, rhs_array,
+			                  executable_run_options_value, b, mlir_context,
+					  hlo_module_config, target_machine_features,
+					  dot_info)) {
+     DotOpEmitter dot_emitter(dot_info, dot.name(),
+                              target_array, lhs_array, rhs_array, nullptr /*addend_array*/,
+                              executable_run_options_value, b, mlir_context,
+                              hlo_module_config, target_machine_features);
+
+     return dot_emitter.EmitBatch();
+  } else {
+     // Lower a batch dot into a sequence of non-batch dot operations.
+
+     int64_t num_batch_dims =
+        dot.dot_dimension_numbers().lhs_batch_dimensions_size();
+
+     // First reshape the inputs to make sure we only have one batch dimension.
+     // This is a no-op bitcast because the operands have to be in row-major layout
+     // (enforced in CpuLayoutAssignment), and the batch dimensions are the leading
+     // dimensions (established by DotDecomposer and checked by
+     // ValidateDotDimensionNumbers above).
+     llvm_ir::IrArray lhs_array_reshaped =
+         CollapseFirstNDims(b, lhs_array, num_batch_dims);
+     llvm_ir::IrArray rhs_array_reshaped =
+         CollapseFirstNDims(b, rhs_array, num_batch_dims);
+     llvm_ir::IrArray target_array_reshaped =
+         CollapseFirstNDims(b, target_array, num_batch_dims);
+
+     int64_t batch_count = lhs_array_reshaped.GetShape().dimensions(0);
+
+     KernelSupportLibrary ksl(b);
+
+     return ksl.ForWithStatus(
+        llvm_ir::IrName(&dot, "bdot"), /*start=*/0, /*end=*/batch_count,
+           /*step=*/1, [&](llvm::Value* indvar) {
+           DotDimensionNumbers adjusted_dim_numbers = dot.dot_dimension_numbers();
+           adjusted_dim_numbers.clear_lhs_batch_dimensions();
+           adjusted_dim_numbers.clear_rhs_batch_dimensions();
+
+           // Create a DotInfo representing the "inner" non-batch dot operation.
+           DotInfo dot_info;
+           dot_info.lhs_shape = DropFirstDim(lhs_array_reshaped.GetShape());
+           dot_info.rhs_shape = DropFirstDim(rhs_array_reshaped.GetShape());
+           dot_info.result_shape = DropFirstDim(target_array_reshaped.GetShape());
+           dot_info.dim_nums = dot.dot_dimension_numbers();
+           dot_info.dim_nums.clear_lhs_batch_dimensions();
+           dot_info.dim_nums.clear_rhs_batch_dimensions();
+
+           dot_info.dim_nums.set_lhs_contracting_dimensions(
+               0,
+               dot_info.dim_nums.lhs_contracting_dimensions(0) - num_batch_dims);
+           dot_info.dim_nums.set_rhs_contracting_dimensions(
+               0,
+               dot_info.dim_nums.rhs_contracting_dimensions(0) - num_batch_dims);
+
+           llvm_ir::IrArray lhs_slice =
+               SliceOutInnerArray(lhs_array_reshaped, /*batch_index=*/indvar, b);
+           llvm_ir::IrArray rhs_slice =
+               SliceOutInnerArray(rhs_array_reshaped, /*batch_index=*/indvar, b);
+           llvm_ir::IrArray target_slice = SliceOutInnerArray(
+               target_array_reshaped, /*batch_index=*/indvar, b);
+
+           // Emit the inner non-batch dot operation.
+           return EmitNonBatchDotOperation(
+               dot_info, dot.name(), target_slice, lhs_slice, rhs_slice, nullptr,
+               executable_run_options_value, b, mlir_context, hlo_module_config,
+               target_machine_features);
+     });
+  }
 }
 
 bool IsBatchDot(const HloInstruction& instr) {
diff --git a/tensorflow/compiler/xla/service/cpu/ir_emitter.cc b/tensorflow/compiler/xla/service/cpu/ir_emitter.cc
index 6f00ec188a1..478c949443a 100644
--- a/tensorflow/compiler/xla/service/cpu/ir_emitter.cc
+++ b/tensorflow/compiler/xla/service/cpu/ir_emitter.cc
@@ -993,6 +993,8 @@ Status IrEmitter::HandleConvolution(HloInstruction* convolution) {
       bool use_mkl_dnn =
           hlo_module_config_.debug_options().xla_cpu_use_mkl_dnn() &&
           convolution->feature_group_count() == 1;
+      bool use_acl =
+          hlo_module_config_.debug_options().xla_cpu_use_acl();
 
       auto valid_num_dims = [](absl::Span<const int64_t> xs) {
         return xs.size() >= 2 && xs.size() <= 3;
@@ -1016,7 +1018,8 @@ Status IrEmitter::HandleConvolution(HloInstruction* convolution) {
                        : runtime::kEigenSingleThreadedConv2DF16SymbolName)
                 : (multi_threaded
                        ? (use_mkl_dnn ? runtime::kMKLConv2DF32SymbolName
-                                      : runtime::kEigenConv2DF32SymbolName)
+                                      : (use_acl ? runtime::kACLConv2DF32SymbolName
+                                                 : runtime::kEigenConv2DF32SymbolName))
                        : runtime::kEigenSingleThreadedConv2DF32SymbolName);
       } else if (input_dims.size() == 3) {
         fn_name =
@@ -1067,6 +1070,8 @@ Status IrEmitter::HandleConvolution(HloInstruction* convolution) {
         args.push_back(b_.getInt64(d));
       }
       args.push_back(b_.getInt64(convolution->feature_group_count()));
+
+      VLOG(1) << "Ir emitter emitted Convolution to runtime:" << fn_name;
       EmitCallToFunc(fn_name, args, b_.getVoidTy(), /*does_not_throw=*/true,
                      /*only_accesses_arg_memory=*/true);
 
diff --git a/tensorflow/compiler/xla/service/cpu/runtime_conv2d_acl.cc b/tensorflow/compiler/xla/service/cpu/runtime_conv2d_acl.cc
new file mode 100644
index 00000000000..18851a3a41c
--- /dev/null
+++ b/tensorflow/compiler/xla/service/cpu/runtime_conv2d_acl.cc
@@ -0,0 +1,284 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "tensorflow/compiler/xla/executable_run_options.h"
+#include "tensorflow/core/platform/dynamic_annotations.h"
+#include "tensorflow/core/platform/types.h"
+#ifdef XLA_CPU_USE_ACL
+#include "tensorflow/compiler/xla/service/cpu/runtime_conv2d.h"
+#include "tensorflow/compiler/xla/service/cpu/runtime_conv2d_acl.h"
+#include "tensorflow/compiler/xla/service/cpu/runtime_lightweight_check.h"
+#include "tensorflow/core/platform/logging.h"
+#include <mutex>
+#define EIGEN_USE_THREADS
+#include "third_party/eigen3/unsupported/Eigen/CXX11/Tensor"
+
+namespace {
+int32_t ACLDepthwiseConvImpl(const void* run_options_ptr, float* out, float* lhs,
+                 float* rhs, int64_t input_batch, int64_t input_rows,
+                 int64_t input_cols, int64_t input_channels,
+                 int64_t kernel_rows, int64_t kernel_cols,
+                 int64_t kernel_channels, int64_t kernel_filters,
+                 int64_t output_rows, int64_t output_cols, int64_t row_stride,
+                 int64_t col_stride, int64_t padding_top,
+                 int64_t padding_bottom, int64_t padding_left,
+                 int64_t padding_right, int64_t lhs_row_dilation,
+                 int64_t lhs_col_dilation, int64_t rhs_row_dilation,
+                 int64_t rhs_col_dilation, int64_t feature_group_count) {
+  VLOG(1) << "Accelerate with ACLDepthwiseConvImpl";
+  /* TODO: optimize this object creation along with tensor init and
+   * gemm configuration by caching the shapes, similar to onednn
+   * primitive caching feature
+   */
+  struct acl_depthwise_conv_obj_t acl_conv_obj;
+  struct acl_conv_conf_t acl_conf;
+
+  /* ir_emitter HandleConvolution ensures the below preconditions before dispatching it to ACL
+   *  layout: NHWC
+   *  format: FP32
+   *  Number of feature groups matches the input channels
+   *  source and kernel dilation is: 1
+   */
+  acl_conf.dilation_info = arm_compute::Size2D(lhs_col_dilation, lhs_row_dilation);
+  acl_conf.padstride_info = arm_compute::PadStrideInfo(col_stride, row_stride,
+                                                       static_cast<unsigned int>(padding_left),
+                                                       static_cast<unsigned int>(padding_right),
+                                                       static_cast<unsigned int>(padding_top),
+                                                       static_cast<unsigned int>(padding_bottom),
+                                                       arm_compute::DimensionRoundingType::FLOOR);
+  acl_conf.with_bias = false;
+
+  acl_conf.act_info = arm_compute::ActivationLayerInfo();
+
+  acl_conf.input_info = arm_compute::TensorInfo(arm_compute::TensorShape(input_channels, input_cols,
+                                                                         input_rows, input_batch),
+                                                1, arm_compute::DataType::F32, arm_compute::DataLayout::NHWC);
+  acl_conf.kernel_info = arm_compute::TensorInfo(arm_compute::TensorShape(kernel_filters,
+                                                                          kernel_cols, kernel_rows),
+                                                 1, arm_compute::DataType::F32, arm_compute::DataLayout::NHWC);
+  acl_conf.output_info = arm_compute::TensorInfo(arm_compute::TensorShape(kernel_filters, output_cols,
+                                                                          output_rows, input_batch),
+                                                 1, arm_compute::DataType::F32, arm_compute::DataLayout::NHWC);
+
+  auto acl_st = arm_compute::NEDepthwiseConvolutionLayer::validate(&acl_conf.input_info,
+                                                              &acl_conf.kernel_info,
+                                                              /*acp.with_bias */ nullptr,
+                                                              &acl_conf.output_info,
+                                                              acl_conf.padstride_info,
+                                                              kernel_channels /*depth_multiplier*/,
+                                                              acl_conf.act_info,
+                                                              acl_conf.dilation_info);
+  if (acl_st.error_code() != arm_compute::ErrorCode::OK) {
+     VLOG(1) << " Gemm conv validation failed";
+     return -1;
+  }
+
+  static std::once_flag flag_once;
+  const xla::ExecutableRunOptions* run_options = static_cast<const xla::ExecutableRunOptions*>(run_options_ptr);
+  XLA_LIGHTWEIGHT_CHECK(run_options->intra_op_thread_pool() != nullptr);
+  const Eigen::ThreadPoolDevice* tpd = (Eigen::ThreadPoolDevice*) (run_options->intra_op_thread_pool());
+  // The threads in Compute Library are bound for the cores 0..max_threads-1
+  const int max_threads = tpd->numThreads();
+
+  // arm_compute::Scheduler does not support concurrent access thus a
+  // workaround here restricts it to only one call
+  std::call_once(flag_once, [&]() {
+     arm_compute::Scheduler::get().set_num_threads(max_threads);
+  });
+
+  //configure the acl obj with the config
+  acl_conv_obj.input_tensor.allocator()->init(acl_conf.input_info);
+  acl_conv_obj.kernel_tensor.allocator()->init(acl_conf.kernel_info);
+  acl_conv_obj.output_tensor.allocator()->init(acl_conf.output_info);
+
+  acl_conv_obj.depthwise_conv.configure(&acl_conv_obj.input_tensor,
+                              &acl_conv_obj.kernel_tensor,
+                              nullptr,
+                              &acl_conv_obj.output_tensor,
+                              acl_conf.padstride_info,
+                              kernel_channels /*depth_multiplier*/,
+                              acl_conf.act_info,
+                              acl_conf.dilation_info);
+
+  /* import_memory() and free() methods do not allocate/free any additional
+   * memory, only acquire/release pointers.
+   */
+  acl_conv_obj.input_tensor.allocator()->import_memory(lhs);
+  acl_conv_obj.kernel_tensor.allocator()->import_memory(rhs);
+  acl_conv_obj.output_tensor.allocator()->import_memory(out);
+
+  acl_conv_obj.depthwise_conv.run();
+
+  acl_conv_obj.input_tensor.allocator()->free();
+  acl_conv_obj.kernel_tensor.allocator()->free();
+  acl_conv_obj.output_tensor.allocator()->free();
+
+  return 0;
+  }
+
+int32_t ACLGemmConvImpl(const void* run_options_ptr, float* out, float* lhs,
+                 float* rhs, int64_t input_batch, int64_t input_rows,
+                 int64_t input_cols, int64_t input_channels,
+                 int64_t kernel_rows, int64_t kernel_cols,
+                 int64_t kernel_channels, int64_t kernel_filters,
+                 int64_t output_rows, int64_t output_cols, int64_t row_stride,
+                 int64_t col_stride, int64_t padding_top,
+                 int64_t padding_bottom, int64_t padding_left,
+                 int64_t padding_right, int64_t lhs_row_dilation,
+                 int64_t lhs_col_dilation, int64_t rhs_row_dilation,
+                 int64_t rhs_col_dilation) {
+  VLOG(1) << "Accelerate with ACLGemmConvImpl";
+  /* TODO: optimize this object creation along with tensor init and
+   * gemm configuration by caching the shapes, similar to onednn
+   * primitive caching feature
+   */
+  struct acl_gemm_conv_obj_t acl_conv_obj;
+  struct acl_conv_conf_t acl_conf;
+
+  /* TODO: add TF_XLA_* flag for runtime control of fast math mode
+   */
+  acl_conf.fast_math = true;
+
+  /* ir_emitter HandleConvolution ensures the below preconditions before dispatching it to ACL
+   *  layout: NHWC
+   *  format: FP32
+   *  Number of feature groups: 1
+   *  source and kernel dilation is: 1
+   */
+  acl_conf.dilation_info = arm_compute::Size2D(lhs_col_dilation, lhs_row_dilation);
+  acl_conf.padstride_info = arm_compute::PadStrideInfo(col_stride, row_stride,
+                                                       static_cast<unsigned int>(padding_left),
+                                                       static_cast<unsigned int>(padding_right),
+                                                       static_cast<unsigned int>(padding_top),
+                                                       static_cast<unsigned int>(padding_bottom),
+                                                       arm_compute::DimensionRoundingType::FLOOR);
+  acl_conf.with_bias = false;
+
+  acl_conf.input_info = arm_compute::TensorInfo(arm_compute::TensorShape(input_channels, input_cols,
+                                                                         input_rows, input_batch),
+                                                1, arm_compute::DataType::F32, arm_compute::DataLayout::NHWC);
+  acl_conf.kernel_info = arm_compute::TensorInfo(arm_compute::TensorShape(input_channels, kernel_cols,
+                                                                          kernel_rows, kernel_filters),
+                                                 1, arm_compute::DataType::F32, arm_compute::DataLayout::NHWC);
+  acl_conf.output_info = arm_compute::TensorInfo(arm_compute::TensorShape(kernel_filters, output_cols,
+                                                                          output_rows, input_batch),
+                                                 1, arm_compute::DataType::F32, arm_compute::DataLayout::NHWC);
+  acl_conf.act_info = arm_compute::ActivationLayerInfo();
+
+  // Validate convolution manually to check for return status
+  auto acl_st = arm_compute::NEGEMMConvolutionLayer::validate(&acl_conf.input_info,
+                                                              &acl_conf.kernel_info,
+                                                              /*acp.with_bias */ nullptr,
+                                                              &acl_conf.output_info,
+                                                              acl_conf.padstride_info,
+                                                              acl_conf.kernel_wei_info,
+                                                              acl_conf.dilation_info,
+                                                              acl_conf.act_info,
+                                                              acl_conf.fast_math);
+  if (acl_st.error_code() != arm_compute::ErrorCode::OK) {
+     VLOG(1) << " Gemm conv validation failed";
+     return -1;
+  }
+
+  static std::once_flag flag_once;
+  const xla::ExecutableRunOptions* run_options = static_cast<const xla::ExecutableRunOptions*>(run_options_ptr);
+  XLA_LIGHTWEIGHT_CHECK(run_options->intra_op_thread_pool() != nullptr);
+  const Eigen::ThreadPoolDevice* tpd = (Eigen::ThreadPoolDevice*) (run_options->intra_op_thread_pool());
+  // The threads in Compute Library are bound for the cores 0..max_threads-1
+  const int max_threads = tpd->numThreads();
+
+  // arm_compute::Scheduler does not support concurrent access thus a
+  // workaround here restricts it to only one call
+  std::call_once(flag_once, [&]() {
+     arm_compute::Scheduler::get().set_num_threads(max_threads);
+  });
+
+  //configure the acl obj with the config
+  acl_conv_obj.input_tensor.allocator()->init(acl_conf.input_info);
+  acl_conv_obj.kernel_tensor.allocator()->init(acl_conf.kernel_info);
+  acl_conv_obj.output_tensor.allocator()->init(acl_conf.output_info);
+
+  // Configure GEMM
+  acl_conv_obj.gemm_conv.configure(&acl_conv_obj.input_tensor,
+                              &acl_conv_obj.kernel_tensor,
+                              nullptr,
+                              &acl_conv_obj.output_tensor,
+                              acl_conf.padstride_info,
+                              acl_conf.kernel_wei_info,
+                              acl_conf.dilation_info,
+                              acl_conf.act_info,
+                              acl_conf.fast_math);
+
+  /* import_memory() and free() methods do not allocate/free any additional
+   * memory, only acquire/release pointers.
+   */
+  acl_conv_obj.input_tensor.allocator()->import_memory(lhs);
+  acl_conv_obj.kernel_tensor.allocator()->import_memory(rhs);
+  acl_conv_obj.output_tensor.allocator()->import_memory(out);
+
+  acl_conv_obj.gemm_conv.run();
+
+  acl_conv_obj.input_tensor.allocator()->free();
+  acl_conv_obj.kernel_tensor.allocator()->free();
+  acl_conv_obj.output_tensor.allocator()->free();
+
+  return 0;
+  }
+}  // namespace
+
+ABSL_ATTRIBUTE_NO_SANITIZE_MEMORY void __xla_cpu_runtime_ACLConv2DF32(
+    const void* run_options_ptr, float* out, float* lhs, float* rhs,
+    int64_t input_batch, int64_t input_rows, int64_t input_cols,
+    int64_t input_channels, int64_t kernel_rows, int64_t kernel_cols,
+    int64_t kernel_channels, int64_t kernel_filters, int64_t output_rows,
+    int64_t output_cols, int64_t row_stride, int64_t col_stride,
+    int64_t padding_top, int64_t padding_bottom, int64_t padding_left,
+    int64_t padding_right, int64_t lhs_row_dilation, int64_t lhs_col_dilation,
+    int64_t rhs_row_dilation, int64_t rhs_col_dilation, int64_t feature_group_count) {
+  bool fallback_to_eigen = false;
+
+  if (lhs_row_dilation > 1 || lhs_col_dilation > 1 ||
+      ((feature_group_count > 1) && (input_channels != feature_group_count))) {
+         fallback_to_eigen = true;
+  } else if ((feature_group_count > 1) && (input_channels == feature_group_count)) {
+     if (ACLDepthwiseConvImpl(run_options_ptr, out, lhs, rhs, input_batch, input_rows, input_cols,
+                              input_channels, kernel_rows, kernel_cols, kernel_channels,
+                              kernel_filters, output_rows, output_cols, row_stride,
+                              col_stride, padding_top, padding_bottom, padding_left,
+                              padding_right, lhs_row_dilation, lhs_col_dilation,
+                              rhs_row_dilation, rhs_col_dilation, feature_group_count) < 0)
+        fallback_to_eigen = true;
+  } else {
+     if (ACLGemmConvImpl(run_options_ptr, out, lhs, rhs, input_batch, input_rows, input_cols,
+                         input_channels, kernel_rows, kernel_cols, kernel_channels,
+                         kernel_filters, output_rows, output_cols, row_stride,
+                         col_stride, padding_top, padding_bottom, padding_left,
+                         padding_right, lhs_row_dilation, lhs_col_dilation,
+                         rhs_row_dilation, rhs_col_dilation) < 0)
+        fallback_to_eigen = true;
+  }
+
+  if (fallback_to_eigen) {
+     VLOG(1) << "XLA conv2d not supported by ACL, fallback to Eigen runtime";
+     __xla_cpu_runtime_EigenConv2DF32(run_options_ptr, out, lhs, rhs, input_batch,
+                                    input_rows, input_cols, input_channels, kernel_rows,
+                                    kernel_cols, kernel_channels,kernel_filters,
+                                    output_rows, output_cols, row_stride, col_stride,
+                                    padding_top, padding_bottom, padding_left, padding_right,
+                                    lhs_row_dilation, lhs_col_dilation, rhs_row_dilation,
+                                    rhs_col_dilation, feature_group_count);
+  }
+}
+#endif  // XLA_CPU_USE_ACL
diff --git a/tensorflow/compiler/xla/service/cpu/runtime_conv2d_acl.h b/tensorflow/compiler/xla/service/cpu/runtime_conv2d_acl.h
new file mode 100644
index 00000000000..8721623d17a
--- /dev/null
+++ b/tensorflow/compiler/xla/service/cpu/runtime_conv2d_acl.h
@@ -0,0 +1,91 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef TENSORFLOW_COMPILER_XLA_SERVICE_CPU_RUNTIME_CONV2D_ACL_H_
+#define TENSORFLOW_COMPILER_XLA_SERVICE_CPU_RUNTIME_CONV2D_ACL_H_
+
+#include "tensorflow/core/platform/types.h"
+
+#ifdef XLA_CPU_USE_ACL
+#include "arm_compute/runtime/NEON/NEFunctions.h""
+#include "arm_compute/runtime/NEON/NEScheduler.h"
+#include "utils/Utils.h"
+
+extern "C" {
+struct acl_depthwise_conv_obj_t {
+    arm_compute::NEDepthwiseConvolutionLayer depthwise_conv;
+    arm_compute::NEArithmeticAddition add;
+    arm_compute::NEActivationLayer act;
+    arm_compute::Tensor input_tensor;
+    arm_compute::Tensor kernel_tensor;
+    arm_compute::Tensor bia_tensor;
+    arm_compute::Tensor output_tensor;
+    arm_compute::Tensor output_acc_tensor;
+};
+
+struct acl_gemm_conv_obj_t {
+    arm_compute::NEGEMMConvolutionLayer gemm_conv;
+    arm_compute::NEArithmeticAddition add;
+    arm_compute::NEActivationLayer act;
+    arm_compute::Tensor input_tensor;
+    arm_compute::Tensor kernel_tensor;
+    arm_compute::Tensor bia_tensor;
+    arm_compute::Tensor output_tensor;
+    arm_compute::Tensor output_acc_tensor;
+};
+
+struct acl_conv_conf_t {
+    bool with_bias;
+    bool is_int8;
+    bool sum_with_eltwise;
+    bool fast_math;
+    arm_compute::TensorInfo input_info;
+    arm_compute::TensorInfo kernel_info;
+    arm_compute::TensorInfo bia_info;
+    arm_compute::TensorInfo output_info;
+    arm_compute::PadStrideInfo padstride_info;
+    arm_compute::Size2D dilation_info;
+    arm_compute::WeightsInfo kernel_wei_info;
+    arm_compute::ActivationLayerInfo act_info;
+};
+
+extern void __xla_cpu_runtime_ACLConv2DF32(
+    const void* /* xla::ExecutableRunOptions* */ run_options_ptr, float* out,
+    float* lhs, float* rhs, int64_t input_batch, int64_t input_rows,
+    int64_t input_cols, int64_t input_channels, int64_t kernel_rows,
+    int64_t kernel_cols, int64_t kernel_channels, int64_t kernel_filters,
+    int64_t output_rows, int64_t output_cols, int64_t row_stride,
+    int64_t col_stride, int64_t padding_top, int64_t padding_bottom,
+    int64_t padding_left, int64_t padding_right, int64_t lhs_row_dilation,
+    int64_t lhs_col_dilation, int64_t rhs_row_dilation,
+    int64_t rhs_col_dilation, int64_t feature_group_count);
+}
+#else
+extern void __xla_cpu_runtime_ACLConv2DF32(
+    const void* /* xla::ExecutableRunOptions* */ run_options_ptr, float* out,
+    float* lhs, float* rhs, int64_t input_batch, int64_t input_rows,
+    int64_t input_cols, int64_t input_channels, int64_t kernel_rows,
+    int64_t kernel_cols, int64_t kernel_channels, int64_t kernel_filters,
+    int64_t output_rows, int64_t output_cols, int64_t row_stride,
+    int64_t col_stride, int64_t padding_top, int64_t padding_bottom,
+    int64_t padding_left, int64_t padding_right, int64_t lhs_row_dilation,
+    int64_t lhs_col_dilation, int64_t rhs_row_dilation,
+    int64_t rhs_col_dilation, int64_t feature_group_count) {
+  std::cerr << "Attempt to call ACL Conv2D runtime library without defining "
+               "XLA_CPU_USE_ACL. Add --define=build_with_acl=true to build with ACL.";
+  exit(1);
+}
+#endif //XLA_CPU_USE_ACL
+#endif  // TENSORFLOW_COMPILER_XLA_SERVICE_CPU_RUNTIME_CONV2D_ACL_H_
diff --git a/tensorflow/compiler/xla/service/cpu/runtime_matmul.cc b/tensorflow/compiler/xla/service/cpu/runtime_matmul.cc
index 807922f6396..b37a0c04faf 100644
--- a/tensorflow/compiler/xla/service/cpu/runtime_matmul.cc
+++ b/tensorflow/compiler/xla/service/cpu/runtime_matmul.cc
@@ -70,6 +70,49 @@ void MatMul(const void* run_options_ptr, T* out, T* lhs, T* rhs, int64_t m,
   C.device(*run_options->intra_op_thread_pool()) = A.contract(B, dims);
 }
 
+template <typename T, Eigen::AlignmentType Alignment>
+void MatMul_Batch(const void* run_options_ptr, T* out, T* lhs, T* rhs, int64_t m,
+                  int64_t n, int64_t k, int64_t batch_size, int32_t transpose_lhs,
+                  int32_t transpose_rhs) {
+  const xla::ExecutableRunOptions* run_options =
+      static_cast<const xla::ExecutableRunOptions*>(run_options_ptr);
+
+  int64_t lhs_rows = m;
+  int64_t lhs_cols = k;
+  const int64_t dim_pair_array_size = batch_size;
+  if (transpose_lhs) {
+    std::swap(lhs_rows, lhs_cols);
+  }
+
+  int64_t rhs_rows = k;
+  int64_t rhs_cols = n;
+  if (transpose_rhs) {
+    std::swap(rhs_rows, rhs_cols);
+  }
+
+  const Eigen::TensorMap<Eigen::Tensor<const T, 3>, Alignment> A(lhs, lhs_rows,
+                                                                 lhs_cols, batch_size);
+  const Eigen::TensorMap<Eigen::Tensor<const T, 3>, Alignment> B(rhs, rhs_rows,
+                                                                 rhs_cols, batch_size);
+  Eigen::TensorMap<Eigen::Tensor<T, 3>, Alignment> C(out, m, n, batch_size);
+
+  typedef typename Eigen::Tensor<T, 2>::DimensionPair DimPair;
+  int lhs_contract_dim = transpose_lhs ? 0 : 1;
+  int rhs_contract_dim = transpose_rhs ? 1 : 0;
+
+  const Eigen::array<DimPair, 1> dims(
+      {DimPair(lhs_contract_dim, rhs_contract_dim)});
+
+  // Matrix multiply is a special case of the "contract" operation where
+  // the contraction is performed along dimension 1 of the lhs and dimension
+  // 0 of the rhs.
+  XLA_LIGHTWEIGHT_CHECK(run_options->intra_op_thread_pool() != nullptr);
+
+  for (int64_t i = 0; i < batch_size; ++i) {
+     C.chip(i, 2).device(*run_options->intra_op_thread_pool()) = A.chip(i, 2).contract(B.chip(i, 2), dims);
+  }
+}
+
 template <typename T>
 void MatMulDispatch(const void* run_options_ptr, T* out, T* lhs, T* rhs,
                     int64_t m, int64_t n, int64_t k, int32_t transpose_lhs,
@@ -87,6 +130,22 @@ void MatMulDispatch(const void* run_options_ptr, T* out, T* lhs, T* rhs,
                               transpose_lhs, transpose_rhs);
 }
 
+template <typename T>
+void BatchMatMulDispatch(const void* run_options_ptr, T* out, T* lhs, T* rhs,
+                    int64_t m, int64_t n, int64_t k, int64_t batch_size,
+                    int32_t transpose_lhs, int32_t transpose_rhs) {
+  bool all_buffers_16b_aligned =
+      Is16BytesAligned(out) && Is16BytesAligned(lhs) && Is16BytesAligned(rhs);
+
+  if (!all_buffers_16b_aligned) {
+     MatMul_Batch<T, Eigen::Unaligned>(run_options_ptr, out, lhs, rhs, m, n, k,
+                                          batch_size, transpose_lhs, transpose_rhs);
+    return;
+  }
+  MatMul_Batch<T, Eigen::Aligned16>(run_options_ptr, out, lhs, rhs, m, n, k,
+                                       batch_size, transpose_lhs, transpose_rhs);
+}
+
 }  // namespace
 
 ABSL_ATTRIBUTE_NO_SANITIZE_MEMORY void __xla_cpu_runtime_EigenMatMulF16(
@@ -135,3 +194,10 @@ ABSL_ATTRIBUTE_NO_SANITIZE_MEMORY void __xla_cpu_runtime_EigenMatMulS32(
   MatMulDispatch<int32_t>(run_options_ptr, out, lhs, rhs, m, n, k,
                           transpose_lhs, transpose_rhs);
 }
+
+ABSL_ATTRIBUTE_NO_SANITIZE_MEMORY void __xla_cpu_runtime_EigenBatchMatMulF32(
+    const void* run_options_ptr, float* out, float* lhs, float* rhs, int64_t m,
+    int64_t n, int64_t k, int64_t batch_size, int32_t transpose_lhs, int32_t transpose_rhs) {
+  BatchMatMulDispatch<float>(run_options_ptr, out, lhs, rhs, m, n, k, batch_size, transpose_lhs,
+                        transpose_rhs);
+}
diff --git a/tensorflow/compiler/xla/service/cpu/runtime_matmul.h b/tensorflow/compiler/xla/service/cpu/runtime_matmul.h
index 2b13701224d..8d69126ab04 100644
--- a/tensorflow/compiler/xla/service/cpu/runtime_matmul.h
+++ b/tensorflow/compiler/xla/service/cpu/runtime_matmul.h
@@ -59,6 +59,10 @@ extern void __xla_cpu_runtime_EigenMatMulS32(
     int32_t* lhs, int32_t* rhs, int64_t m, int64_t n, int64_t k,
     int32_t transpose_lhs, int32_t transpose_rhs);
 
+extern void __xla_cpu_runtime_EigenBatchMatMulF32(
+    const void* /* xla::ExecutableRunOptions* */ run_options_ptr, float* out,
+    float* lhs, float* rhs, int64_t m, int64_t n, int64_t k,
+    int64_t batch_size, int32_t transpose_lhs, int32_t transpose_rhs);
 }  // extern "C"
 
 #endif  // TENSORFLOW_COMPILER_XLA_SERVICE_CPU_RUNTIME_MATMUL_H_
diff --git a/tensorflow/compiler/xla/service/cpu/runtime_matmul_acl.cc b/tensorflow/compiler/xla/service/cpu/runtime_matmul_acl.cc
new file mode 100644
index 00000000000..dcf4d252346
--- /dev/null
+++ b/tensorflow/compiler/xla/service/cpu/runtime_matmul_acl.cc
@@ -0,0 +1,194 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifdef XLA_CPU_USE_ACL
+#include "tensorflow/compiler/xla/service/cpu/runtime_matmul_acl.h"
+#include "tensorflow/compiler/xla/service/cpu/runtime_matmul.h"
+#include "tensorflow/compiler/xla/service/cpu/runtime_lightweight_check.h"
+#include "tensorflow/compiler/xla/executable_run_options.h"
+#include "tensorflow/core/platform/types.h"
+#include "tensorflow/core/platform/logging.h"
+
+#define EIGEN_USE_THREADS
+#include "third_party/eigen3/unsupported/Eigen/CXX11/Tensor"
+#include "tensorflow/core/platform/dynamic_annotations.h"
+
+namespace {
+// ACL GEMM API for 32-bit Matrix Multiplication.
+
+// MatMul function is defined as: c = alpha * op(a) * op(b) + beta * c.
+// Since XLA MatMul does not use alpha, beta, we set them to 1.0 and 0.0.
+// Matrix lhs, rhs and out are all column-major.
+int32_t MatMulF32(const void* run_options_ptr, float* out, float* lhs, float* rhs,
+                  int64_t m, int64_t n, int64_t k, int64_t batch_size, int32_t transpose_lhs,
+                  int32_t transpose_rhs) {
+  const float alpha = 1.0f, beta = 0.0f;
+
+  /* TODO: optimize this object creation along with tensor init and
+   * gemm configuration by caching the shapes, similar to onednn
+   * primitive caching feature
+   */
+  struct acl_matmul_obj_t acl_obj;
+  struct acl_matmul_conf_t acl_conf;
+
+  acl_conf.is_trans_lhs = (bool) transpose_lhs;
+  acl_conf.is_trans_rhs = (bool) transpose_rhs;
+
+  if (acl_conf.is_trans_lhs) {
+     acl_conf.lhs_acc_info = arm_compute::TensorInfo(arm_compute::TensorShape(k, m, batch_size),
+                                                     1, arm_compute::DataType::F32);
+  }
+  if (acl_conf.is_trans_rhs) {
+     acl_conf.rhs_acc_info = arm_compute::TensorInfo(arm_compute::TensorShape(n, k, 1, batch_size),
+		                                     1, arm_compute::DataType::F32);
+  }
+
+  acl_conf.lhs_info = arm_compute::TensorInfo(arm_compute::TensorShape(m, k, batch_size), 1,
+                                              arm_compute::DataType::F32);
+  acl_conf.rhs_info = arm_compute::TensorInfo(arm_compute::TensorShape(k, n, 1, batch_size),
+                                              1, arm_compute::DataType::F32);
+  acl_conf.out_info = arm_compute::TensorInfo(arm_compute::TensorShape(m, n, 1, batch_size),
+                                              1, arm_compute::DataType::F32);
+
+  /* TODO: add TF_XLA_* flag for runtime control of fast math mode*/
+  bool is_fastmath_enabled = true;
+  acl_conf.gemm_info.set_fast_math(is_fastmath_enabled);
+
+  // Fused ReLU activation
+  acl_conf.gemm_info.set_activation_info(arm_compute::ActivationLayerInfo());
+
+  // Set alpha (output scaling)
+  acl_conf.alpha = alpha;
+
+  // Validate ACL transpose
+  if (acl_conf.is_trans_lhs) {
+     auto acl_trans_lhs_st = arm_compute::NETranspose::validate(&acl_conf.lhs_acc_info,
+		                                                &acl_conf.lhs_info);
+     if (acl_trans_lhs_st.error_code() != arm_compute::ErrorCode::OK) {
+        VLOG(1) << "lhs transpose validation failed";
+	return -1;
+     }
+  }
+  if (acl_conf.is_trans_rhs) {
+     auto acl_trans_rhs_st = arm_compute::NETranspose::validate(&acl_conf.rhs_acc_info,
+		                                                &acl_conf.rhs_info);
+     if (acl_trans_rhs_st.error_code() != arm_compute::ErrorCode::OK) {
+        VLOG(1) << "rhs transpose validation failed";
+        return -1;
+     }
+  }
+
+  // Validate ACL GEMM
+  auto acl_st = arm_compute::NEGEMM::validate(&acl_conf.rhs_info, &acl_conf.lhs_info,
+                                              nullptr, &acl_conf.out_info, acl_conf.alpha,
+					      0.0f, acl_conf.gemm_info);
+  if (acl_st.error_code() != arm_compute::ErrorCode::OK) {
+     VLOG(1) << "validate acl GEMM FAILED";
+     return -1;
+  }
+
+  static std::once_flag flag_once;
+  const xla::ExecutableRunOptions* run_options = static_cast<const xla::ExecutableRunOptions*>(run_options_ptr);
+  XLA_LIGHTWEIGHT_CHECK(run_options->intra_op_thread_pool() != nullptr);
+  const Eigen::ThreadPoolDevice* tpd = (Eigen::ThreadPoolDevice*) (run_options->intra_op_thread_pool());
+  // The threads in Compute Library are bound for the cores 0..max_threads-1
+  const int max_threads = tpd->numThreads();
+
+  // arm_compute::Scheduler does not support concurrent access thus a
+  // workaround here restricts it to only one call
+  std::call_once(flag_once, [&]() {
+     arm_compute::Scheduler::get().set_num_threads(max_threads);
+  });
+
+  //configure the acl obj with the config
+  acl_obj.lhs_tensor.allocator()->init(acl_conf.lhs_info);
+  acl_obj.rhs_tensor.allocator()->init(acl_conf.rhs_info);
+  acl_obj.out_tensor.allocator()->init(acl_conf.out_info);
+
+  // Configure transpose kernel for src, wei or both
+  if (acl_conf.is_trans_lhs) {
+     acl_obj.lhs_acc_tensor.allocator()->init(acl_conf.lhs_acc_info);
+     acl_obj.trans_lhs.configure(&acl_obj.lhs_acc_tensor, &acl_obj.lhs_tensor);
+  }
+  if (acl_conf.is_trans_rhs) {
+     acl_obj.rhs_acc_tensor.allocator()->init(acl_conf.rhs_acc_info);
+     acl_obj.trans_rhs.configure(&acl_obj.rhs_acc_tensor, &acl_obj.rhs_tensor);
+  }
+  // Configure GEMM
+  acl_obj.gemm.configure(&acl_obj.rhs_tensor, &acl_obj.lhs_tensor,
+                         nullptr, &acl_obj.out_tensor, acl_conf.alpha,
+			 0.0f, acl_conf.gemm_info);
+
+  // Run transpose kernel
+  if (transpose_lhs && !transpose_rhs) {
+     acl_obj.lhs_tensor.allocator()->allocate();
+     acl_obj.lhs_acc_tensor.allocator()->import_memory(lhs);
+     acl_obj.trans_lhs.run();
+     acl_obj.rhs_tensor.allocator()->import_memory(rhs);
+  } else if (transpose_rhs && !transpose_lhs) {
+     acl_obj.rhs_tensor.allocator()->allocate();
+     acl_obj.rhs_acc_tensor.allocator()->import_memory(rhs);
+     acl_obj.trans_rhs.run();
+     acl_obj.lhs_tensor.allocator()->import_memory(lhs);
+  } else if (transpose_rhs && transpose_lhs) {
+     acl_obj.lhs_tensor.allocator()->allocate();
+     acl_obj.lhs_acc_tensor.allocator()->import_memory(lhs);
+     acl_obj.rhs_tensor.allocator()->allocate();
+     acl_obj.rhs_acc_tensor.allocator()->import_memory(rhs);
+     acl_obj.trans_lhs.run();
+     acl_obj.trans_rhs.run();
+  } else {
+     acl_obj.lhs_tensor.allocator()->import_memory(lhs);
+     acl_obj.rhs_tensor.allocator()->import_memory(rhs);
+  }
+
+  acl_obj.out_tensor.allocator()->import_memory(out);
+
+  // Execute the function
+  acl_obj.gemm.run();
+
+  acl_obj.lhs_tensor.allocator()->free();
+  acl_obj.rhs_tensor.allocator()->free();
+  acl_obj.out_tensor.allocator()->free();
+  if (acl_conf.is_trans_lhs) acl_obj.lhs_acc_tensor.allocator()->free();
+  if (acl_conf.is_trans_rhs) acl_obj.rhs_acc_tensor.allocator()->free();
+
+  return 0;
+}
+
+}  // namespace
+
+ABSL_ATTRIBUTE_NO_SANITIZE_MEMORY void __xla_cpu_runtime_ACLMatMulF32(
+    const void* run_options_ptr, float* out, float* lhs, float* rhs, int64_t m,
+    int64_t n, int64_t k, int32_t transpose_lhs, int32_t transpose_rhs) {
+
+  if (MatMulF32(run_options_ptr, out, lhs, rhs, m, n, k, 1/*batch_size*/, transpose_lhs, transpose_rhs) < 0) {
+     VLOG(1) << "ACL matmul failed, fallback to Eigen matmul";
+     __xla_cpu_runtime_EigenMatMulF32(run_options_ptr, out, lhs, rhs, m, n, k, transpose_lhs, transpose_rhs);
+  }
+}
+
+ABSL_ATTRIBUTE_NO_SANITIZE_MEMORY void __xla_cpu_runtime_ACLBatchMatMulF32(
+    const void* run_options_ptr, float* out, float* lhs, float* rhs, int64_t m,
+    int64_t n, int64_t k, int64_t batch_size, int32_t transpose_lhs, int32_t transpose_rhs) {
+
+  if (MatMulF32(run_options_ptr, out, lhs, rhs, m, n, k, batch_size, transpose_lhs, transpose_rhs) < 0) {
+     VLOG(1) << "ACL batch matmul failed, fallback to Eigen batch matmul";
+     __xla_cpu_runtime_EigenBatchMatMulF32(run_options_ptr, out, lhs, rhs, m, n, k,
+		                      batch_size, transpose_lhs, transpose_rhs);
+  }
+}
+
+#endif  // XLA_CPU_USE_ACL
diff --git a/tensorflow/compiler/xla/service/cpu/runtime_matmul_acl.h b/tensorflow/compiler/xla/service/cpu/runtime_matmul_acl.h
new file mode 100644
index 00000000000..955e1a045f2
--- /dev/null
+++ b/tensorflow/compiler/xla/service/cpu/runtime_matmul_acl.h
@@ -0,0 +1,81 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef TENSORFLOW_COMPILER_XLA_SERVICE_CPU_RUNTIME_MATMUL_ACL_H_
+#define TENSORFLOW_COMPILER_XLA_SERVICE_CPU_RUNTIME_MATMUL_ACL_H_
+
+#include <iostream>
+#include "tensorflow/core/platform/types.h"
+
+#ifdef XLA_CPU_USE_ACL
+#include "arm_compute/runtime/NEON/NEFunctions.h""
+#include "arm_compute/runtime/NEON/NEScheduler.h"
+#include "utils/Utils.h"
+
+struct acl_matmul_obj_t {
+  arm_compute::NEGEMM gemm;
+  arm_compute::NETranspose trans_lhs;
+  arm_compute::NETranspose trans_rhs;
+  arm_compute::Tensor rhs_tensor;
+  arm_compute::Tensor rhs_acc_tensor;
+  arm_compute::Tensor lhs_tensor;
+  arm_compute::Tensor lhs_acc_tensor;
+  arm_compute::Tensor out_tensor;
+};
+
+struct acl_matmul_conf_t {
+  bool with_bias;
+  bool is_trans_lhs;
+  bool is_trans_rhs;
+  arm_compute::TensorInfo lhs_info;
+  arm_compute::TensorInfo lhs_acc_info;
+  arm_compute::TensorInfo rhs_info;
+  arm_compute::TensorInfo rhs_acc_info;
+  arm_compute::TensorInfo out_info;
+  arm_compute::GEMMInfo gemm_info;
+  float alpha;
+};
+
+extern void __xla_cpu_runtime_ACLMatMulF32(
+    const void* /* xla::ExecutableRunOptions* */ run_options_ptr, float* out,
+    float* lhs, float* rhs, int64_t m, int64_t n, int64_t k,
+    int32_t transpose_lhs, int32_t transpose_rhs);
+
+extern void __xla_cpu_runtime_ACLBatchMatMulF32(
+    const void* /* xla::ExecutableRunOptions* */ run_options_ptr, float* out,
+    float* lhs, float* rhs, int64_t m, int64_t n, int64_t k,
+    int64_t batch_size, int32_t transpose_lhs, int32_t transpose_rhs);
+
+#else
+extern void __xla_cpu_runtime_ACLMatMulF32(
+    const void* /* xla::ExecutableRunOptions* */ run_options_ptr, float* out,
+    float* lhs, float* rhs, int64_t m, int64_t n, int64_t k,
+    int32_t transpose_lhs, int32_t transpose_rhs) {
+  std::cerr << "Attempt to call ACL MatMul runtime library without defining "
+               "XLA_CPU_USE_ACL. Add --define=build_with_acl=true to build with ACL.";
+  exit(1);
+}
+
+extern void __xla_cpu_runtime_ACLBatchMatMulF32(
+    const void* /* xla::ExecutableRunOptions* */ run_options_ptr, float* out,
+    float* lhs, float* rhs, int64_t m, int64_t n, int64_t k,
+    int64_t batch_size, int32_t transpose_lhs, int32_t transpose_rhs) {
+  std::cerr << "Attempt to call ACL MatMul runtime library without defining "
+               "XLA_CPU_USE_ACL. Add --define=build_with_acl=true to build with ACL.";
+  exit(1);
+}
+
+#endif  // XLA_CPU_USE_ACL
+#endif  // TENSORFLOW_COMPILER_XLA_SERVICE_CPU_RUNTIME_MATMUL_ACL_H_
diff --git a/tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc b/tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc
index 4f81cecce36..7c78920fba2 100644
--- a/tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc
+++ b/tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc
@@ -43,6 +43,8 @@ limitations under the License.
 #include "tensorflow/compiler/xla/service/cpu/runtime_key_value_sort.h"
 #include "tensorflow/compiler/xla/service/cpu/runtime_matmul.h"
 #include "tensorflow/compiler/xla/service/cpu/runtime_matmul_mkl.h"
+#include "tensorflow/compiler/xla/service/cpu/runtime_matmul_acl.h"
+#include "tensorflow/compiler/xla/service/cpu/runtime_conv2d_acl.h"
 #include "tensorflow/compiler/xla/service/cpu/runtime_pow.h"
 #include "tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.h"
 #include "tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv3d.h"
@@ -281,10 +283,14 @@ bool RegisterKnownJITSymbols() {
   REGISTER_CPU_RUNTIME_SYMBOL(EigenMatMulC64);
   REGISTER_CPU_RUNTIME_SYMBOL(EigenMatMulC128);
   REGISTER_CPU_RUNTIME_SYMBOL(EigenMatMulS32);
+  REGISTER_CPU_RUNTIME_SYMBOL(EigenBatchMatMulF32);
   REGISTER_CPU_RUNTIME_SYMBOL(MKLMatMulF32);
   REGISTER_CPU_RUNTIME_SYMBOL(MKLMatMulF64);
   REGISTER_CPU_RUNTIME_SYMBOL(MKLSingleThreadedMatMulF32);
   REGISTER_CPU_RUNTIME_SYMBOL(MKLSingleThreadedMatMulF64);
+  REGISTER_CPU_RUNTIME_SYMBOL(ACLMatMulF32);
+  REGISTER_CPU_RUNTIME_SYMBOL(ACLBatchMatMulF32);
+  REGISTER_CPU_RUNTIME_SYMBOL(ACLConv2DF32);
   REGISTER_CPU_RUNTIME_SYMBOL(EigenSingleThreadedConv2DF16);
   REGISTER_CPU_RUNTIME_SYMBOL(EigenSingleThreadedConv2DF32);
   REGISTER_CPU_RUNTIME_SYMBOL(EigenSingleThreadedConv3DF16);
diff --git a/tensorflow/compiler/xla/xla.proto b/tensorflow/compiler/xla/xla.proto
index 46e915541ae..6286f09b71f 100644
--- a/tensorflow/compiler/xla/xla.proto
+++ b/tensorflow/compiler/xla/xla.proto
@@ -409,7 +409,10 @@ message DebugOptions {
   // no-ops, e.g. `bf16 -> f32 -> bf16`. Removing these improves accuracy.
   bool xla_gpu_simplify_all_fp_conversions = 168;
 
-  // Next id: 171
+  // Generate calls to Arm Compute Library in the CPU backend.
+  bool xla_cpu_use_acl = 171;
+
+  // Next id: 172
 
   // Extra options to pass to the compilation backend (e.g. LLVM); specific
   // interpretation of these values is left to the backend.
diff --git a/tensorflow/tensorflow.bzl b/tensorflow/tensorflow.bzl
index f918da617fd..d179b1fab2a 100644
--- a/tensorflow/tensorflow.bzl
+++ b/tensorflow/tensorflow.bzl
@@ -42,6 +42,10 @@ load(
     "if_mkldnn_aarch64_acl",
     "if_mkldnn_openmp",
 )
+load(
+    "//third_party/compute_library:build_defs.bzl",
+    "if_enable_acl",
+)
 load(
     "//third_party/llvm_openmp:openmp.bzl",
     "windows_llvm_openmp_linkopts",
@@ -410,6 +414,14 @@ def tf_copts(
         })
     )
 
+def tf_xla_acl_opts_defines():
+    return [
+       "-DXLA_CPU_USE_ACL=1",
+    ]
+
+def tf_xla_acl_copts():
+    return if_enable_acl(tf_xla_acl_opts_defines())
+
 def tf_openmp_copts():
     # We assume when compiling on Linux gcc/clang will be used and MSVC on Windows
     return select({
diff --git a/third_party/compute_library/BUILD b/third_party/compute_library/BUILD
index e4f42b46d5c..38dd7015e94 100644
--- a/third_party/compute_library/BUILD
+++ b/third_party/compute_library/BUILD
@@ -136,3 +136,18 @@ cc_library(
         "include",
     ],
 )
+
+load("@bazel_skylib//:bzl_library.bzl", "bzl_library")
+config_setting(
+    name = "build_with_acl",
+    define_values = {
+        "build_with_acl": "true",
+    },
+    visibility = ["//visibility:public"],
+)
+
+bzl_library(
+    name = "build_defs_bzl",
+    srcs = ["build_defs.bzl"],
+    visibility = ["//visibility:public"],
+)
diff --git a/third_party/compute_library/build_defs.bzl b/third_party/compute_library/build_defs.bzl
new file mode 100644
index 00000000000..74102fd3e6d
--- /dev/null
+++ b/third_party/compute_library/build_defs.bzl
@@ -0,0 +1,20 @@
+def if_enable_acl(if_true, if_false = []):
+    return select({
+        "@org_tensorflow//third_party/compute_library:build_with_acl": if_true,
+        "//conditions:default": if_false,
+    })
+
+def acl_deps():
+    """Returns the correct set of ACL library dependencies.
+
+      Shorthand for select() to pull in the correct set of ACL library deps
+      for aarch64 platform
+
+    Returns:
+      a select evaluating to a list of library dependencies, suitable for
+      inclusion in the deps attribute of rules.
+    """
+    return select({
+        "@org_tensorflow//third_party/compute_library:build_with_acl": ["@compute_library//:arm_compute"],
+        "//conditions:default": [],
+    })
