 *******************************************************************************
 Copyright 2023-2024 Arm Limited and affiliates.
 SPDX-License-Identifier: Apache-2.0

 Licensed under the Apache License, Version 2.0 (the "License");
 you may not use this file except in compliance with the License.
 You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License.
 *******************************************************************************
diff --git a/Android.bp b/Android.bp
index 2983e2e21d..16923a2254 100644
--- a/Android.bp
+++ b/Android.bp
@@ -329,6 +329,7 @@ cc_library_static {
         "src/core/NEON/kernels/arm_gemm/gemm_int8.cpp",
         "src/core/NEON/kernels/arm_gemm/gemm_qint8.cpp",
         "src/core/NEON/kernels/arm_gemm/gemm_quint8.cpp",
+        "src/core/NEON/kernels/arm_gemm/gemm_s8fp32.cpp",
         "src/core/NEON/kernels/arm_gemm/gemm_uint16.cpp",
         "src/core/NEON/kernels/arm_gemm/gemm_uint8.cpp",
         "src/core/NEON/kernels/arm_gemm/interleave_indirect-sve.cpp",
diff --git a/filelist.json b/filelist.json
index 2f33b5cd5e..04b1204581 100644
--- a/filelist.json
+++ b/filelist.json
@@ -1588,6 +1588,7 @@
               "src/core/NEON/kernels/arm_gemm/gemm_fp32.cpp",
               "src/core/NEON/kernels/arm_gemm/gemm_int16.cpp",
               "src/core/NEON/kernels/arm_gemm/gemm_int8.cpp",
+              "src/core/NEON/kernels/arm_gemm/gemm_s8fp32.cpp",
               "src/core/NEON/kernels/arm_gemm/gemm_qint8.cpp",
               "src/core/NEON/kernels/arm_gemm/gemm_quint8.cpp",
               "src/core/NEON/kernels/arm_gemm/gemm_uint16.cpp",
diff --git a/src/BUILD.bazel b/src/BUILD.bazel
index 9d5ae63484..9f3d851027 100644
--- a/src/BUILD.bazel
+++ b/src/BUILD.bazel
@@ -515,6 +515,7 @@ filegroup(
 	"core/NEON/kernels/arm_gemm/gemm_int8.cpp",
 	"core/NEON/kernels/arm_gemm/gemm_qint8.cpp",
 	"core/NEON/kernels/arm_gemm/gemm_quint8.cpp",
+	"core/NEON/kernels/arm_gemm/gemm_s8fp32.cpp",
 	"core/NEON/kernels/arm_gemm/gemm_uint16.cpp",
 	"core/NEON/kernels/arm_gemm/gemm_uint8.cpp",
 	"core/NEON/kernels/arm_gemm/interleave_indirect.cpp",
diff --git a/src/CMakeLists.txt b/src/CMakeLists.txt
index be7a6ef188..a91f6573cf 100644
--- a/src/CMakeLists.txt
+++ b/src/CMakeLists.txt
@@ -506,6 +506,7 @@ target_sources(
 	core/NEON/kernels/arm_gemm/gemm_int8.cpp
 	core/NEON/kernels/arm_gemm/gemm_qint8.cpp
 	core/NEON/kernels/arm_gemm/gemm_quint8.cpp
+	core/NEON/kernels/arm_gemm/gemm_s8fp32.cpp
 	core/NEON/kernels/arm_gemm/gemm_uint16.cpp
 	core/NEON/kernels/arm_gemm/gemm_uint8.cpp
 	core/NEON/kernels/arm_gemm/interleave_indirect.cpp
diff --git a/src/core/NEON/kernels/arm_gemm/gemm_interleaved.hpp b/src/core/NEON/kernels/arm_gemm/gemm_interleaved.hpp
index 362a3e30ea..ccf8f4d656 100644
--- a/src/core/NEON/kernels/arm_gemm/gemm_interleaved.hpp
+++ b/src/core/NEON/kernels/arm_gemm/gemm_interleaved.hpp
@@ -1,5 +1,5 @@
 /*
- * Copyright (c) 2017-2023 Arm Limited.
+ * Copyright (c) 2017-2024 Arm Limited.
  *
  * SPDX-License-Identifier: MIT
  *
@@ -26,6 +26,8 @@
 #include <algorithm>
 #include <cassert>
 
+#include <arm_neon.h>
+
 #include "arm_gemm.hpp"
 #include "bfloat.hpp"
 #include "convolver.hpp"
@@ -247,6 +249,77 @@ void kernel_and_merge<true, false, Requantize32>::run(
     }
 }
 
+template<>
+template<typename strategy, typename To, typename Tr, typename Tri, typename Tab>
+void kernel_and_merge<true, false, DequantizeFloat>::run(
+#ifdef CYCLE_PROFILING
+        profiler &prof,
+#endif
+        strategy &strat, const To *a_ptr, const To *b_panel, size_t, Tri *c_panel,
+        Tr *c_ptr, int ldc, int kern_k, unsigned int m_0,
+        unsigned int m_max, unsigned int n_0, unsigned int n_max, const Tr *bias,
+        const Activation &, bool not_first_pass, const DequantizeFloat &qp, const int32_t *col_bias,
+        Tab *)
+{
+    const int bblocks = iceildiv(n_max - n_0, strategy::out_width());
+
+    {
+#ifdef CYCLE_PROFILING
+        auto p=prof.ScopedProfiler(PROFILE_KERNEL, (strategy::out_height() * bblocks * strategy::out_width() * kern_k));
+#endif
+
+        strat.kernel(a_ptr, b_panel, c_panel, 1, bblocks, kern_k);
+    }
+
+    {
+#ifdef CYCLE_PROFILING
+        auto p=prof.ScopedProfiler(PROFILE_QUANTIZE, ((m_max-m_0) * bblocks * strategy::out_width() * sizeof(Tr)));
+#endif
+        for (int i=0; i<bblocks; i++) {
+            unsigned int n_start = n_0 + (strategy::out_width() * i);
+            unsigned int n_end = std::min(n_start + strategy::out_width(), n_max);
+
+            const int32_t* in_ptr = c_panel + (i * strategy::out_width() * strategy::out_height());
+            float* out_ptr = c_ptr + m_0 * ldc + n_start;
+            const float* bias_ptr = bias != nullptr ? bias + n_start : nullptr;
+
+            unsigned int height = m_max-m_0;
+            int width = n_end-n_start;
+            unsigned int elements = height*width;
+
+            const float32x4_t vscale = vdupq_n_f32(qp.scale);
+
+            for(unsigned int row=0; row<height; row++) {
+                int col=0;
+                for(; col <= (width - 4); col+= 4) {
+                    const int32x4_t vin = vld1q_s32(in_ptr + col + (row * strategy::out_width()));
+                    float32x4_t vdeq = vmulq_f32(vcvtq_f32_s32(vin), vscale);
+                    if(bias_ptr) {
+                        const float32x4_t bin = vld1q_f32(bias_ptr + col);
+                        vdeq = vaddq_f32(vdeq, bin);
+                    }
+                    if(not_first_pass) {
+                        vdeq = vaddq_f32(vdeq, vld1q_f32(out_ptr + col + (row * ldc)));
+                    }
+                    vst1q_f32(reinterpret_cast<float *>(out_ptr + col + (row * ldc)), vdeq);
+                }
+                    // left-over elements
+                for(; col < width; ++col) {
+                    const int32_t val = *(in_ptr + (row * strategy::out_width()) + col);
+                    float res = static_cast<float>(val * qp.scale);
+                    if(bias_ptr) {
+                        res += static_cast<float>(*(bias_ptr + col));
+                    }
+                    if(not_first_pass) {
+                        res += *(out_ptr + (row * ldc) + col);
+                    }
+                    *(out_ptr + (row * ldc) + col) = res;
+                }
+            }
+        }
+    }
+}
+
 // Integer GEMMs can be used in two contexts - "normal" where the full 32-bit output is required, or in
 // "requantizing" context where the output will be requantized.
 //
@@ -1176,6 +1249,13 @@ public:
         }
     }
 
+    void set_dequantize_scale(const float scale) {
+        if(std::is_same<OutputStage, DequantizeFloat>::value) {
+            DequantizeFloat* df = reinterpret_cast<DequantizeFloat *>(&_os);
+            df->scale = scale;
+        }
+    }
+
     void set_indirect_parameters(size_t string_len, const To * const * const *ptr) override {
         assert(string_len == _Ksize);
         _indirect_buf = ptr;
@@ -1240,4 +1320,7 @@ using GemmInterleavedPretransposedNoMergeQuantizedInline = GemmInterleaved<strat
 template<typename strategy, typename To, typename Tr>
 using GemmInterleavedQuantized = GemmInterleaved<strategy, To, Tr, Requantize32>;
 
+template<typename strategy, typename To, typename Tr>
+using GemmInterleavedDequantized = GemmInterleaved<strategy, To, Tr, DequantizeFloat>;
+
 } // namespace arm_gemm
diff --git a/src/core/NEON/kernels/arm_gemm/gemm_s8fp32.cpp b/src/core/NEON/kernels/arm_gemm/gemm_s8fp32.cpp
new file mode 100644
index 0000000000..3b499fae27
--- /dev/null
+++ b/src/core/NEON/kernels/arm_gemm/gemm_s8fp32.cpp
@@ -0,0 +1,134 @@
+/*
+ * Copyright (c) 2023-2024 Arm Limited.
+ *
+ * SPDX-License-Identifier: MIT
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to
+ * deal in the Software without restriction, including without limitation the
+ * rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
+ * sell copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in all
+ * copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+#ifdef __aarch64__
+
+#include "arm_gemm.hpp"
+
+#include "kernels/a64_gemm_s16_8x12.hpp"
+#include "kernels/a64_gemm_s8_4x4.hpp"
+#include "kernels/a64_gemm_s8_8x12.hpp"
+#include "kernels/a64_hybrid_s8qa_dot_4x16.hpp"
+#include "kernels/a64_hybrid_s8qa_mmla_4x16.hpp"
+#include "kernels/a64_hybrid_s8qs_dot_6x16.hpp"
+#include "kernels/a64_hybrid_s8qs_mmla_6x16.hpp"
+#include "kernels/a64_hybrid_s8s32_dot_6x16.hpp"
+#include "kernels/a64_hybrid_s8s32_mmla_6x16.hpp"
+#include "kernels/a64_interleaved_s8s32_mmla_8x12.hpp"
+#include "kernels/a64_smallK_hybrid_s8s32_dot_6x4.hpp"
+#include "kernels/a64_smallK_hybrid_s8s32_dot_8x4.hpp"
+
+#ifdef ARM_COMPUTE_ENABLE_SVE
+#ifdef ARM_COMPUTE_ENABLE_SME2
+#if defined(EXPERIMENTAL_KERNELS)
+#include "kernels/sme_gemv_s8qa_dot_8VL.hpp"
+#include "kernels/sme_interleaved_nomerge_s8q_mopa_1VLx4VL.hpp"
+#include "kernels/sme_interleaved_nomerge_s8q_mopa_2VLx2VL.hpp"
+#include "kernels/sme_interleaved_nomerge_s8q_mopa_4VLx1VL.hpp"
+#endif // defined(EXPERIMENTAL_KERNELS)
+#include "kernels/sme2_gemv_s8qa_dot_16VL.hpp"
+#include "kernels/sme2_interleaved_nomerge_s8q_mopa_1VLx4VL.hpp"
+#include "kernels/sme2_interleaved_nomerge_s8q_mopa_2VLx2VL.hpp"
+#include "kernels/sme2_interleaved_nomerge_s8q_mopa_4VLx1VL.hpp"
+#endif // ARM_COMPUTE_ENABLE_SME2
+
+#include "kernels/sve_interleaved_s8s32_dot_8x3VL.hpp"
+#include "kernels/sve_interleaved_s8s32_mmla_8x3VL.hpp"
+#endif // ARM_COMPUTE_ENABLE_SVE
+
+#include "gemm_interleaved.hpp"
+#include "gemv_pretransposed.hpp"
+#include "quantize_wrapper.hpp"
+#include "utils.hpp"
+
+namespace arm_gemm {
+
+static const GemmImplementation<int8_t, float, DequantizeFloat> gemm_s8fp32_methods[] =
+{
+#ifdef ARM_COMPUTE_ENABLE_SVE
+GemmImplementation<int8_t, float, DequantizeFloat>::with_estimate(
+    GemmMethod::GEMM_INTERLEAVED,
+    "sve_interleaved_s8s32_mmla_8x3VL",
+    [](const GemmArgs &args, const DequantizeFloat &) { return args._ci->has_svei8mm(); },
+    [](const GemmArgs &args, const DequantizeFloat &) { return GemmInterleavedDequantized<cls_sve_interleaved_s8s32_mmla_8x3VL, int8_t, float>::estimate_cycles<int8_t>(args); },
+    [](const GemmArgs &args, const DequantizeFloat &qp) { return new GemmInterleavedDequantized<cls_sve_interleaved_s8s32_mmla_8x3VL, int8_t, float>(args, qp); }
+),
+GemmImplementation<int8_t, float, DequantizeFloat>::with_estimate(
+    GemmMethod::GEMM_INTERLEAVED,
+    "sve_interleaved_s8s32_dot_8x3VL",
+    [](const GemmArgs &args, const DequantizeFloat &) { return args._ci->has_sve(); },
+    [](const GemmArgs &args, const DequantizeFloat &) { return GemmInterleavedDequantized<cls_sve_interleaved_s8s32_dot_8x3VL, int8_t, float>::estimate_cycles<int8_t>(args); },
+    [](const GemmArgs &args, const DequantizeFloat &qp) { return new GemmInterleavedDequantized<cls_sve_interleaved_s8s32_dot_8x3VL, int8_t, float>(args, qp); }
+),
+#endif // ARM_COMPUTE_ENABLE_SVE
+GemmImplementation<int8_t, float, DequantizeFloat>::with_estimate(
+    GemmMethod::GEMM_INTERLEAVED,
+    "a64_interleaved_s8s32_mmla_8x12",
+    [](const GemmArgs &args, const DequantizeFloat &) { return args._ci->has_i8mm(); },
+    [](const GemmArgs &args, const DequantizeFloat &) { return GemmInterleavedDequantized<cls_a64_interleaved_s8s32_mmla_8x12, int8_t, float>::estimate_cycles<int8_t>(args); },
+    [](const GemmArgs &args, const DequantizeFloat &qp) { return new GemmInterleavedDequantized<cls_a64_interleaved_s8s32_mmla_8x12, int8_t, float>(args, qp); }
+),
+#if 0
+{
+    GemmMethod::GEMM_INTERLEAVED,
+    "a64_gemm_s16_8x12",
+    nullptr,
+    [](const GemmArgs &args, const DequantizeFloat &) { return args._ci->get_cpu_model() == CPUModel::A53 && ((args._Msize > 28) || ((args._Msize % 8) > 4)); },
+    [](const GemmArgs &args, const DequantizeFloat &qp) { return new GemmInterleavedDequantized<cls_a64_gemm_s16_8x12, int8_t, float>(args, qp); }
+},
+GemmImplementation<int8_t, float, DequantizeFloat>::with_estimate(
+    GemmMethod::GEMM_INTERLEAVED,
+    "a64_gemm_s8_8x12",
+    [](const GemmArgs &args, const DequantizeFloat &) { return args._ci->has_dotprod(); },
+    [](const GemmArgs &args, const DequantizeFloat &) { return GemmInterleavedDequantized<cls_a64_gemm_s8_8x12, int8_t, float>::estimate_cycles<int8_t>(args); },
+    [](const GemmArgs &args, const DequantizeFloat &qp) { return new GemmInterleavedDequantized<cls_a64_gemm_s8_8x12, int8_t, float>(args, qp); }
+),
+GemmImplementation<int8_t, float, DequantizeFloat>::with_estimate(
+    GemmMethod::GEMM_INTERLEAVED,
+    "a64_gemm_s8_4x4",
+    nullptr,
+    [](const GemmArgs &args, const DequantizeFloat &) { return GemmInterleavedDequantized<cls_a64_gemm_s8_4x4, int8_t, float>::estimate_cycles<int8_t>(args); },
+    [](const GemmArgs &args, const DequantizeFloat &qp) { return new GemmInterleavedDequantized<cls_a64_gemm_s8_4x4, int8_t, float>(args, qp); }
+),
+#endif
+{
+    GemmMethod::DEFAULT,
+    "",
+    nullptr,
+    nullptr,
+    nullptr
+}
+};
+
+template<>
+const GemmImplementation<int8_t, float, DequantizeFloat> *gemm_implementation_list<int8_t, float, DequantizeFloat>() {
+    return gemm_s8fp32_methods;
+}
+
+template UniqueGemmCommon<int8_t, float> gemm<int8_t, float, DequantizeFloat>(const GemmArgs &args, const DequantizeFloat &os);
+template KernelDescription get_gemm_method<int8_t, float, DequantizeFloat>(const GemmArgs &args, const DequantizeFloat &os);
+template std::vector<KernelDescription> get_compatible_kernels<int8_t, float, DequantizeFloat>(const GemmArgs &args, const DequantizeFloat &os);
+
+} // namespace arm_gemm
+
+#endif // __aarch64__
\ No newline at end of file
diff --git a/src/cpu/kernels/CpuGemmLowpMatrixReductionKernel.cpp b/src/cpu/kernels/CpuGemmLowpMatrixReductionKernel.cpp
index 9a099bd1b6..c5d232a0ec 100644
--- a/src/cpu/kernels/CpuGemmLowpMatrixReductionKernel.cpp
+++ b/src/cpu/kernels/CpuGemmLowpMatrixReductionKernel.cpp
@@ -315,7 +315,7 @@ void CpuGemmLowpMatrixBReductionKernel::run_internal(const ITensor    *src,
                 // Row index
                 int i = 0;
                 // 4 x u/int32x4_t = 16 columns unrolled across 4 rows
-                for (; i <= (_k - 4); i += 4)
+                for (; i <= (_k - 4) && (width_matrix_b - id.x()) >= 16; i += 4)
                 {
                     // Load 4 rows of 16 columns of 8bit elements
                     // (|                   |        )
@@ -363,36 +363,34 @@ void CpuGemmLowpMatrixBReductionKernel::run_internal(const ITensor    *src,
                 // This for loop accumulates the rows left over from the 4x unrolling above
                 for (; i < _k; ++i)
                 {
-                    const auto b0_b8 = wrapper::vloadq(matrix_b + 0 * in_b_stride);
+                    if ((width_matrix_b - id.x()) >= 16)
+                    {
+                        const auto b0_b8 = wrapper::vloadq(matrix_b + 0 * in_b_stride);
 
-                    // Convert 8bit => 16bit
-                    const typename wrapper::traits::neon_bitvector<TIAcc, wrapper::traits::BitWidth::W128>::type
-                        b0_b16[2]{wrapper::vmovl(wrapper::vgetlow(b0_b8)), wrapper::vmovl(wrapper::vgethigh(b0_b8))};
+                        // Convert S8 to S16
+                        const typename wrapper::traits::neon_bitvector<TIAcc, wrapper::traits::BitWidth::W128>::type
+                            b0_b16[2]{wrapper::vmovl(wrapper::vgetlow(b0_b8)),
+                                      wrapper::vmovl(wrapper::vgethigh(b0_b8))};
 
-                    // Accumulate to 32bit
-                    sum_col[0] = wrapper::vaddw(sum_col[0], wrapper::vgetlow(b0_b16[0]));
-                    sum_col[1] = wrapper::vaddw(sum_col[1], wrapper::vgethigh(b0_b16[0]));
-                    sum_col[2] = wrapper::vaddw(sum_col[2], wrapper::vgetlow(b0_b16[1]));
-                    sum_col[3] = wrapper::vaddw(sum_col[3], wrapper::vgethigh(b0_b16[1]));
+                        // Accumulate to 32bit
+                        sum_col[0] = wrapper::vaddw(sum_col[0], wrapper::vgetlow(b0_b16[0]));
+                        sum_col[1] = wrapper::vaddw(sum_col[1], wrapper::vgethigh(b0_b16[0]));
+                        sum_col[2] = wrapper::vaddw(sum_col[2], wrapper::vgetlow(b0_b16[1]));
+                        sum_col[3] = wrapper::vaddw(sum_col[3], wrapper::vgethigh(b0_b16[1]));
 
-                    matrix_b += in_b_stride;
-                }
-            }
-            else
-            {
-                // Accumulate left over columns to sum_cols
-                for (int i = 0; i < _k; ++i) // row loop
-                {
-                    auto left_over_cols = width_matrix_b - id.x();
-                    auto l              = left_over_cols;
-                    for (auto k = 0; k < 4 && l; ++k)
+                        matrix_b += in_b_stride;
+                    }
+                    else
                     {
-                        for (auto j = 0; j < 4 && l; ++j, --l)
+                        auto left_over = width_matrix_b - id.x();
+                        for (auto k = 0; k < 4 && left_over; ++k)
                         {
-                            sum_col[k][j] += matrix_b[left_over_cols - l];
+                            for (auto j = 0; j < 4 && left_over; ++j, --left_over)
+                            {
+                                sum_col[k][j] += matrix_b[i * width_matrix_b + ((width_matrix_b - id.x()) - left_over)];
+                            }
                         }
                     }
-                    matrix_b += in_b_stride;
                 }
             }
 
diff --git a/src/cpu/kernels/CpuGemmLowpOffsetContributionKernel.cpp b/src/cpu/kernels/CpuGemmLowpOffsetContributionKernel.cpp
index e290783021..ffa7ce475d 100644
--- a/src/cpu/kernels/CpuGemmLowpOffsetContributionKernel.cpp
+++ b/src/cpu/kernels/CpuGemmLowpOffsetContributionKernel.cpp
@@ -1,5 +1,5 @@
 /*
- * Copyright (c) 2017-2022 Arm Limited.
+ * Copyright (c) 2017-2022, 2024 Arm Limited.
  *
  * SPDX-License-Identifier: MIT
  *
@@ -51,7 +51,7 @@ Status validate_arguments(const ITensorInfo *mm_result,
                           int32_t            a_offset,
                           int32_t            b_offset)
 {
-    ARM_COMPUTE_RETURN_ERROR_ON_DATA_TYPE_CHANNEL_NOT_IN(mm_result, 1, DataType::S32);
+    ARM_COMPUTE_RETURN_ERROR_ON_DATA_TYPE_CHANNEL_NOT_IN(mm_result, 1, DataType::S32, DataType::F32);
 
     // If a_offset == 0, vector_sum_col can be a nullptr
     if (a_offset != 0)
@@ -102,6 +102,101 @@ Status validate_arguments(const ITensorInfo *mm_result,
     return Status{};
 }
 
+void run_offset_contribution_float(const Window  &window,
+                                   ITensor       *mm_result,
+                                   const ITensor *vector_sum_col,
+                                   const ITensor *vector_sum_row,
+                                   int32_t        a_offset,
+                                   bool           slide_vector_sum_col,
+                                   bool           is_gemm3d,
+                                   float          scale)
+{
+    Window collapsed_window = window.collapse_if_possible(window, Window::DimZ);
+    collapsed_window.set(Window::DimX, Window::Dimension(0, 1, 1));
+
+    const int height_input = is_gemm3d ? mm_result->info()->dimension(1) : 0;
+    const int depth_input  = is_gemm3d ? mm_result->info()->dimension(2) : 1;
+
+    const int window_start_x = window.x().start();
+    const int window_end_x   = window.x().end();
+    const int window_step_x  = 16;
+
+    // if vector_sum_col is nullptr then stride_y is 0, else get stride_y
+    const size_t sum_col_stride_y = (vector_sum_col != nullptr) ? (vector_sum_col->info()->strides_in_bytes().y()) : 0;
+    Iterator     mm_result_it(mm_result, collapsed_window);
+
+    // we always here assume that a_offset != 0 and b_offset is always zero
+    Window win_vector_sum_col(collapsed_window);
+    win_vector_sum_col.set(Window::DimY, Window::Dimension(0, 0, 0));
+    win_vector_sum_col.set(Window::DimZ, Window::Dimension(0, 0, 0));
+
+    Iterator vector_sum_col_it(vector_sum_col, win_vector_sum_col);
+
+    // Offset in case vector_sum_col is batched
+    const int vector_sum_col_batch_offset = slide_vector_sum_col ? vector_sum_col->info()->strides_in_bytes().z() : 0;
+
+    execute_window_loop(
+        collapsed_window,
+        [&](const Coordinates &id)
+        {
+            const int    batch_id = id.z() / depth_input;
+            const size_t batch_offset_col =
+                batch_id *
+                (sum_col_stride_y); // Value to offset vector_sum_col_ptr to allow for iteration of y values in tensor
+            auto vector_sum_col_ptr = reinterpret_cast<const int32_t *>(vector_sum_col_it.ptr() + batch_offset_col +
+                                                                        batch_id * vector_sum_col_batch_offset);
+            auto mm_result_ptr      = reinterpret_cast<float *>(mm_result_it.ptr());
+
+            int x = window_start_x;
+            for (; x <= (window_end_x - window_step_x); x += window_step_x)
+            {
+                // Compute the leftover term due to a_offset.
+                int32x4x4_t a_offset_term_s32 = {
+                    {vld1q_s32(vector_sum_col_ptr + x + 0), vld1q_s32(vector_sum_col_ptr + x + 4),
+                     vld1q_s32(vector_sum_col_ptr + x + 8), vld1q_s32(vector_sum_col_ptr + x + 12)}};
+
+                a_offset_term_s32.val[0] = vmulq_n_s32(a_offset_term_s32.val[0], a_offset);
+                a_offset_term_s32.val[1] = vmulq_n_s32(a_offset_term_s32.val[1], a_offset);
+                a_offset_term_s32.val[2] = vmulq_n_s32(a_offset_term_s32.val[2], a_offset);
+                a_offset_term_s32.val[3] = vmulq_n_s32(a_offset_term_s32.val[3], a_offset);
+
+                float32x4x4_t a_offset_term_scaled = {{
+                    vmulq_n_f32(vcvtq_f32_s32(a_offset_term_s32.val[0]), scale),
+                    vmulq_n_f32(vcvtq_f32_s32(a_offset_term_s32.val[1]), scale),
+                    vmulq_n_f32(vcvtq_f32_s32(a_offset_term_s32.val[2]), scale),
+                    vmulq_n_f32(vcvtq_f32_s32(a_offset_term_s32.val[3]), scale),
+                }};
+
+                float32x4x4_t in_s32 = {{vld1q_f32(mm_result_ptr + x + 0), vld1q_f32(mm_result_ptr + x + 4),
+                                         vld1q_f32(mm_result_ptr + x + 8), vld1q_f32(mm_result_ptr + x + 12)}};
+
+                // Add the offset terms to GEMM's result
+                in_s32.val[0] = vaddq_f32(in_s32.val[0], a_offset_term_scaled.val[0]);
+                in_s32.val[1] = vaddq_f32(in_s32.val[1], a_offset_term_scaled.val[1]);
+                in_s32.val[2] = vaddq_f32(in_s32.val[2], a_offset_term_scaled.val[2]);
+                in_s32.val[3] = vaddq_f32(in_s32.val[3], a_offset_term_scaled.val[3]);
+
+                // Store the result with the offset contribution
+                vst1q_f32(mm_result_ptr + x + 0, in_s32.val[0]);
+                vst1q_f32(mm_result_ptr + x + 4, in_s32.val[1]);
+                vst1q_f32(mm_result_ptr + x + 8, in_s32.val[2]);
+                vst1q_f32(mm_result_ptr + x + 12, in_s32.val[3]);
+            }
+
+            // Left-overs loop
+            for (; x < window_end_x; ++x)
+            {
+                // Compute the leftover term due to a_offset.
+                const int32_t a_offset_term_s32 = *(vector_sum_col_ptr + x);
+
+                // Add the offset terms to GEMM's result
+                // Store the result with the offset contribution
+                mm_result_ptr[x] += a_offset_term_s32 * a_offset * scale;
+            }
+        },
+        vector_sum_col_it, mm_result_it);
+}
+
 void run_offset_contribution(const Window  &window,
                              ITensor       *mm_result,
                              const ITensor *vector_sum_col,
@@ -410,8 +505,17 @@ void CpuGemmLowpOffsetContributionKernel::run_op(ITensorPack &tensors, const Win
     const bool reinterpret_as_3d = vector_sum_row != nullptr && mm_result->info()->num_dimensions() > 1 &&
                                    mm_result->info()->tensor_shape().y() != vector_sum_row->info()->tensor_shape().x();
 
-    run_offset_contribution(window, mm_result, vector_sum_col, vector_sum_row, _a_offset, _b_offset, _k_offset,
-                            _slide_vector_sum_col, reinterpret_as_3d);
+    // check to see what is the output type of result
+    if (mm_result->info()->data_type() == DataType::F32)
+    {
+        run_offset_contribution_float(window, mm_result, vector_sum_col, vector_sum_row, _a_offset,
+                                      _slide_vector_sum_col, reinterpret_as_3d, _scale);
+    }
+    else
+    {
+        run_offset_contribution(window, mm_result, vector_sum_col, vector_sum_row, _a_offset, _b_offset, _k_offset,
+                                _slide_vector_sum_col, reinterpret_as_3d);
+    }
 }
 
 const char *CpuGemmLowpOffsetContributionKernel::name() const
diff --git a/src/cpu/kernels/CpuGemmLowpOffsetContributionKernel.h b/src/cpu/kernels/CpuGemmLowpOffsetContributionKernel.h
index 08b2d47529..48a94aeb13 100644
--- a/src/cpu/kernels/CpuGemmLowpOffsetContributionKernel.h
+++ b/src/cpu/kernels/CpuGemmLowpOffsetContributionKernel.h
@@ -1,5 +1,5 @@
 /*
- * Copyright (c) 2017-2022 Arm Limited.
+ * Copyright (c) 2017-2022, 2024 Arm Limited.
  *
  * SPDX-License-Identifier: MIT
  *
@@ -21,8 +21,8 @@
  * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
  * SOFTWARE.
  */
-#ifndef ARM_COMPUTE_CPU_GEMMLOWP_OFFSETCONTRIBUTION_KERNEL_H
-#define ARM_COMPUTE_CPU_GEMMLOWP_OFFSETCONTRIBUTION_KERNEL_H
+#ifndef ACL_SRC_CPU_KERNELS_CPUGEMMLOWPOFFSETCONTRIBUTIONKERNEL_H
+#define ACL_SRC_CPU_KERNELS_CPUGEMMLOWPOFFSETCONTRIBUTIONKERNEL_H
 
 #include "src/core/common/Macros.h"
 #include "src/cpu/ICpuKernel.h"
@@ -85,13 +85,24 @@ public:
     void        run_op(ITensorPack &tensors, const Window &window, const ThreadInfo &info) override;
     const char *name() const override;
 
+    void update_a_offset(int32_t a_offset)
+    {
+        _a_offset = a_offset;
+    }
+
+    void update_scale(float scale)
+    {
+        _scale = scale;
+    }
+
 private:
     int32_t _a_offset{0};
     int32_t _b_offset{0};
     int32_t _k_offset{0};
+    float   _scale{1.0};
     bool    _slide_vector_sum_col{true};
 };
 } // namespace kernels
 } // namespace cpu
 } // namespace arm_compute
-#endif /* ARM_COMPUTE_CPU_GEMMLOWP_OFFSETCONTRIBUTION_KERNEL_H */
+#endif // ACL_SRC_CPU_KERNELS_CPUGEMMLOWPOFFSETCONTRIBUTIONKERNEL_H
diff --git a/src/cpu/kernels/assembly/arm_gemm.hpp b/src/cpu/kernels/assembly/arm_gemm.hpp
index 9a913c5c58..d8156ed54e 100644
--- a/src/cpu/kernels/assembly/arm_gemm.hpp
+++ b/src/cpu/kernels/assembly/arm_gemm.hpp
@@ -1,5 +1,5 @@
 /*
- * Copyright (c) 2018-2022 Arm Limited.
+ * Copyright (c) 2018-2022, 2024 Arm Limited.
  *
  * SPDX-License-Identifier: MIT
  *
@@ -21,6 +21,10 @@
  * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
  * SOFTWARE.
  */
+
+#ifndef ACL_SRC_CPU_KERNELS_ASSEMBLY_ARM_GEMM_HPP
+#define ACL_SRC_CPU_KERNELS_ASSEMBLY_ARM_GEMM_HPP
+
 #pragma once
 
 #include "arm_gemm_local.hpp"
@@ -253,6 +257,19 @@ public:
     }
 };
 
+struct DequantizeFloat
+{
+public:
+    float scale = 0;
+
+    DequantizeFloat() = default;
+
+    // Constructor
+    DequantizeFloat(const float scale) : scale(scale)
+    {
+    }
+};
+
 struct Nothing
 {
 };
@@ -278,3 +295,5 @@ template <typename Top, typename Tret, class OutputStage = Nothing>
 bool has_opt_gemm(WeightFormat &weight_format, const GemmArgs &args, const OutputStage & = {});
 
 } // namespace arm_gemm
+
+#endif // ACL_SRC_CPU_KERNELS_ASSEMBLY_ARM_GEMM_HPP
diff --git a/src/cpu/kernels/assembly/gemm_common.hpp b/src/cpu/kernels/assembly/gemm_common.hpp
index 6fe9f13f02..251a559366 100644
--- a/src/cpu/kernels/assembly/gemm_common.hpp
+++ b/src/cpu/kernels/assembly/gemm_common.hpp
@@ -1,5 +1,5 @@
 /*
- * Copyright (c) 2017-2021,2023 Arm Limited.
+ * Copyright (c) 2017-2021,2023-2024 Arm Limited.
  *
  * SPDX-License-Identifier: MIT
  *
@@ -21,6 +21,10 @@
  * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
  * SOFTWARE.
  */
+
+#ifndef ACL_SRC_CPU_KERNELS_ASSEMBLY_GEMM_COMMON_HPP
+#define ACL_SRC_CPU_KERNELS_ASSEMBLY_GEMM_COMMON_HPP
+
 #pragma once
 
 #include "convolution_parameters.hpp"
@@ -157,6 +161,10 @@ public:
     {
     }
 
+    virtual void set_dequantize_scale(const float)
+    {
+    }
+
     /*** Introspection interface ***/
     /* Get the configuration of this GEMM */
     virtual GemmConfig get_config() = 0;
@@ -287,3 +295,5 @@ public:
 };
 
 } // namespace arm_gemm
+
+#endif // ACL_SRC_CPU_KERNELS_ASSEMBLY_GEMM_COMMON_HPP
diff --git a/src/cpu/kernels/internal/CpuDepthwiseConv2dAssemblyWrapperKernel.cpp b/src/cpu/kernels/internal/CpuDepthwiseConv2dAssemblyWrapperKernel.cpp
index 296fe88791..84bc784997 100644
--- a/src/cpu/kernels/internal/CpuDepthwiseConv2dAssemblyWrapperKernel.cpp
+++ b/src/cpu/kernels/internal/CpuDepthwiseConv2dAssemblyWrapperKernel.cpp
@@ -1,5 +1,5 @@
 /*
- * Copyright (c) 2021-2023 Arm Limited.
+ * Copyright (c) 2021-2024 Arm Limited.
  *
  * SPDX-License-Identifier: MIT
  *
@@ -70,15 +70,24 @@ void create_arm_dwc(const ITensorInfo                                      *src,
 
     const arm_conv::PaddingValues padding = assembly_utils::map_to_arm_conv_padding(info.pad_stride_info);
 
-    const unsigned int n_batches  = src->dimension(idx_batches);
-    const unsigned int src_rows   = src->dimension(idx_height);
+    unsigned int       n_batches   = src->dimension(idx_batches);
+    unsigned int       src_rows    = src->dimension(idx_height);
+    unsigned int       dst_rows    = dst->dimension(idx_height);
+    const unsigned int kernel_rows = weights->dimension(idx_height);
+
+    if (n_batches > 1 && src_rows == 1 && dst_rows == 1 && kernel_rows == 1 && padding.left == 0 && padding.right == 0)
+    {
+        // Casting batch into row
+        src_rows  = n_batches;
+        dst_rows  = n_batches;
+        n_batches = 1;
+    }
+
     const unsigned int src_cols   = src->dimension(idx_width);
     const unsigned int n_channels = src->dimension(idx_channels);
-    const unsigned int dst_rows   = dst->dimension(idx_height);
     const unsigned int dst_cols   = dst->dimension(idx_width);
 
     const unsigned int kernel_cols = weights->dimension(idx_width);
-    const unsigned int kernel_rows = weights->dimension(idx_height);
 
     const arm_gemm::Activation activation = assembly_utils::map_to_arm_gemm_activation(info.act_info);
 
diff --git a/src/cpu/operators/CpuGemmLowpMatrixMultiplyCore.cpp b/src/cpu/operators/CpuGemmLowpMatrixMultiplyCore.cpp
index b25505a85d..1cb2c02a9b 100644
--- a/src/cpu/operators/CpuGemmLowpMatrixMultiplyCore.cpp
+++ b/src/cpu/operators/CpuGemmLowpMatrixMultiplyCore.cpp
@@ -1,5 +1,5 @@
 /*
- * Copyright (c) 2021-2023 Arm Limited.
+ * Copyright (c) 2021-2024 Arm Limited.
  *
  * SPDX-License-Identifier: MIT
  *
@@ -228,15 +228,11 @@ void CpuGemmLowpMatrixMultiplyCore::configure(
         // Build reduction info
         const GEMMLowpReductionKernelInfo reduction_info(a_to_use->dimension(0), false, 0, false);
 
-        // Initialize matrix B reduction kernel only if _a_offset is not equal to 0
-        if (_a_offset != 0)
-        {
-            _vector_sum_col = TensorInfo(compute_reductionA_shape(*b), 1, DataType::S32);
+        _vector_sum_col = TensorInfo(compute_reductionA_shape(*b), 1, DataType::S32);
 
-            // Configure Matrix B reduction kernel
-            _mtx_b_reduction_kernel = std::make_unique<kernels::CpuGemmLowpMatrixBReductionKernel>();
-            _mtx_b_reduction_kernel->configure(b, &_vector_sum_col, reduction_info);
-        }
+        // Configure Matrix B reduction kernel
+        _mtx_b_reduction_kernel = std::make_unique<kernels::CpuGemmLowpMatrixBReductionKernel>();
+        _mtx_b_reduction_kernel->configure(b, &_vector_sum_col, reduction_info);
 
         // Initialize Matrix A reduction kernel only if _b_offset is not equal to 0
         if (_b_offset != 0)
@@ -260,9 +256,9 @@ void CpuGemmLowpMatrixMultiplyCore::configure(
             _offset_contribution_output_stage_kernel =
                 std::make_unique<kernels::CpuGemmLowpOffsetContributionOutputStageKernel>();
             _offset_contribution_output_stage_kernel->configure(
-                &_mm_result_s32, _a_offset == 0 ? nullptr : &_vector_sum_col,
-                _b_offset == 0 ? nullptr : &_vector_sum_row, c, _flip_signedness ? &_signed_output : dst,
-                a->dimension(0), _a_offset, _b_offset, info.gemmlowp_output_stage());
+                &_mm_result_s32, &_vector_sum_col, _b_offset == 0 ? nullptr : &_vector_sum_row, c,
+                _flip_signedness ? &_signed_output : dst, a->dimension(0), _a_offset, _b_offset,
+                info.gemmlowp_output_stage());
 
             if (_flip_signedness)
             {
@@ -280,9 +276,8 @@ void CpuGemmLowpMatrixMultiplyCore::configure(
             }
             // Configure offset contribution kernel
             _offset_contribution_kernel = std::make_unique<kernels::CpuGemmLowpOffsetContributionKernel>();
-            _offset_contribution_kernel->configure(dst, _a_offset == 0 ? nullptr : &_vector_sum_col,
-                                                   _b_offset == 0 ? nullptr : &_vector_sum_row, a_to_use->dimension(0),
-                                                   _a_offset, _b_offset);
+            _offset_contribution_kernel->configure(dst, &_vector_sum_col, _b_offset == 0 ? nullptr : &_vector_sum_row,
+                                                   a_to_use->dimension(0), _a_offset, _b_offset);
         }
     }
     // Configure activation
@@ -305,11 +300,10 @@ void CpuGemmLowpMatrixMultiplyCore::configure(
     }
 
     // Request memory for LHS and RHS reshape matrix
-    _aux_mem[VectorSumCol] =
-        MemoryInfo(offset_int_vec(VectorSumCol),
-                   !_fused_assembly_path && _a_offset != 0 && _reshape_b_only_on_first_run ? MemoryLifetime::Persistent
-                                                                                           : MemoryLifetime::Temporary,
-                   _vector_sum_col.total_size());
+    _aux_mem[VectorSumCol] = MemoryInfo(
+        offset_int_vec(VectorSumCol),
+        !_fused_assembly_path && _reshape_b_only_on_first_run ? MemoryLifetime::Persistent : MemoryLifetime::Temporary,
+        _vector_sum_col.total_size());
     _aux_mem[VectorSumRow] =
         MemoryInfo(offset_int_vec(VectorSumRow), MemoryLifetime::Temporary, _vector_sum_row.total_size());
     _aux_mem[TmpA] = MemoryInfo(offset_int_vec(TmpA), MemoryLifetime::Temporary, _tmp_a.total_size());
@@ -333,8 +327,8 @@ Status CpuGemmLowpMatrixMultiplyCore::validate(const ITensorInfo *a,
     ARM_COMPUTE_RETURN_ERROR_ON_DATA_TYPE_CHANNEL_NOT_IN(b, 1, DataType::QASYMM8, DataType::QASYMM8_SIGNED,
                                                          DataType::QSYMM8, DataType::QSYMM8_PER_CHANNEL);
     ARM_COMPUTE_RETURN_ERROR_ON_DATA_TYPE_CHANNEL_NOT_IN(output, 1, DataType::S32, DataType::QASYMM8,
-                                                         DataType::QASYMM8_SIGNED);
-    ARM_COMPUTE_RETURN_ERROR_ON_MSG(c != nullptr &&
+                                                         DataType::QASYMM8_SIGNED, DataType::F32);
+    ARM_COMPUTE_RETURN_ERROR_ON_MSG(c != nullptr && output->data_type() != DataType::F32 &&
                                         gemm_info.gemmlowp_output_stage().type == GEMMLowpOutputStageType::NONE,
                                     "Bias addition not supported in NEGEMMLowpMatrixMultiplyCore for output S32");
     ARM_COMPUTE_RETURN_ERROR_ON_MSG(
@@ -553,7 +547,9 @@ void CpuGemmLowpMatrixMultiplyCore::run(ITensorPack &tensors)
 {
     prepare(tensors);
 
-    auto a        = tensors.get_const_tensor(TensorType::ACL_SRC_0);
+    auto a    = tensors.get_const_tensor(TensorType::ACL_SRC_0);
+    _a_offset = (a->info()->quantization_info().uniform().offset);
+
     auto b        = tensors.get_const_tensor(TensorType::ACL_SRC_1);
     auto c        = tensors.get_const_tensor(TensorType::ACL_SRC_2);
     auto dst      = tensors.get_tensor(TensorType::ACL_DST);
@@ -642,7 +638,7 @@ void CpuGemmLowpMatrixMultiplyCore::run(ITensorPack &tensors)
         }
 
         // Run matrix B reduction kernel only if _a_offset is not equal to 0
-        if (_a_offset != 0 && !_reshape_b_only_on_first_run)
+        if (!_reshape_b_only_on_first_run)
         {
             ITensorPack pack = {{TensorType::ACL_SRC, b}, {TensorType::ACL_DST, vector_sum_col.get()}};
             NEScheduler::get().schedule_op(_mtx_b_reduction_kernel.get(), Window::DimX,
@@ -653,7 +649,7 @@ void CpuGemmLowpMatrixMultiplyCore::run(ITensorPack &tensors)
         {
             ITensorPack pack;
             pack.add_tensor(TensorType::ACL_SRC_0, mm_result_s32.get());
-            pack.add_tensor(TensorType::ACL_SRC_1, _a_offset == 0 ? nullptr : vector_sum_col.get());
+            pack.add_tensor(TensorType::ACL_SRC_1, vector_sum_col.get());
             pack.add_tensor(TensorType::ACL_SRC_2, _b_offset == 0 ? nullptr : vector_sum_row.get());
             pack.add_tensor(TensorType::ACL_SRC_3, c);
             pack.add_tensor(TensorType::ACL_DST, _flip_signedness ? signed_output.get() : dst);
@@ -664,8 +660,10 @@ void CpuGemmLowpMatrixMultiplyCore::run(ITensorPack &tensors)
         }
         else
         {
+            _offset_contribution_kernel.get()->update_a_offset(_a_offset);
+            _offset_contribution_kernel.get()->update_scale(dst->info()->quantization_info().uniform().scale);
             ITensorPack pack;
-            pack.add_tensor(TensorType::ACL_SRC_0, _a_offset == 0 ? nullptr : vector_sum_col.get());
+            pack.add_tensor(TensorType::ACL_SRC_0, vector_sum_col.get());
             pack.add_tensor(TensorType::ACL_SRC_1, _b_offset == 0 ? nullptr : vector_sum_row.get());
             pack.add_tensor(TensorType::ACL_DST, dst);
 
@@ -713,7 +711,7 @@ void CpuGemmLowpMatrixMultiplyCore::prepare(ITensorPack &tensors)
         }
 
         // Run matrix B reduction kernel only if _a_offset is not equal to 0
-        if (!_fused_assembly_path && _a_offset != 0 && _reshape_b_only_on_first_run)
+        if (!_fused_assembly_path && _reshape_b_only_on_first_run)
         {
             ITensor *vector_sum_col_p =
                 utils::cast::polymorphic_downcast<ITensor *>(tensors.get_tensor(offset_int_vec(VectorSumCol)));
diff --git a/src/cpu/operators/internal/CpuGemmAssemblyDispatch.cpp b/src/cpu/operators/internal/CpuGemmAssemblyDispatch.cpp
index 611bc76463..7308aedf2f 100644
--- a/src/cpu/operators/internal/CpuGemmAssemblyDispatch.cpp
+++ b/src/cpu/operators/internal/CpuGemmAssemblyDispatch.cpp
@@ -1,5 +1,5 @@
 /*
- * Copyright (c) 2018-2023 Arm Limited.
+ * Copyright (c) 2018-2024 Arm Limited.
  *
  * SPDX-License-Identifier: MIT
  *
@@ -617,6 +617,12 @@ void Fallback<TypeInput, TypeOutput, OutputStage>::run(ITensorPack &tensors)
     auto d = tensors.get_tensor(TensorType::ACL_DST);
     ARM_COMPUTE_ERROR_ON_NULLPTR(a, d);
 
+    // check if the type of OutputStage is DequantizeFloat
+    if (std::is_same<OutputStage, arm_gemm::DequantizeFloat>::value)
+    {
+        _gemm_kernel_asm->set_dequantize_scale(d->info()->quantization_info().uniform().scale);
+    }
+
     int       lda = a->info()->strides_in_bytes().y() / a->info()->element_size();
     int       ldb = 0;
     const int ldd = d->info()->strides_in_bytes().y() / d->info()->element_size();
@@ -770,6 +776,42 @@ void create_arm_gemm(std::unique_ptr<CpuGemmAssemblyDispatch::IFallback> &arm_ge
     arm_gemm = std::move(fallback);
 }
 
+template <typename TypeInput, typename TypeOutput>
+void create_arm_gemm_dequant(std::unique_ptr<CpuGemmAssemblyDispatch::IFallback> &arm_gemm,
+                             const ITensorInfo                                   *a,
+                             const ITensorInfo                                   *b,
+                             const ITensorInfo                                   *c,
+                             ITensorInfo                                         *d,
+                             arm_gemm::Activation                                 activation,
+                             const AsmGemmInfo                                   &info)
+{
+    ARM_COMPUTE_UNUSED(activation);
+
+    Params             p           = extract_parameters(a, b, d, info);
+    const CPUInfo     &ci          = NEScheduler::get().cpu_info();
+    const unsigned int num_threads = NEScheduler::get().num_threads();
+
+    arm_gemm::GemmConfig cfg;
+    cfg.weight_format = assembly_utils::map_to_arm_gemm_weight_format(info.weight_format);
+    arm_gemm::GemmArgs args(&ci, p.M, p.N, p.K, p.sections, p.batches, p.multis, p.indirect, activation, num_threads,
+                            info.fixed_format, info.fast_mode, &cfg);
+
+    // Create arm_gemm fallback
+    auto fallback = std::make_unique<Fallback<TypeInput, TypeOutput, arm_gemm::DequantizeFloat>>();
+
+    // Configure requantization info
+    const int32_t                 negation = info.negated_offsets ? 1 : -1;
+    const int32_t                 a_offset = -a->quantization_info().uniform().offset * negation;
+    const int32_t                 b_offset = -b->quantization_info().uniform().offset * negation;
+    const GEMMLowpOutputStageInfo os_info  = info.output_stage;
+
+    arm_gemm::DequantizeFloat gemm_dequant_info{};
+    gemm_dequant_info = arm_gemm::DequantizeFloat(d->quantization_info().uniform().scale);
+
+    fallback->configure(a, b, c, d, args, info, gemm_dequant_info);
+    arm_gemm = std::move(fallback);
+}
+
 template <typename TypeInput, typename TypeOutput>
 void create_arm_gemm_quant(std::unique_ptr<CpuGemmAssemblyDispatch::IFallback> &arm_gemm,
                            const ITensorInfo                                   *a,
@@ -1008,6 +1050,10 @@ void CpuGemmAssemblyDispatch::configure(
             {
                 create_arm_gemm<int8_t, int32_t>(_arm_gemm, a, b, c, d, act, info);
             }
+            else if (d->data_type() == DataType::F32)
+            {
+                create_arm_gemm_dequant<int8_t, float>(_arm_gemm, a, b, c, d, act, info);
+            }
             else
             {
                 create_arm_gemm_quant<int8_t, int8_t>(_arm_gemm, a, b, c, d, act, info);