*******************************************************************************
 Copyright 2024 Arm Limited and affiliates.
 SPDX-License-Identifier: Apache-2.0

 Licensed under the Apache License, Version 2.0 (the "License");
 you may not use this file except in compliance with the License.
 You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License.
 *******************************************************************************
diff --git a/src/common/memory_tracking.hpp b/src/common/memory_tracking.hpp
index 9f6b41a001..28b58c6158 100644
--- a/src/common/memory_tracking.hpp
+++ b/src/common/memory_tracking.hpp
@@ -241,6 +241,9 @@ enum {
     key_lnorm_tmp_diff_ss,
     key_lnorm_reduction,
     key_matmul_dst_in_acc_dt,
+    key_matmul_src_trans,
+    key_matmul_wei_trans,
+    key_matmul_dst_trans,
     key_pool_dst_bf16cvt,
     key_pool_dst_plain2blocked_cvt,
     key_pool_ind_plain2blocked_cvt,
diff --git a/src/cpu/aarch64/matmul/acl_matmul.cpp b/src/cpu/aarch64/matmul/acl_matmul.cpp
index 91f342d44e..06b3ef21b3 100644
--- a/src/cpu/aarch64/matmul/acl_matmul.cpp
+++ b/src/cpu/aarch64/matmul/acl_matmul.cpp
@@ -31,70 +31,168 @@ status_t acl_matmul_t::execute_forward(const exec_ctx_t &ctx) const {
     auto src_base = CTX_IN_MEM(const data_t *, DNNL_ARG_SRC);
     auto wei_base = CTX_IN_MEM(const data_t *, DNNL_ARG_WEIGHTS);
 
-    bool is_transA = pd()->amp_.is_transA;
-    bool is_transB = pd()->amp_.is_transB;
-    bool do_transC = pd()->amp_.do_transC;
-    bool use_dst_acc_for_sum = pd()->amp_.use_dst_acc_for_sum;
-
-    std::lock_guard<std::mutex> _lock {this->mtx};
-    auto *acl_resource = ctx.get_resource_mapper()->get<acl_resource_t>(this);
-    acl_matmul_obj_t &acl_obj = acl_resource->get_acl_obj();
+    auto amp = pd()->amp_;
+    bool is_transA = amp.is_transA;
+    bool is_transB = amp.is_transB;
+    bool do_transC = amp.do_transC;
+    bool do_act = amp.do_act;
+    bool use_dst_acc_for_sum = amp.use_dst_acc_for_sum;
 
     const auto scratchpad = ctx.get_scratchpad_grantor();
 
+    arm_compute::Tensor src_tensor;
+    arm_compute::Tensor wei_tensor;
+    arm_compute::Tensor bia_tensor = nullptr;
+    arm_compute::Tensor dst_tensor;
+    arm_compute::Tensor dst_acc_tensor;
+    src_tensor.allocator()->init(amp.src_tensor_info);
+    wei_tensor.allocator()->init(amp.wei_tensor_info);
+    dst_tensor.allocator()->init(amp.dst_tensor_info);
+
+    // If we have an unfused sum post op, put the result in a scratchpad tensor.
+    // Result will be summed to the dst during acl_post_ops.execute
+    auto dst_base = use_dst_acc_for_sum ? scratchpad.get<void>(
+                            memory_tracking::names::key_matmul_dst_in_acc_dt)
+                                        : CTX_OUT_MEM(data_t *, DNNL_ARG_DST);
+    dst_tensor.allocator()->import_memory(dst_base);
+
     // Run transpose kernel
     if (is_transA && !is_transB) {
-        acl_obj.src_tensor.allocator()->allocate();
-        acl_obj.src_acc_tensor.allocator()->import_memory(
+        arm_compute::Tensor src_acc_tensor;
+        src_acc_tensor.allocator()->init(amp.src_acc_info);
+        src_acc_tensor.allocator()->import_memory(
                 const_cast<data_t *>(src_base));
-        acl_obj.transA.run();
-        acl_obj.wei_tensor.allocator()->import_memory(
-                const_cast<data_t *>(wei_base));
+        auto transA_scratch = scratchpad.get<void>(
+                memory_tracking::names::key_matmul_src_trans);
+        src_tensor.allocator()->import_memory(transA_scratch);
+        arm_compute::ITensorPack transpose_pack;
+        transpose_pack.add_tensor(
+                arm_compute::TensorType::ACL_SRC, &src_acc_tensor);
+        transpose_pack.add_tensor(
+                arm_compute::TensorType::ACL_DST, &src_tensor);
+        acl_obj_->transA.run(transpose_pack);
+        wei_tensor.allocator()->import_memory(const_cast<data_t *>(wei_base));
+        src_acc_tensor.allocator()->free();
     } else if (is_transB && !is_transA) {
-        acl_obj.wei_tensor.allocator()->allocate();
-        acl_obj.wei_acc_tensor.allocator()->import_memory(
+        arm_compute::Tensor wei_acc_tensor;
+        wei_acc_tensor.allocator()->init(amp.wei_acc_info);
+        wei_acc_tensor.allocator()->import_memory(
                 const_cast<data_t *>(wei_base));
-        acl_obj.transB.run();
-        acl_obj.src_tensor.allocator()->import_memory(
-                const_cast<data_t *>(src_base));
+        auto transB_scratch = scratchpad.get<void>(
+                memory_tracking::names::key_matmul_wei_trans);
+        wei_tensor.allocator()->import_memory(transB_scratch);
+        arm_compute::ITensorPack transpose_pack;
+        transpose_pack.add_tensor(
+                arm_compute::TensorType::ACL_SRC, &wei_acc_tensor);
+        transpose_pack.add_tensor(
+                arm_compute::TensorType::ACL_DST, &wei_tensor);
+        acl_obj_->transB.run(transpose_pack);
+        src_tensor.allocator()->import_memory(const_cast<data_t *>(src_base));
+        wei_acc_tensor.allocator()->free();
     } else if (is_transA && is_transB && !do_transC) {
-        acl_obj.src_tensor.allocator()->allocate();
-        acl_obj.src_acc_tensor.allocator()->import_memory(
+        arm_compute::Tensor src_acc_tensor;
+        arm_compute::Tensor wei_acc_tensor;
+        src_acc_tensor.allocator()->init(amp.src_acc_info);
+        src_acc_tensor.allocator()->import_memory(
                 const_cast<data_t *>(src_base));
-        acl_obj.wei_tensor.allocator()->allocate();
-        acl_obj.wei_acc_tensor.allocator()->import_memory(
+        wei_acc_tensor.allocator()->init(amp.wei_acc_info);
+        wei_acc_tensor.allocator()->import_memory(
                 const_cast<data_t *>(wei_base));
-        acl_obj.transA.run();
-        acl_obj.transB.run();
+        auto transA_scratch = scratchpad.get<void>(
+                memory_tracking::names::key_matmul_src_trans);
+        auto transB_scratch = scratchpad.get<void>(
+                memory_tracking::names::key_matmul_wei_trans);
+        src_tensor.allocator()->import_memory(transA_scratch);
+        wei_tensor.allocator()->import_memory(transB_scratch);
+        arm_compute::ITensorPack transpose_packA;
+        transpose_packA.add_tensor(
+                arm_compute::TensorType::ACL_SRC, &src_acc_tensor);
+        transpose_packA.add_tensor(
+                arm_compute::TensorType::ACL_DST, &src_tensor);
+        arm_compute::ITensorPack transpose_packB;
+        transpose_packB.add_tensor(
+                arm_compute::TensorType::ACL_SRC, &wei_acc_tensor);
+        transpose_packB.add_tensor(
+                arm_compute::TensorType::ACL_DST, &wei_tensor);
+        acl_obj_->transA.run(transpose_packA);
+        acl_obj_->transB.run(transpose_packB);
+        src_acc_tensor.allocator()->free();
+        wei_acc_tensor.allocator()->free();
     } else {
-        acl_obj.src_tensor.allocator()->import_memory(
-                const_cast<data_t *>(src_base));
-        acl_obj.wei_tensor.allocator()->import_memory(
-                const_cast<data_t *>(wei_base));
-        if (do_transC) { acl_obj.dst_acc_tensor.allocator()->allocate(); }
+        src_tensor.allocator()->import_memory(const_cast<data_t *>(src_base));
+        wei_tensor.allocator()->import_memory(const_cast<data_t *>(wei_base));
+        if (do_transC) {
+            auto transC_scratch = scratchpad.get<void>(
+                    memory_tracking::names::key_matmul_dst_trans);
+            dst_acc_tensor.allocator()->init(amp.dst_acc_info);
+            dst_acc_tensor.allocator()->import_memory(transC_scratch);
+        }
     }
 
-    // If we have an unfused sum post op, put the result in a scratchpad tensor.
-    // Result will be summed to the dst during acl_post_ops.execute
-    auto dst_base = use_dst_acc_for_sum ? scratchpad.get<void>(
-                            memory_tracking::names::key_matmul_dst_in_acc_dt)
-                                        : CTX_OUT_MEM(data_t *, DNNL_ARG_DST);
-    acl_obj.dst_tensor.allocator()->import_memory(dst_base);
+    if (do_transC) {
+        acl_obj_->matmul_pack.add_const_tensor(
+                arm_compute::TensorType::ACL_SRC_0, &wei_tensor);
+        acl_obj_->matmul_pack.add_const_tensor(
+                arm_compute::TensorType::ACL_SRC_1, &src_tensor);
+        acl_obj_->matmul_pack.add_tensor(
+                arm_compute::TensorType::ACL_SRC_2, &bia_tensor);
+        acl_obj_->matmul_pack.add_tensor(
+                arm_compute::TensorType::ACL_DST, &dst_acc_tensor);
+    } else {
+        acl_obj_->matmul_pack.add_const_tensor(
+                arm_compute::TensorType::ACL_SRC_0, &src_tensor);
+        acl_obj_->matmul_pack.add_const_tensor(
+                arm_compute::TensorType::ACL_SRC_1, &wei_tensor);
+        acl_obj_->matmul_pack.add_tensor(
+                arm_compute::TensorType::ACL_SRC_2, &bia_tensor);
+        acl_obj_->matmul_pack.add_tensor(
+                arm_compute::TensorType::ACL_DST, &dst_tensor);
+    }
 
-    acl_obj.gemm.run();
+    // Get pointer to scratchpad memory and create a workspace tensor for
+    // CpuGemm. Fixed-format kernel does not need this workspace tensor.
+    arm_compute::Tensor tmp_tensor;
+    if (!IsFixedFormat && acl_obj_->aux_mem_req[0].size != 0) {
+        auto buffer = scratchpad.get<void>(
+                memory_tracking::names::key_gemm_tmp_buffer);
+        tmp_tensor.allocator()->init(
+                amp.tmp_tensor_info, acl_obj_->aux_mem_req[0].alignment);
+        tmp_tensor.allocator()->import_memory(buffer);
+        acl_obj_->matmul_pack.add_tensor(
+                acl_obj_->aux_mem_req[0].slot, &tmp_tensor);
+    }
+    arm_compute::Tensor aux_tensor;
+    if (!IsFixedFormat && acl_obj_->aux_mem_req[2].size != 0) {
+        auto buffer = scratchpad.get<void>(
+                memory_tracking::names::key_gemm_blocked_b);
+        aux_tensor.allocator()->init(
+                amp.aux_tensor_info, acl_obj_->aux_mem_req[2].alignment);
+        aux_tensor.allocator()->import_memory(buffer);
+        acl_obj_->matmul_pack.add_tensor(
+                acl_obj_->aux_mem_req[2].slot, &aux_tensor);
+    }
 
-    if (do_transC) { acl_obj.transC.run(); }
+    acl_obj_->asm_gemm.run(acl_obj_->matmul_pack);
+    if (do_act) {
+        auto dst_to_use = do_transC ? &dst_acc_tensor : &dst_tensor;
+        arm_compute::ITensorPack act_pack;
+        act_pack.add_tensor(arm_compute::TensorType::ACL_SRC, dst_to_use);
+        act_pack.add_tensor(arm_compute::TensorType::ACL_DST, dst_to_use);
+        acl_obj_->act.run(act_pack);
+    }
 
-    acl_obj.src_tensor.allocator()->free();
-    acl_obj.wei_tensor.allocator()->free();
-    if (is_transA) acl_obj.src_acc_tensor.allocator()->free();
-    if (is_transB) acl_obj.wei_acc_tensor.allocator()->free();
+    if (do_transC) {
+        arm_compute::ITensorPack transpose_packC;
+        transpose_packC.add_tensor(
+                arm_compute::TensorType::ACL_SRC, &dst_acc_tensor);
+        transpose_packC.add_tensor(
+                arm_compute::TensorType::ACL_DST, &dst_tensor);
+        acl_obj_->transC.run(transpose_packC);
+    }
 
-    void *dst = acl_obj.dst_tensor.buffer();
+    void *dst = dst_tensor.buffer();
     pd()->acl_post_ops.execute(ctx, dst);
 
-    acl_obj.dst_tensor.allocator()->free();
-
     return status;
 }
 
diff --git a/src/cpu/aarch64/matmul/acl_matmul.hpp b/src/cpu/aarch64/matmul/acl_matmul.hpp
index 1427a5bcb8..50426550f7 100644
--- a/src/cpu/aarch64/matmul/acl_matmul.hpp
+++ b/src/cpu/aarch64/matmul/acl_matmul.hpp
@@ -27,56 +27,6 @@ namespace cpu {
 namespace aarch64 {
 namespace matmul {
 
-struct acl_resource_t : public resource_t {
-    acl_resource_t() : acl_obj_(utils::make_unique<acl_matmul_obj_t>()) {}
-
-    status_t configure(const acl_matmul_conf_t &amp,
-            const dnnl::impl::format_kind_t weights_format_kind_) {
-        if (!acl_obj_) return status::out_of_memory;
-        acl_obj_->src_tensor.allocator()->init(amp.src_tensor_info);
-        acl_obj_->wei_tensor.allocator()->init(amp.wei_tensor_info);
-        acl_obj_->dst_tensor.allocator()->init(amp.dst_tensor_info);
-
-        // Configure transpose kernel for src, wei, or dst
-        if (amp.is_transA && !amp.do_transC) {
-            acl_obj_->src_acc_tensor.allocator()->init(amp.src_acc_info);
-            acl_obj_->transA.configure(
-                    &acl_obj_->src_acc_tensor, &acl_obj_->src_tensor);
-        }
-
-        if (amp.is_transB && !amp.do_transC) {
-            acl_obj_->wei_acc_tensor.allocator()->init(amp.wei_acc_info);
-            acl_obj_->transB.configure(
-                    &acl_obj_->wei_acc_tensor, &acl_obj_->wei_tensor);
-        }
-
-        if (amp.do_transC) {
-            acl_obj_->dst_acc_tensor.allocator()->init(amp.dst_acc_info);
-            acl_obj_->transC.configure(
-                    &acl_obj_->dst_acc_tensor, &acl_obj_->dst_tensor);
-        }
-
-        // Configure GEMM
-        if (amp.do_transC) {
-            acl_obj_->gemm.configure(&acl_obj_->wei_tensor,
-                    &acl_obj_->src_tensor, nullptr, &acl_obj_->dst_acc_tensor,
-                    1.0f, 0.0f, amp.gemm_info);
-        } else {
-            acl_obj_->gemm.configure(&acl_obj_->src_tensor,
-                    &acl_obj_->wei_tensor, nullptr, &acl_obj_->dst_tensor, 1.0f,
-                    0.0f, amp.gemm_info);
-        }
-
-        return status::success;
-    }
-    acl_matmul_obj_t &get_acl_obj() const { return *acl_obj_; }
-
-    DNNL_DISALLOW_COPY_AND_ASSIGN(acl_resource_t);
-
-private:
-    std::unique_ptr<acl_matmul_obj_t> acl_obj_;
-};
-
 struct acl_matmul_t : public primitive_t {
     struct pd_t : public dnnl::impl::cpu::matmul::cpu_matmul_pd_t {
 
@@ -152,25 +102,73 @@ struct acl_matmul_t : public primitive_t {
                 amp_.gemm_info.set_accumulate(true);
             }
 
+            amp_.do_act = false;
             arm_compute::ActivationLayerInfo act_info;
             CHECK(acl_post_ops.init(engine, attr_.post_ops_, dst_md_, act_info,
-                    amp_.gemm_info.accumulate() ? 1 : 0));
+                    amp_.gemm_info.accumulate()));
+            if (act_info.enabled()
+                    && !arm_compute::op::CpuGemmAssemblyDispatch::
+                               is_activation_supported(act_info)) {
+                auto dst_info_to_use = amp_.do_transC ? &amp_.dst_acc_info
+                                                      : &amp_.dst_tensor_info;
+                ACL_CHECK_VALID(arm_compute::op::CpuActivation::validate(
+                        dst_info_to_use, dst_info_to_use, act_info));
+                amp_.do_act = true;
+            }
             amp_.gemm_info.set_activation_info(act_info);
             amp_.use_dst_acc_for_sum = acl_post_ops.has_sum();
 
             // Validate ACL GEMM
             if (amp_.do_transC) {
-                ACL_CHECK_VALID(arm_compute::NEGEMM::validate(
-                        &amp_.wei_tensor_info, &amp_.src_tensor_info, nullptr,
-                        &amp_.dst_acc_info, 1.0f, 0.0f, amp_.gemm_info));
+                ACL_CHECK_VALID(
+                        arm_compute::op::CpuGemmAssemblyDispatch::validate(
+                                &amp_.wei_tensor_info, &amp_.src_tensor_info,
+                                nullptr, &amp_.dst_acc_info, amp_.gemm_info));
             } else {
-                ACL_CHECK_VALID(arm_compute::NEGEMM::validate(
-                        &amp_.src_tensor_info, &amp_.wei_tensor_info, nullptr,
-                        &amp_.dst_tensor_info, 1.0f, 0.0f, amp_.gemm_info));
+                ACL_CHECK_VALID(
+                        arm_compute::op::CpuGemmAssemblyDispatch::validate(
+                                &amp_.src_tensor_info, &amp_.wei_tensor_info,
+                                nullptr, &amp_.dst_tensor_info,
+                                amp_.gemm_info));
             }
 
             auto scratchpad = scratchpad_registry().registrar();
-            CHECK(acl_matmul_utils::init_scratchpad(scratchpad, amp_, dst_md_));
+            CHECK(acl_matmul_utils::init_scratchpad(
+                    scratchpad, amp_, src_md_, weights_md_, dst_md_));
+
+            // Query buffer memory requirement, if not using fixed-format kernel
+            if (weights_format_kind_ != format_kind::any) {
+                arm_compute::op::CpuGemmAssemblyDispatch asm_gemm;
+                if (amp_.do_transC) {
+                    asm_gemm.configure(&amp_.wei_tensor_info,
+                            &amp_.src_tensor_info, nullptr, &amp_.dst_acc_info,
+                            amp_.gemm_info);
+                } else {
+                    asm_gemm.configure(&amp_.src_tensor_info,
+                            &amp_.wei_tensor_info, nullptr,
+                            &amp_.dst_tensor_info, amp_.gemm_info);
+                }
+                auto aux_mem_req = asm_gemm.workspace();
+
+                if (aux_mem_req[0].size != 0) {
+                    auto scratchpad = scratchpad_registry().registrar();
+                    scratchpad.book(memory_tracking::names::key_gemm_tmp_buffer,
+                            aux_mem_req[0].size, 1, aux_mem_req[0].alignment,
+                            aux_mem_req[0].alignment);
+                    amp_.tmp_tensor_info = arm_compute::TensorInfo(
+                            arm_compute::TensorShape(aux_mem_req[0].size), 1,
+                            arm_compute::DataType::U8);
+                }
+                if (aux_mem_req[2].size != 0) {
+                    auto scratchpad = scratchpad_registry().registrar();
+                    scratchpad.book(memory_tracking::names::key_gemm_blocked_b,
+                            aux_mem_req[2].size, 1, aux_mem_req[2].alignment,
+                            aux_mem_req[2].alignment);
+                    amp_.aux_tensor_info = arm_compute::TensorInfo(
+                            arm_compute::TensorShape(aux_mem_req[2].size), 1,
+                            arm_compute::DataType::U8);
+                }
+            }
 
             return status::success;
         }
@@ -186,23 +184,53 @@ struct acl_matmul_t : public primitive_t {
         }
     };
 
-    acl_matmul_t(const pd_t *apd) : primitive_t(apd) {}
+    acl_matmul_t(const pd_t *apd)
+        : primitive_t(apd), acl_obj_(std::make_unique<acl_matmul_obj_t>()) {}
 
     status_t create_resource(
             engine_t *engine, resource_mapper_t &mapper) const override {
-        if (mapper.has_resource(this)) return status::success;
-        auto r = utils::make_unique<acl_resource_t>();
-        if (!r) return status::out_of_memory;
-
-        // Configure the resource based on information from primitive descriptor
-        CHECK(r->configure(pd()->amp_, pd()->weights_format_kind_));
-        mapper.add(this, std::move(r));
 
         CHECK(pd()->acl_post_ops.create_resource(engine, mapper));
 
         return status::success;
     }
 
+    status_t init(engine_t *engine) override {
+        auto amp_ = pd()->amp_;
+        // Configure transpose kernel for src and wei
+        if (amp_.is_transA && !amp_.do_transC) {
+            acl_obj_->transA.configure(
+                    &amp_.src_acc_info, &amp_.src_tensor_info);
+        }
+        if (amp_.is_transB && !amp_.do_transC) {
+            acl_obj_->transB.configure(
+                    &amp_.wei_acc_info, &amp_.wei_tensor_info);
+        }
+        if (amp_.do_transC) {
+            acl_obj_->transC.configure(
+                    &amp_.dst_acc_info, &amp_.dst_tensor_info);
+        }
+        // Configure GEMM
+        if (amp_.do_transC) {
+            acl_obj_->asm_gemm.configure(&amp_.wei_tensor_info,
+                    &amp_.src_tensor_info, nullptr, &amp_.dst_acc_info,
+                    amp_.gemm_info);
+        } else {
+            acl_obj_->asm_gemm.configure(&amp_.src_tensor_info,
+                    &amp_.wei_tensor_info, nullptr, &amp_.dst_tensor_info,
+                    amp_.gemm_info);
+        }
+        acl_obj_->aux_mem_req = acl_obj_->asm_gemm.workspace();
+        if (amp_.do_act) {
+            auto dst_info_to_use = amp_.do_transC ? &amp_.dst_acc_info
+                                                  : &amp_.dst_tensor_info;
+            acl_obj_->act.configure(dst_info_to_use, dst_info_to_use,
+                    amp_.gemm_info.activation_info());
+        }
+
+        return status::success;
+    }
+
     typedef typename prec_traits<data_type::f32>::type data_t;
 
     status_t execute(const exec_ctx_t &ctx) const override {
@@ -214,12 +242,14 @@ struct acl_matmul_t : public primitive_t {
     }
 
 private:
-    // To guard the const execute_forward(), the mutex must be 'mutable'
-    mutable std::mutex mtx;
     template <bool IsFixedFormat>
     status_t execute_forward(const exec_ctx_t &ctx) const;
 
-    const pd_t *pd() const { return (const pd_t *)primitive_t::pd().get(); }
+    const pd_t *pd() const {
+        return (const pd_t *)primitive_t::pd().get();
+    }
+
+    std::unique_ptr<acl_matmul_obj_t> acl_obj_;
 }; // acl_matmul_t
 
 } // namespace matmul
diff --git a/src/cpu/aarch64/matmul/acl_matmul_utils.cpp b/src/cpu/aarch64/matmul/acl_matmul_utils.cpp
index 134ce94c90..0a64e5d8e1 100644
--- a/src/cpu/aarch64/matmul/acl_matmul_utils.cpp
+++ b/src/cpu/aarch64/matmul/acl_matmul_utils.cpp
@@ -104,6 +104,7 @@ status_t init_conf_matmul(acl_matmul_conf_t &amp, memory_desc_t &src_md,
                 acl_dst_data_t);
         amp.src_tensor_info = arm_compute::TensorInfo(
                 arm_compute::TensorShape(M, K, src_batch), 1, acl_src_data_t);
+        amp.src_tensor_info.set_are_values_constant(false);
         amp.wei_tensor_info = arm_compute::TensorInfo(
                 arm_compute::TensorShape(K, N, 1, wei_batch), 1,
                 acl_wei_data_t);
@@ -113,6 +114,7 @@ status_t init_conf_matmul(acl_matmul_conf_t &amp, memory_desc_t &src_md,
                 acl_src_data_t);
         amp.wei_tensor_info = arm_compute::TensorInfo(
                 arm_compute::TensorShape(N, K, wei_batch), 1, acl_wei_data_t);
+        amp.wei_tensor_info.set_are_values_constant(false);
     }
 
     amp.dst_tensor_info = arm_compute::TensorInfo(
@@ -120,13 +122,13 @@ status_t init_conf_matmul(acl_matmul_conf_t &amp, memory_desc_t &src_md,
 
     // Validate ACL transpose
     if (amp.is_transA && !amp.do_transC)
-        ACL_CHECK_VALID(arm_compute::NETranspose::validate(
+        ACL_CHECK_VALID(arm_compute::op::CpuTranspose::validate(
                 &amp.src_acc_info, &amp.src_tensor_info));
     if (amp.is_transB && !amp.do_transC)
-        ACL_CHECK_VALID(arm_compute::NETranspose::validate(
+        ACL_CHECK_VALID(arm_compute::op::CpuTranspose::validate(
                 &amp.wei_acc_info, &amp.wei_tensor_info));
     if (amp.do_transC)
-        ACL_CHECK_VALID(arm_compute::NETranspose::validate(
+        ACL_CHECK_VALID(arm_compute::op::CpuTranspose::validate(
                 &amp.dst_acc_info, &amp.dst_tensor_info));
 
     bool is_fastmath_enabled = utils::one_of(
@@ -143,10 +145,10 @@ status_t init_conf_matmul(acl_matmul_conf_t &amp, memory_desc_t &src_md,
         // in (if a kernel exists). Note that these are referred to as fixed format
         // kernels, because they require one specific weights format
         arm_compute::WeightFormat expected_weight_format;
-        ACL_CHECK_VALID(
-                arm_compute::NEGEMM::has_opt_impl(expected_weight_format,
-                        &amp.src_tensor_info, &amp.wei_tensor_info, nullptr,
-                        &amp.dst_tensor_info, 1.0f, 0.0f, amp.gemm_info));
+        ACL_CHECK_VALID(arm_compute::op::CpuGemmAssemblyDispatch::has_opt_impl(
+                expected_weight_format, &amp.src_tensor_info,
+                &amp.wei_tensor_info, nullptr, &amp.dst_tensor_info,
+                amp.gemm_info));
 
         // Set gemm weights info to the one returned by has_opt_impl
         amp.gemm_info.set_weight_format(expected_weight_format);
@@ -174,12 +176,28 @@ status_t init_conf_matmul(acl_matmul_conf_t &amp, memory_desc_t &src_md,
 }
 
 status_t init_scratchpad(memory_tracking::registrar_t &scratchpad,
-        acl_matmul_conf_t &amp, memory_desc_t &dst_md) {
+        acl_matmul_conf_t &amp, memory_desc_t &src_md,
+        memory_desc_t &weights_md, memory_desc_t &dst_md) {
     if (amp.use_dst_acc_for_sum) {
         const memory_desc_wrapper dst_d(&dst_md);
         scratchpad.book(memory_tracking::names::key_matmul_dst_in_acc_dt,
                 dst_d.nelems(), dst_d.data_type_size());
     }
+    if (amp.is_transA) {
+        const memory_desc_wrapper src_d(&src_md);
+        scratchpad.book(memory_tracking::names::key_matmul_src_trans,
+                src_d.nelems(), src_d.data_type_size());
+    }
+    if (amp.is_transB) {
+        const memory_desc_wrapper wei_d(&weights_md);
+        scratchpad.book(memory_tracking::names::key_matmul_wei_trans,
+                wei_d.nelems(), wei_d.data_type_size());
+    }
+    if (amp.do_transC) {
+        const memory_desc_wrapper dst_d(&dst_md);
+        scratchpad.book(memory_tracking::names::key_matmul_dst_trans,
+                dst_d.nelems(), dst_d.data_type_size());
+    }
     return status::success;
 }
 
diff --git a/src/cpu/aarch64/matmul/acl_matmul_utils.hpp b/src/cpu/aarch64/matmul/acl_matmul_utils.hpp
index d3fa65d915..6bda7f5cec 100644
--- a/src/cpu/aarch64/matmul/acl_matmul_utils.hpp
+++ b/src/cpu/aarch64/matmul/acl_matmul_utils.hpp
@@ -17,9 +17,11 @@
 #ifndef CPU_AARCH64_ACL_MATMUL_UTILS_HPP
 #define CPU_AARCH64_ACL_MATMUL_UTILS_HPP
 
-#include "cpu/matmul/cpu_matmul_pd.hpp"
-
+#include "arm_compute/runtime/operators/CpuActivation.h"
+#include "arm_compute/runtime/operators/CpuTranspose.h"
+#include "arm_compute/runtime/operators/CpuGemmAssemblyDispatch.h"
 #include "cpu/aarch64/acl_utils.hpp"
+#include "cpu/matmul/cpu_matmul_pd.hpp"
 
 namespace dnnl {
 namespace impl {
@@ -27,28 +29,28 @@ namespace cpu {
 namespace aarch64 {
 
 struct acl_matmul_obj_t {
-    arm_compute::NEGEMM gemm;
-    arm_compute::NETranspose transA;
-    arm_compute::NETranspose transB;
-    arm_compute::NETranspose transC;
-    arm_compute::Tensor src_tensor;
-    arm_compute::Tensor src_acc_tensor;
-    arm_compute::Tensor wei_acc_tensor;
-    arm_compute::Tensor dst_acc_tensor;
-    arm_compute::Tensor wei_tensor;
-    arm_compute::Tensor dst_tensor;
+    arm_compute::op::CpuGemmAssemblyDispatch asm_gemm;
+    arm_compute::op::CpuActivation act;
+    arm_compute::op::CpuTranspose transA;
+    arm_compute::op::CpuTranspose transB;
+    arm_compute::op::CpuTranspose transC;
+    arm_compute::experimental::MemoryRequirements aux_mem_req;
+    arm_compute::ITensorPack matmul_pack;
 };
 
 struct acl_matmul_conf_t {
     bool is_transA;
     bool is_transB;
     bool do_transC;
+    bool do_act;
     // If this is true, the result of the matmul goes into a temporarily
     // allocated ACL tensor to be accumulated into the oneDNN dst during postops
     bool use_dst_acc_for_sum;
     arm_compute::TensorInfo src_tensor_info;
     arm_compute::TensorInfo wei_tensor_info;
     arm_compute::TensorInfo dst_tensor_info;
+    arm_compute::TensorInfo tmp_tensor_info;
+    arm_compute::TensorInfo aux_tensor_info;
     arm_compute::TensorInfo src_acc_info;
     arm_compute::TensorInfo wei_acc_info;
     arm_compute::TensorInfo dst_acc_info;
@@ -63,7 +65,8 @@ status_t init_conf_matmul(acl_matmul_conf_t &amp, memory_desc_t &src_md,
         const primitive_attr_t &attr);
 
 status_t init_scratchpad(memory_tracking::registrar_t &scratchpad,
-        acl_matmul_conf_t &amp, memory_desc_t &dst_md);
+        acl_matmul_conf_t &amp, memory_desc_t &src_md,
+        memory_desc_t &weights_md, memory_desc_t &dst_md);
 
 } // namespace acl_matmul_utils
 
